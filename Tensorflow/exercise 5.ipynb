{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"exercise 5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"r-Chp8YyyvxN"},"source":["import tensorflow as tf\n","from tensorflow.keras.activations import softmax, tanh\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVE7aMvC4iXK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637517487162,"user_tz":-60,"elapsed":11955,"user":{"displayName":"Janusz Feigel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16985238717625251785"}},"outputId":"7ebf2433-e17c-42ec-81ae-30df84644b89"},"source":["num_words = 20000\n","(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)\n","print(train_sequences.shape)\n","print(test_sequences.shape)\n","word_to_index = tf.keras.datasets.imdb.get_word_index()\n","index_to_word = dict((index, word) for (word, index) in word_to_index.items())\n","sequence_lengths = [len(sequence) for sequence in train_sequences]\n","max_len = max(sequence_lengths)\n","max_len = 200\n","train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)\n","train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n","batch_size = 128\n","epochs = 5\n","train_data = train_data.shuffle(25000).batch(batch_size).repeat(epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n","(25000,)\n","(25000,)\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n","1654784/1641221 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"BS64iKdFXyif"},"source":["loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n","opt = tf.keras.optimizers.Adam(0.001)\n","accuracy_fn = tf.keras.metrics.SparseCategoricalAccuracy()\n","\n","state_size = 1024\n","U = tf.Variable(tf.random.uniform(shape=[num_words, state_size], minval=-0.1, maxval=0.1, dtype=tf.dtypes.float32))\n","b = tf.Variable(tf.zeros(shape=[state_size], dtype=tf.dtypes.float32))\n","W = tf.Variable(tf.random.uniform(shape=[state_size, state_size], minval=-0.1, maxval=0.1, dtype=tf.dtypes.float32))\n","V = tf.Variable(tf.random.uniform(shape=[state_size, 2], minval=-0.1, maxval=0.1, dtype=tf.dtypes.float32))\n","c = tf.Variable(tf.zeros(shape=[2], dtype=tf.dtypes.float32))\n","initial_state = tf.Variable(tf.zeros(shape=[state_size], dtype=tf.dtypes.float32))\n","trainable_weights = [initial_state, U, b, W, V, c]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4fwUhqBACri","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637512870403,"user_tz":-60,"elapsed":4825093,"user":{"displayName":"Janusz Feigel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16985238717625251785"}},"outputId":"0dd4731b-0899-4820-ec01-c0010b0b960f"},"source":["def train_loop():\n","    for step, (sequence_batch, label_batch) in enumerate(train_data):\n","        train_step(sequence_batch, label_batch, step)\n","\n","def train_step(sequences, labels, step):\n","    with tf.GradientTape() as tape:\n","        output = rnn_loop(sequences)\n","        loss = loss_fn(labels, output)\n","\n","    gradients = tape.gradient(loss, trainable_weights)\n","    opt.apply_gradients(zip(gradients, trainable_weights))\n","\n","    if step % 195 == 0:\n","      print(\"\\nNext Epoch\")\n","    if step % 10 == 0:\n","      accuracy_fn.update_state(labels, output)\n","      print(\"Step {} Accuracy: {} Loss: {}\".format(step, accuracy_fn.result().numpy(), loss.numpy()))\n","    \n","\n","def rnn_loop(sequences):\n","    old_state = initial_state\n","\n","    for step in range(max_len):\n","        x_t = sequences[:, step]\n","        x_t = tf.one_hot(x_t, depth=num_words)\n","        new_state = rnn_step(old_state, x_t)\n","\n","        old_state = new_state\n","\n","    o_t = output_layer(new_state)\n","\n","    return o_t\n","\n","def rnn_step(state, x_t):\n","    a = b + tf.linalg.matvec(W, state) + tf.matmul(x_t, U)\n","    h = tanh(a)\n","    return h\n","\n","def output_layer(state):\n","    o = c + tf.matmul(state, V)\n","    o = softmax(o)\n","    return o\n","\n","train_loop()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Next Epoch\n","Step 0 Accuracy: 0.4921875 Loss: 1.122739553451538\n","Step 10 Accuracy: 0.484375 Loss: 0.7648497223854065\n","Step 20 Accuracy: 0.5078125 Loss: 0.6973837614059448\n","Step 30 Accuracy: 0.509765625 Loss: 0.6842910051345825\n","Step 40 Accuracy: 0.5140625238418579 Loss: 0.6883311867713928\n","Step 50 Accuracy: 0.5143229365348816 Loss: 0.6939480900764465\n","Step 60 Accuracy: 0.5066964030265808 Loss: 0.7066179513931274\n","Step 70 Accuracy: 0.5 Loss: 0.737827718257904\n","Step 80 Accuracy: 0.5112847089767456 Loss: 0.680735170841217\n","Step 90 Accuracy: 0.5093749761581421 Loss: 0.6889183521270752\n","Step 100 Accuracy: 0.515625 Loss: 0.6857764720916748\n","Step 110 Accuracy: 0.5162760615348816 Loss: 0.7019472122192383\n","Step 120 Accuracy: 0.5168269276618958 Loss: 0.6960265636444092\n","Step 130 Accuracy: 0.5133928656578064 Loss: 0.7120664119720459\n","Step 140 Accuracy: 0.5151041746139526 Loss: 0.6959869861602783\n","Step 150 Accuracy: 0.51806640625 Loss: 0.7122974395751953\n","Step 160 Accuracy: 0.515625 Loss: 0.7051507830619812\n","Step 170 Accuracy: 0.515625 Loss: 0.6962547302246094\n","Step 180 Accuracy: 0.5160362124443054 Loss: 0.6942243576049805\n","Step 190 Accuracy: 0.5191406011581421 Loss: 0.6881616115570068\n","\n","Next Epoch\n","Step 200 Accuracy: 0.5256696343421936 Loss: 0.6509373188018799\n","Step 210 Accuracy: 0.5298295617103577 Loss: 0.6736952662467957\n","Step 220 Accuracy: 0.5261548757553101 Loss: 0.7445555925369263\n","Step 230 Accuracy: 0.5247395634651184 Loss: 0.7550144791603088\n","Step 240 Accuracy: 0.5256249904632568 Loss: 0.6793889999389648\n","Step 250 Accuracy: 0.5267428159713745 Loss: 0.6808210015296936\n","Step 260 Accuracy: 0.5243055820465088 Loss: 0.7375359535217285\n","Step 270 Accuracy: 0.5267857313156128 Loss: 0.6680616140365601\n","Step 280 Accuracy: 0.5285560488700867 Loss: 0.6716539263725281\n","Step 290 Accuracy: 0.5341145992279053 Loss: 0.6414685845375061\n","Step 300 Accuracy: 0.5355342626571655 Loss: 0.6681746244430542\n","Step 310 Accuracy: 0.538330078125 Loss: 0.6627172231674194\n","Step 320 Accuracy: 0.5421401262283325 Loss: 0.6436479091644287\n","Step 330 Accuracy: 0.5436580777168274 Loss: 0.6625171899795532\n","Step 340 Accuracy: 0.5457589030265808 Loss: 0.6564788818359375\n","Step 350 Accuracy: 0.5460069179534912 Loss: 0.6690183281898499\n","Step 360 Accuracy: 0.5470861196517944 Loss: 0.6609538197517395\n","Step 370 Accuracy: 0.5474917888641357 Loss: 0.664015531539917\n","Step 380 Accuracy: 0.5494791865348816 Loss: 0.6423478722572327\n","\n","Next Epoch\n","Step 390 Accuracy: 0.549609363079071 Loss: 0.6612406373023987\n","Step 400 Accuracy: 0.5554497241973877 Loss: 0.5818545818328857\n","Step 410 Accuracy: 0.5591517686843872 Loss: 0.586234450340271\n","Step 420 Accuracy: 0.5614098906517029 Loss: 0.6509160995483398\n","Step 430 Accuracy: 0.5642755627632141 Loss: 0.5991443991661072\n","Step 440 Accuracy: 0.565625011920929 Loss: 0.6325370073318481\n","Step 450 Accuracy: 0.56810462474823 Loss: 0.620608925819397\n","Step 460 Accuracy: 0.5698138475418091 Loss: 0.6141393184661865\n","Step 470 Accuracy: 0.5701497197151184 Loss: 0.6315224170684814\n","Step 480 Accuracy: 0.5723851919174194 Loss: 0.6089451909065247\n","Step 490 Accuracy: 0.5745312571525574 Loss: 0.5918893814086914\n","Step 500 Accuracy: 0.5770527124404907 Loss: 0.5795231461524963\n","Step 510 Accuracy: 0.5797776579856873 Loss: 0.5809247493743896\n","Step 520 Accuracy: 0.5818101167678833 Loss: 0.581193208694458\n","Step 530 Accuracy: 0.5837673544883728 Loss: 0.5991429686546326\n","Step 540 Accuracy: 0.5849431753158569 Loss: 0.5943468809127808\n","Step 550 Accuracy: 0.5880301594734192 Loss: 0.5423546433448792\n","Step 560 Accuracy: 0.5908716917037964 Loss: 0.5818004012107849\n","Step 570 Accuracy: 0.5946928858757019 Loss: 0.5201285481452942\n","Step 580 Accuracy: 0.5961334705352783 Loss: 0.5941101908683777\n","\n","Next Epoch\n","Step 590 Accuracy: 0.5992187261581421 Loss: 0.4761236310005188\n","Step 600 Accuracy: 0.6015625 Loss: 0.5014097094535828\n","Step 610 Accuracy: 0.6039566397666931 Loss: 0.522483229637146\n","Step 620 Accuracy: 0.6071428656578064 Loss: 0.4829188585281372\n","Step 630 Accuracy: 0.6087646484375 Loss: 0.5435688495635986\n","Step 640 Accuracy: 0.612379789352417 Loss: 0.406215637922287\n","Step 650 Accuracy: 0.6138731241226196 Loss: 0.5065475702285767\n","Step 660 Accuracy: 0.6162546873092651 Loss: 0.4540196657180786\n","Step 670 Accuracy: 0.617302417755127 Loss: 0.5258601903915405\n","Step 680 Accuracy: 0.6193387508392334 Loss: 0.4596380591392517\n","Step 690 Accuracy: 0.6215401887893677 Loss: 0.45767486095428467\n","Step 700 Accuracy: 0.6229093074798584 Loss: 0.4729149341583252\n","Step 710 Accuracy: 0.6247829794883728 Loss: 0.4716281592845917\n","Step 720 Accuracy: 0.6274614930152893 Loss: 0.40600061416625977\n","Step 730 Accuracy: 0.6290118098258972 Loss: 0.46998441219329834\n","Step 740 Accuracy: 0.6311458349227905 Loss: 0.4603637456893921\n","Step 750 Accuracy: 0.6332237124443054 Loss: 0.48563626408576965\n","Step 760 Accuracy: 0.6347402334213257 Loss: 0.4758891761302948\n","Step 770 Accuracy: 0.6365184187889099 Loss: 0.49797236919403076\n","\n","Next Epoch\n","Step 780 Accuracy: 0.6385482549667358 Loss: 0.4257085919380188\n","Step 790 Accuracy: 0.640917956829071 Loss: 0.35801398754119873\n","Step 800 Accuracy: 0.6435185074806213 Loss: 0.3747052252292633\n","Step 810 Accuracy: 0.6461508870124817 Loss: 0.36113348603248596\n","Step 820 Accuracy: 0.6480609774589539 Loss: 0.38602471351623535\n","Step 830 Accuracy: 0.6497395634651184 Loss: 0.4171822965145111\n","Step 840 Accuracy: 0.6520220637321472 Loss: 0.34240037202835083\n","Step 850 Accuracy: 0.6545239686965942 Loss: 0.3510083556175232\n","Step 860 Accuracy: 0.656699001789093 Loss: 0.3394562602043152\n","Step 870 Accuracy: 0.6579368114471436 Loss: 0.45003044605255127\n","Step 880 Accuracy: 0.6598489880561829 Loss: 0.38938573002815247\n","Step 890 Accuracy: 0.6611111164093018 Loss: 0.42488330602645874\n","Step 900 Accuracy: 0.6627747416496277 Loss: 0.3541034758090973\n","Step 910 Accuracy: 0.66431725025177 Loss: 0.39984771609306335\n","Step 920 Accuracy: 0.6659946441650391 Loss: 0.3496192991733551\n","Step 930 Accuracy: 0.6682180762290955 Loss: 0.33191484212875366\n","Step 940 Accuracy: 0.6701480150222778 Loss: 0.34207144379615784\n","Step 950 Accuracy: 0.6714680790901184 Loss: 0.4340742230415344\n","Step 960 Accuracy: 0.673163652420044 Loss: 0.36132216453552246\n","Step 970 Accuracy: 0.6745854616165161 Loss: 0.35461026430130005\n","\n","Next Epoch\n"]}]},{"cell_type":"markdown","metadata":{"id":"yhVKe0voBVsk"},"source":["#Food for Thought\n","1. Group minibatches by size and pad them to the longest sequence length of the batch is less wasteful.  \n","2. Removing with a size that contains most reviews could be good, since the important words could be at the end or beginning. Truncating could also be good for the model to learn longer sequences. \n","3. (Learned) Embeddings could be used or tf-idf.\n","4. They both classify the inputs to two classes.\n","5. It makes sense to learn the initial state, because the model may need something from the state and it can encode, that the one is not important.\n","6. The output comes from the last time step so pre padding makes more sense to have a correct input at the last time step.\n","7. With an if else statement and a mask that could be possible.\n","8. That would be possible, but more expensive. It would be a many to many RNN with an average over the many outputs, but we can just use this many to one architecture."]},{"cell_type":"markdown","metadata":{"id":"eqIA9gDbILnK"},"source":["#Experiments\n","* lr = 0.001: 67% Accuracy  \n","* lr = 0.01: 50% Accuracy  \n","* lr = 0.1: Loss nan  \n","* Variable initialization [-0.5, 0.5], lr = 0.001: Loss nan\n","* max_len = 400: Loss nan\n"]}]}