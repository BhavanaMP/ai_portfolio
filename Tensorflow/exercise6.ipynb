{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise6.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bNFlDB6htO_4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UjgraYmcX7A"
      },
      "source": [
        "Team Memebers:\n",
        "Brinda Rao,\n",
        "Janusz Feigel,\n",
        "Bhavana Malla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF9p_AfDcVmK"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgxkqBIoXWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e9ec6d-ac80-490a-cf74-7e11ce5f2fe6"
      },
      "source": [
        "#prepare the train data\n",
        "max_words = 20000\n",
        "max_len = 200\n",
        "\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=max_words)\n",
        "\n",
        "def preprocess(sequences, labels):\n",
        "    return sequences, labels.astype(np.int32)\n",
        "\n",
        "train_sequences, train_labels = preprocess(train_sequences, train_labels)\n",
        "test_sequences, test_labels = preprocess(test_sequences, test_labels)\n",
        "\n",
        "def gen():\n",
        "    for sequence, label in zip(train_sequences, train_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(gen, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "buckets = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "bucket_batch_size = [32] * (len(buckets) + 1)\n",
        "train_data = train_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0],\n",
        "                                                  bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)\n",
        "#prepare the test data\n",
        "def gen_test():\n",
        "    for sequence, label in zip(test_sequences, test_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "test_data = tf.data.Dataset.from_generator(gen_test, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "test_data = test_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0], bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)\n",
        "\n",
        "#define the model\n",
        "# embedding, masking and keras LSTM\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(max_words, 128, mask_zero=True), \n",
        "                             tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                             tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                             tf.keras.layers.LSTM(32),\n",
        "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "#run the model \n",
        "history = model.fit(train_data, epochs=5)\n",
        "\n",
        "test_scores = model.evaluate(test_data)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "786/786 [==============================] - 126s 143ms/step - loss: 0.4246 - accuracy: 0.8059\n",
            "Epoch 2/5\n",
            "786/786 [==============================] - 111s 141ms/step - loss: 0.3329 - accuracy: 0.8599\n",
            "Epoch 3/5\n",
            "786/786 [==============================] - 111s 141ms/step - loss: 0.2672 - accuracy: 0.8980\n",
            "Epoch 4/5\n",
            "786/786 [==============================] - 114s 145ms/step - loss: 0.1605 - accuracy: 0.9441\n",
            "Epoch 5/5\n",
            "786/786 [==============================] - 111s 141ms/step - loss: 0.1157 - accuracy: 0.9621\n",
            "787/787 [==============================] - 53s 62ms/step - loss: 0.4077 - accuracy: 0.8541\n",
            "Test loss: 0.40771278738975525\n",
            "Test accuracy: 0.8541200160980225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNFlDB6htO_4"
      },
      "source": [
        "#Embedding and LSTM Experiments\n",
        "* Embedding 256, LSTM 128, LSTM 64, 5 epochs: 0.95 train acc, 0.82 test acc\n",
        "* Embedding 128, LSTM 32, LSTM 32, 5 epochs: 0.93 train acc, 0.86 test acc\n",
        "* Embedding 128, LSTM 32, LSTM 32, LSTM 32, 5 epochs: 0.97 train acc, 0.86 test acc\n",
        "* Embedding 256, LSTM 32, LSTM 32, LSTM 32, 5 epochs: 0.90 train acc, 0.86 test acc\n",
        "* Embedding 128, LSTM 128, LSTM 64, LSTM 32, 5 epochs: 0.92 train acc, 0.86 test acc\n",
        "* Embedding 128, LSTM 128, LSTM 128, LSTM 128, 5 epochs: 0.93 train acc, 0.86 test acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEd6hO5sEhab"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6C4QpyDEgXK",
        "outputId": "1a283bec-5f4d-4034-9dbb-b35b20fe6884"
      },
      "source": [
        "#prepare the train data\n",
        "max_words = 20000\n",
        "max_len = 200\n",
        "\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=max_words)\n",
        "\n",
        "def preprocess(sequences, labels):\n",
        "    return sequences, labels.astype(np.int32)\n",
        "\n",
        "train_sequences, train_labels = preprocess(train_sequences, train_labels)\n",
        "test_sequences, test_labels = preprocess(test_sequences, test_labels)\n",
        "\n",
        "def gen():\n",
        "    for sequence, label in zip(train_sequences, train_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(gen, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "buckets = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "bucket_batch_size = [32] * (len(buckets) + 1)\n",
        "train_data = train_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0],\n",
        "                                                  bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)\n",
        "#prepare the test data\n",
        "def gen_test():\n",
        "    for sequence, label in zip(test_sequences, test_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "test_data = tf.data.Dataset.from_generator(gen_test, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "test_data = test_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0], bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)\n",
        "\n",
        "#define the model\n",
        "# embedding, masking and keras LSTM\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(max_words, 128, mask_zero=True), \n",
        "                             tf.keras.layers.GRU(32, return_sequences=True),\n",
        "                             tf.keras.layers.GRU(32, return_sequences=True),\n",
        "                             tf.keras.layers.GRU(32),\n",
        "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "#run the model \n",
        "history = model.fit(train_data, epochs=5)\n",
        "\n",
        "test_scores = model.evaluate(test_data)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "786/786 [==============================] - 131s 150ms/step - loss: 0.4575 - accuracy: 0.7834\n",
            "Epoch 2/5\n",
            "786/786 [==============================] - 118s 150ms/step - loss: 0.2390 - accuracy: 0.9081\n",
            "Epoch 3/5\n",
            "786/786 [==============================] - 118s 150ms/step - loss: 0.1654 - accuracy: 0.9408\n",
            "Epoch 4/5\n",
            "786/786 [==============================] - 118s 150ms/step - loss: 0.0917 - accuracy: 0.9707\n",
            "Epoch 5/5\n",
            "786/786 [==============================] - 117s 149ms/step - loss: 0.0595 - accuracy: 0.9817\n",
            "787/787 [==============================] - 55s 65ms/step - loss: 0.4842 - accuracy: 0.8569\n",
            "Test loss: 0.48423057794570923\n",
            "Test accuracy: 0.8568800091743469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qof7xzDwIwL9"
      },
      "source": [
        "same speed, better train accuracy, same test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si7Udlh5IsPh"
      },
      "source": [
        "#Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nxu6gfjGH458",
        "outputId": "0c0f3d36-798d-4795-e864-69ce71f889d9"
      },
      "source": [
        "#prepare the train data\n",
        "max_words = 20000\n",
        "max_len = 200\n",
        "\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=max_words)\n",
        "\n",
        "def preprocess(sequences, labels):\n",
        "    return sequences, labels.astype(np.int32)\n",
        "\n",
        "train_sequences, train_labels = preprocess(train_sequences, train_labels)\n",
        "test_sequences, test_labels = preprocess(test_sequences, test_labels)\n",
        "\n",
        "def gen():\n",
        "    for sequence, label in zip(train_sequences, train_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(gen, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "buckets = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "bucket_batch_size = [32] * (len(buckets) + 1)\n",
        "train_data = train_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0],\n",
        "                                                  bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)\n",
        "#prepare the test data\n",
        "def gen_test():\n",
        "    for sequence, label in zip(test_sequences, test_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "test_data = tf.data.Dataset.from_generator(gen_test, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "test_data = test_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0], bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)\n",
        "\n",
        "#define the model\n",
        "# embedding, masking and keras LSTM\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(max_words, 128, mask_zero=True), \n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "#run the model \n",
        "history = model.fit(train_data, epochs=5)\n",
        "\n",
        "test_scores = model.evaluate(test_data)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "786/786 [==============================] - 266s 303ms/step - loss: 0.3648 - accuracy: 0.8322\n",
            "Epoch 2/5\n",
            "786/786 [==============================] - 237s 302ms/step - loss: 0.1737 - accuracy: 0.9360\n",
            "Epoch 3/5\n",
            "786/786 [==============================] - 236s 301ms/step - loss: 0.0918 - accuracy: 0.9682\n",
            "Epoch 4/5\n",
            "786/786 [==============================] - 237s 301ms/step - loss: 0.0780 - accuracy: 0.9716\n",
            "Epoch 5/5\n",
            "786/786 [==============================] - 237s 302ms/step - loss: 0.0459 - accuracy: 0.9838\n",
            "787/787 [==============================] - 105s 123ms/step - loss: 0.5764 - accuracy: 0.8508\n",
            "Test loss: 0.5764022469520569\n",
            "Test accuracy: 0.8508399724960327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yopd8BpLSNJn"
      },
      "source": [
        "2 times slower, better train accuracy, similar test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6qbAJj6SELP"
      },
      "source": [
        "#Without bucketing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA_E4Z2HSDNt",
        "outputId": "5d2bbfdb-9235-49c2-cb71-b0cdc9499783"
      },
      "source": [
        "#prepare the train data\n",
        "max_words = 20000\n",
        "max_len = 200\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=max_words)\n",
        "\n",
        "def preprocess(sequences, labels):\n",
        "    return sequences, labels.astype(np.int32)\n",
        "\n",
        "train_sequences, train_labels = preprocess(train_sequences, train_labels)\n",
        "test_sequences, test_labels = preprocess(test_sequences, test_labels)\n",
        "\n",
        "def gen():\n",
        "    for sequence, label in zip(train_sequences, train_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(gen, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "# within-batch padding\n",
        "train_data = train_data.padded_batch(32)\n",
        "\n",
        "#prepare the test data\n",
        "def gen_test():\n",
        "    for sequence, label in zip(test_sequences, test_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "test_data = tf.data.Dataset.from_generator(gen_test, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "# within-batch padding\n",
        "test_data = test_data.padded_batch(32)\n",
        "\n",
        "#define the model\n",
        "# embedding, masking and keras LSTM\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(max_words, 128, mask_zero=True), \n",
        "                             tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                             tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                             tf.keras.layers.LSTM(32),\n",
        "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "#run the model \n",
        "history = model.fit(train_data, epochs=5)\n",
        "\n",
        "test_scores = model.evaluate(test_data)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 261s 316ms/step - loss: 0.3897 - accuracy: 0.8240\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 250s 319ms/step - loss: 0.1977 - accuracy: 0.9279\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 249s 319ms/step - loss: 0.1305 - accuracy: 0.9548\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 251s 321ms/step - loss: 0.1106 - accuracy: 0.9613\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 251s 321ms/step - loss: 0.0918 - accuracy: 0.9700\n",
            "782/782 [==============================] - 111s 135ms/step - loss: 0.4819 - accuracy: 0.8528\n",
            "Test loss: 0.4819253981113434\n",
            "Test accuracy: 0.8527600169181824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX99eDTyzK0u"
      },
      "source": [
        "similar results, but 2 times slower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9ack_-OUV09"
      },
      "source": [
        "#Without bucketing and within-batch padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65TO_n8OUbng",
        "outputId": "960edc76-e0be-44d4-c269-132063a41689"
      },
      "source": [
        "#prepare the train data\n",
        "max_words = 20000\n",
        "max_len = 200\n",
        "\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=max_words)\n",
        "\n",
        "def preprocess(sequences, labels):\n",
        "    return sequences, labels.astype(np.int32)\n",
        "\n",
        "train_sequences, train_labels = preprocess(train_sequences, train_labels)\n",
        "test_sequences, test_labels = preprocess(test_sequences, test_labels)\n",
        "\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n",
        "train_data = train_data.shuffle(25000).batch(64)\n",
        "\n",
        "#prepare the test data\n",
        "#without bucketing and within-batch padding\n",
        "\n",
        "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_len)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_padded, test_labels))\n",
        "test_data = test_data.shuffle(25000).batch(64)\n",
        "#define the model\n",
        "# embedding, masking and keras LSTM\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(max_words, 128, mask_zero=True), \n",
        "                             tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                             tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                             tf.keras.layers.LSTM(32),\n",
        "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "#run the model \n",
        "history = model.fit(train_data, epochs=5)\n",
        "\n",
        "test_scores = model.evaluate(test_data)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 795s 2s/step - loss: 0.3914 - accuracy: 0.8211\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 775s 2s/step - loss: 0.2018 - accuracy: 0.9245\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 783s 2s/step - loss: 0.1114 - accuracy: 0.9602\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 786s 2s/step - loss: 0.0726 - accuracy: 0.9754\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 785s 2s/step - loss: 0.0520 - accuracy: 0.9829\n",
            "391/391 [==============================] - 60s 140ms/step - loss: 0.5163 - accuracy: 0.8469\n",
            "Test loss: 0.5163354277610779\n",
            "Test accuracy: 0.8468800187110901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6aNta44zboE"
      },
      "source": [
        "similar results, 8 times slower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDQxH2AHUcAd"
      },
      "source": [
        "#Without Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7JUe_wzUed4",
        "outputId": "4ff08d6b-3c9a-4998-f01a-182fd3d9ec11"
      },
      "source": [
        "#prepare the train data\n",
        "max_words = 20000\n",
        "max_len = 200\n",
        "\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=max_words)\n",
        "\n",
        "def preprocess(sequences, labels):\n",
        "    return sequences, labels.astype(np.int32)\n",
        "\n",
        "train_sequences, train_labels = preprocess(train_sequences, train_labels)\n",
        "test_sequences, test_labels = preprocess(test_sequences, test_labels)\n",
        "\n",
        "def gen():\n",
        "    for sequence, label in zip(train_sequences, train_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "train_data = tf.data.Dataset.from_generator(gen, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "buckets = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "bucket_batch_size = [32] * (len(buckets) + 1)\n",
        "train_data = train_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0],\n",
        "                                                  bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)\n",
        "#prepare the test data\n",
        "def gen_test():\n",
        "    for sequence, label in zip(test_sequences, test_labels):\n",
        "        yield sequence, label\n",
        "\n",
        "test_data = tf.data.Dataset.from_generator(gen_test, output_signature=(\n",
        "         tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "         tf.TensorSpec(shape=(), dtype=tf.int32)))\n",
        "\n",
        "#bucketing and within-batch padding\n",
        "test_data = test_data.bucket_by_sequence_length(lambda sequence, label: tf.shape(sequence)[0], bucket_boundaries=buckets, bucket_batch_sizes=bucket_batch_size)\n",
        "\n",
        "#define the model\n",
        "# embedding, masking and keras LSTM\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(max_words, 128, mask_zero=False), \n",
        "                             tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                             tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "                             tf.keras.layers.LSTM(32),\n",
        "                             tf.keras.layers.Dense(1, activation=\"sigmoid\")])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "#run the model \n",
        "history = model.fit(train_data, epochs=5)\n",
        "\n",
        "test_scores = model.evaluate(test_data)\n",
        "print(\"Test loss:\", test_scores[0])\n",
        "print(\"Test accuracy:\", test_scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "786/786 [==============================] - 126s 154ms/step - loss: 0.6389 - accuracy: 0.6022\n",
            "Epoch 2/5\n",
            "786/786 [==============================] - 118s 150ms/step - loss: 0.6796 - accuracy: 0.5158\n",
            "Epoch 3/5\n",
            "786/786 [==============================] - 116s 148ms/step - loss: 0.6945 - accuracy: 0.5031\n",
            "Epoch 4/5\n",
            "786/786 [==============================] - 114s 145ms/step - loss: 0.6936 - accuracy: 0.4972\n",
            "Epoch 5/5\n",
            "786/786 [==============================] - 114s 145ms/step - loss: 0.6938 - accuracy: 0.5023\n",
            "787/787 [==============================] - 58s 71ms/step - loss: 0.6931 - accuracy: 0.5000\n",
            "Test loss: 0.6931464672088623\n",
            "Test accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLOlgQaszHWd"
      },
      "source": [
        "similar speed, far worse results"
      ]
    }
  ]
}