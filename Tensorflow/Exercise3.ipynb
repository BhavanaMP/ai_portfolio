{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Exercise3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKQpUhCEPMMX"
      },
      "source": [
        "Group Details:\n",
        "\n",
        "*   Janusz Feigel\n",
        "*   Bhavana Malla\n",
        "*   Brinda Rao"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNavim6MtdQu",
        "outputId": "8565f8fd-bbcf-463d-8d67-833b654e9715"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w81E__8NtwXQ",
        "outputId": "8565620c-9696-422b-d53e-3f7e71e33bab"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/Introduction to Deep Learning')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Introduction to Deep Learning'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4tLIHDMtx_W"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import MNISTDataset\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras import backend as k\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfyEPG2z1sy9"
      },
      "source": [
        "def load_dataset_mnist():\n",
        "\t# load dataset\n",
        "  \n",
        "  (trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "\t  # reshape dataset to have a single channel\n",
        "  if k.image_data_format() == 'channels_first':\n",
        "   trainX = trainX.reshape(trainX.shape[0], 1, img_rows, img_cols)\n",
        "   testX = testX.reshape(testX.shape[0], 1, img_rows, img_cols)\n",
        "   inpx = (1, img_rows, img_cols)\n",
        "  else:\n",
        "   trainX = trainX.reshape(trainX.shape[0], img_rows, img_cols, 1)\n",
        "   testX = testX.reshape(testX.shape[0], img_rows, img_cols, 1)\n",
        "   inpx = (img_rows, img_cols, 1)\n",
        "\t  # one hot encode target values\n",
        "  #trainY = to_categorical(trainY,10)\n",
        "  #testY = to_categorical(testY,10)\n",
        "  return inpx,trainX, trainY, testX, testY\n",
        "\n",
        "def prepare_dataset(trainX, trainY, testX, testY):\n",
        "\n",
        "# Prepare the training dataset.\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainY))\n",
        "  train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)\n",
        "\n",
        "#Prepare the test dataset\n",
        "  test_dataset =  tf.data.Dataset.from_tensor_slices((testX, testY))\n",
        "  test_dataset = test_dataset.batch(128)\n",
        "\n",
        "  return train_dataset,test_dataset\n",
        "\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "def define_model_mnist():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(28,28,1)))\n",
        "  model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2),padding='same'))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-VcAAyF21ou"
      },
      "source": [
        "\t# compile model\n",
        "\topt = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\tloss_fn = keras.losses.SparseCategoricalCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2kd-9zZVz3p"
      },
      "source": [
        "# MNIST **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjdfOOCy4NUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3824c1-b1ba-4ef1-c0dd-ebb7f60a1a1b"
      },
      "source": [
        "# load dataset\n",
        "img_rows, img_cols=28, 28\n",
        "mnist = tf.keras.datasets.mnist\n",
        "inpx, trainX, trainY, testX, testY = load_dataset_mnist()\n",
        "#prepare dataset\n",
        "train_dataset,test_dataset = prepare_dataset(trainX, trainY, testX, testY)\n",
        "# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)\n",
        "# define model\n",
        "model = define_model_mnist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDWY5UOOChcn"
      },
      "source": [
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYeY6Ii8H4ft"
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    test_logits = model(x, training=False)\n",
        "    test_acc_metric.update_state(y, test_logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXRJ1__n1xLZ",
        "outputId": "02f45988-87a9-49c2-b5e0-5e4578fe0180"
      },
      "source": [
        "import time\n",
        "\n",
        "epochs = 25\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in test_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    test_acc = test_acc_metric.result()\n",
        "    test_acc_metric.reset_states()\n",
        "print(\"Validation acc: %.4f\" % (float(test_acc),))\n",
        "print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 0.0056\n",
            "Training loss (for one batch) at step 200: 0.0805\n",
            "Training loss (for one batch) at step 400: 0.0023\n",
            "Training acc over epoch: 0.9930\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.0102\n",
            "Training loss (for one batch) at step 200: 0.0206\n",
            "Training loss (for one batch) at step 400: 0.1133\n",
            "Training acc over epoch: 0.9932\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 0.0049\n",
            "Training loss (for one batch) at step 200: 0.0045\n",
            "Training loss (for one batch) at step 400: 0.0503\n",
            "Training acc over epoch: 0.9921\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 0.0397\n",
            "Training loss (for one batch) at step 200: 0.0063\n",
            "Training loss (for one batch) at step 400: 0.0030\n",
            "Training acc over epoch: 0.9933\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 0.0108\n",
            "Training loss (for one batch) at step 200: 0.0290\n",
            "Training loss (for one batch) at step 400: 0.0001\n",
            "Training acc over epoch: 0.9933\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 0.0977\n",
            "Training loss (for one batch) at step 200: 0.0101\n",
            "Training loss (for one batch) at step 400: 0.0158\n",
            "Training acc over epoch: 0.9929\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 0.0942\n",
            "Training loss (for one batch) at step 200: 0.0516\n",
            "Training loss (for one batch) at step 400: 0.0008\n",
            "Training acc over epoch: 0.9934\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 0.0483\n",
            "Training loss (for one batch) at step 200: 0.0875\n",
            "Training loss (for one batch) at step 400: 0.0321\n",
            "Training acc over epoch: 0.9937\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 0.0545\n",
            "Training loss (for one batch) at step 200: 0.0321\n",
            "Training loss (for one batch) at step 400: 0.0044\n",
            "Training acc over epoch: 0.9944\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 0.0033\n",
            "Training loss (for one batch) at step 200: 0.0007\n",
            "Training loss (for one batch) at step 400: 0.0613\n",
            "Training acc over epoch: 0.9947\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss (for one batch) at step 0: 0.0207\n",
            "Training loss (for one batch) at step 200: 0.0019\n",
            "Training loss (for one batch) at step 400: 0.0023\n",
            "Training acc over epoch: 0.9942\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss (for one batch) at step 0: 0.0841\n",
            "Training loss (for one batch) at step 200: 0.0108\n",
            "Training loss (for one batch) at step 400: 0.0095\n",
            "Training acc over epoch: 0.9940\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss (for one batch) at step 0: 0.0139\n",
            "Training loss (for one batch) at step 200: 0.0226\n",
            "Training loss (for one batch) at step 400: 0.0754\n",
            "Training acc over epoch: 0.9941\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss (for one batch) at step 0: 0.0208\n",
            "Training loss (for one batch) at step 200: 0.0026\n",
            "Training loss (for one batch) at step 400: 0.0662\n",
            "Training acc over epoch: 0.9948\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss (for one batch) at step 0: 0.1277\n",
            "Training loss (for one batch) at step 200: 0.0388\n",
            "Training loss (for one batch) at step 400: 0.0083\n",
            "Training acc over epoch: 0.9956\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss (for one batch) at step 0: 0.0600\n",
            "Training loss (for one batch) at step 200: 0.0370\n",
            "Training loss (for one batch) at step 400: 0.0067\n",
            "Training acc over epoch: 0.9954\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss (for one batch) at step 0: 0.0067\n",
            "Training loss (for one batch) at step 200: 0.0232\n",
            "Training loss (for one batch) at step 400: 0.0019\n",
            "Training acc over epoch: 0.9956\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss (for one batch) at step 0: 0.0027\n",
            "Training loss (for one batch) at step 200: 0.0809\n",
            "Training loss (for one batch) at step 400: 0.0005\n",
            "Training acc over epoch: 0.9952\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss (for one batch) at step 0: 0.0318\n",
            "Training loss (for one batch) at step 200: 0.0002\n",
            "Training loss (for one batch) at step 400: 0.0037\n",
            "Training acc over epoch: 0.9944\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss (for one batch) at step 0: 0.0011\n",
            "Training loss (for one batch) at step 200: 0.0178\n",
            "Training loss (for one batch) at step 400: 0.0927\n",
            "Training acc over epoch: 0.9947\n",
            "\n",
            "Start of epoch 20\n",
            "Training loss (for one batch) at step 0: 0.0229\n",
            "Training loss (for one batch) at step 200: 0.0805\n",
            "Training loss (for one batch) at step 400: 0.0007\n",
            "Training acc over epoch: 0.9956\n",
            "\n",
            "Start of epoch 21\n",
            "Training loss (for one batch) at step 0: 0.0146\n",
            "Training loss (for one batch) at step 200: 0.0008\n",
            "Training loss (for one batch) at step 400: 0.0004\n",
            "Training acc over epoch: 0.9960\n",
            "\n",
            "Start of epoch 22\n",
            "Training loss (for one batch) at step 0: 0.0128\n",
            "Training loss (for one batch) at step 200: 0.0111\n",
            "Training loss (for one batch) at step 400: 0.0227\n",
            "Training acc over epoch: 0.9957\n",
            "\n",
            "Start of epoch 23\n",
            "Training loss (for one batch) at step 0: 0.1821\n",
            "Training loss (for one batch) at step 200: 0.0003\n",
            "Training loss (for one batch) at step 400: 0.0682\n",
            "Training acc over epoch: 0.9955\n",
            "\n",
            "Start of epoch 24\n",
            "Training loss (for one batch) at step 0: 0.0376\n",
            "Training loss (for one batch) at step 200: 0.0084\n",
            "Training loss (for one batch) at step 400: 0.0096\n",
            "Training acc over epoch: 0.9956\n",
            "Validation acc: 0.9907\n",
            "Time taken: 4.51s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy0Y5S0fUIrn"
      },
      "source": [
        "Using SGD the o/p after two epochs:\n",
        "\n",
        "Training acc over epoch: 0.9381\n",
        "Validation acc: 0.9495"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjDgRkwGV6ax"
      },
      "source": [
        "# CIFAR **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDxOfjsJbDfY"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "def load_dataset_cifar():\n",
        "  # load dataset\n",
        "  (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "  #trainY = to_categorical(trainY)\n",
        "  #testY = to_categorical(testY)\n",
        "  return trainX, trainY, testX, testY\n",
        "\n",
        "\n",
        "def prepare_dataset(trainX, trainY, testX, testY):\n",
        "\n",
        "# Prepare the training dataset.\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainY))\n",
        "  train_dataset = train_dataset.shuffle(buffer_size=50000).batch(128)\n",
        "\n",
        "#Prepare the test dataset\n",
        "  test_dataset =  tf.data.Dataset.from_tensor_slices((testX, testY))\n",
        "  test_dataset = test_dataset.batch(128)\n",
        "\n",
        "  return train_dataset,test_dataset\n",
        "\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "def define_model_cifar():\n",
        " \n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32,32,3)))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "def define_model2():\n",
        "\n",
        "  data_augmentation = keras.Sequential(\n",
        "  [layers.RandomFlip(\"horizontal\", input_shape=(32,32,3)),\n",
        "   layers.RandomRotation(0.1),\n",
        "   layers.RandomZoom(0.1)]\n",
        "  )\n",
        "\n",
        "\n",
        "  model = tf.keras.Sequential(\n",
        "    [data_augmentation,\n",
        "     layers.Input(shape=(32,32,3)),\n",
        "     layers.Conv2D(64, 7, padding='same', activation='relu'),\n",
        "     layers.MaxPooling2D(padding='same'),\n",
        "     layers.Conv2D(128, 5, padding='same', activation='relu'),\n",
        "     layers.MaxPooling2D(padding='same'),\n",
        "     layers.Conv2D(128, 5, padding='same', activation='relu'),\n",
        "     layers.MaxPooling2D(padding='same'),\n",
        "     layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
        "     layers.MaxPooling2D(padding='same'),\n",
        "     layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
        "     layers.MaxPooling2D(padding='same'),\n",
        "     layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
        "     layers.MaxPooling2D(padding='same'),\n",
        "     layers.Conv2D(512, 3, padding='same', activation='relu'),\n",
        "     layers.MaxPooling2D(padding='same'),\n",
        "     layers.Flatten(),\n",
        "     layers.Dropout(.2),\n",
        "     layers.Dense(512, activation='relu'),\n",
        "     layers.Dense(10)     \n",
        "    ]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    opt.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "    \n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    test_logits = model(x, training=False)\n",
        "    test_acc_metric.update_state(y, test_logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_-bP9-6V_Sk"
      },
      "source": [
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset_cifar()\n",
        "#prepare dataset\n",
        "train_dataset,test_dataset = prepare_dataset(trainX, trainY, testX, testY)\n",
        "# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)\n",
        "# define model\n",
        "model = define_model_cifar()\n",
        "# compile model\n",
        "opt = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24-F023Ral73"
      },
      "source": [
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "test_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAXZ2_X5arnu",
        "outputId": "fd90a06b-2c8c-4c1a-8fb8-5bc08af3f2b6"
      },
      "source": [
        "import time\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in test_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = test_acc_metric.result()\n",
        "    test_acc_metric.reset_states()\n",
        "print(\"Test acc: %.4f\" % (float(val_acc),))\n",
        "print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 575.9200\n",
            "Training loss (for one batch) at step 200: 1.8255\n",
            "Training acc over epoch: 0.2700\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 1.5836\n",
            "Training loss (for one batch) at step 200: 1.7260\n",
            "Training acc over epoch: 0.4183\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 1.5391\n",
            "Training loss (for one batch) at step 200: 1.3596\n",
            "Training acc over epoch: 0.4869\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 1.3958\n",
            "Training loss (for one batch) at step 200: 1.3891\n",
            "Training acc over epoch: 0.5327\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 1.1006\n",
            "Training loss (for one batch) at step 200: 1.1597\n",
            "Training acc over epoch: 0.5678\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 1.0242\n",
            "Training loss (for one batch) at step 200: 1.0455\n",
            "Training acc over epoch: 0.5955\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 1.1376\n",
            "Training loss (for one batch) at step 200: 1.1275\n",
            "Training acc over epoch: 0.6249\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 0.9293\n",
            "Training loss (for one batch) at step 200: 0.8597\n",
            "Training acc over epoch: 0.6481\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 0.8444\n",
            "Training loss (for one batch) at step 200: 0.9646\n",
            "Training acc over epoch: 0.6654\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 0.9372\n",
            "Training loss (for one batch) at step 200: 1.0788\n",
            "Training acc over epoch: 0.6869\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss (for one batch) at step 0: 0.8842\n",
            "Training loss (for one batch) at step 200: 0.9768\n",
            "Training acc over epoch: 0.6997\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss (for one batch) at step 0: 0.6593\n",
            "Training loss (for one batch) at step 200: 0.7537\n",
            "Training acc over epoch: 0.7145\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss (for one batch) at step 0: 0.7976\n",
            "Training loss (for one batch) at step 200: 0.8163\n",
            "Training acc over epoch: 0.7322\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss (for one batch) at step 0: 0.6968\n",
            "Training loss (for one batch) at step 200: 0.7945\n",
            "Training acc over epoch: 0.7407\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss (for one batch) at step 0: 0.6014\n",
            "Training loss (for one batch) at step 200: 0.6531\n",
            "Training acc over epoch: 0.7553\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss (for one batch) at step 0: 0.5832\n",
            "Training loss (for one batch) at step 200: 0.5700\n",
            "Training acc over epoch: 0.7685\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss (for one batch) at step 0: 0.5423\n",
            "Training loss (for one batch) at step 200: 0.6208\n",
            "Training acc over epoch: 0.7777\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss (for one batch) at step 0: 0.6269\n",
            "Training loss (for one batch) at step 200: 0.6407\n",
            "Training acc over epoch: 0.7838\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss (for one batch) at step 0: 0.5146\n",
            "Training loss (for one batch) at step 200: 0.7462\n",
            "Training acc over epoch: 0.7952\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss (for one batch) at step 0: 0.4691\n",
            "Training loss (for one batch) at step 200: 0.6698\n",
            "Training acc over epoch: 0.8021\n",
            "\n",
            "Start of epoch 20\n",
            "Training loss (for one batch) at step 0: 0.5541\n",
            "Training loss (for one batch) at step 200: 0.5981\n",
            "Training acc over epoch: 0.8113\n",
            "\n",
            "Start of epoch 21\n",
            "Training loss (for one batch) at step 0: 0.3594\n",
            "Training loss (for one batch) at step 200: 0.5282\n",
            "Training acc over epoch: 0.8186\n",
            "\n",
            "Start of epoch 22\n",
            "Training loss (for one batch) at step 0: 0.4539\n",
            "Training loss (for one batch) at step 200: 0.4693\n",
            "Training acc over epoch: 0.8235\n",
            "\n",
            "Start of epoch 23\n",
            "Training loss (for one batch) at step 0: 0.5195\n",
            "Training loss (for one batch) at step 200: 0.4524\n",
            "Training acc over epoch: 0.8315\n",
            "\n",
            "Start of epoch 24\n",
            "Training loss (for one batch) at step 0: 0.4007\n",
            "Training loss (for one batch) at step 200: 0.5132\n",
            "Training acc over epoch: 0.8380\n",
            "\n",
            "Start of epoch 25\n",
            "Training loss (for one batch) at step 0: 0.4803\n",
            "Training loss (for one batch) at step 200: 0.5092\n",
            "Training acc over epoch: 0.8429\n",
            "\n",
            "Start of epoch 26\n",
            "Training loss (for one batch) at step 0: 0.2691\n",
            "Training loss (for one batch) at step 200: 0.4316\n",
            "Training acc over epoch: 0.8469\n",
            "\n",
            "Start of epoch 27\n",
            "Training loss (for one batch) at step 0: 0.5068\n",
            "Training loss (for one batch) at step 200: 0.2793\n",
            "Training acc over epoch: 0.8551\n",
            "\n",
            "Start of epoch 28\n",
            "Training loss (for one batch) at step 0: 0.2148\n",
            "Training loss (for one batch) at step 200: 0.3993\n",
            "Training acc over epoch: 0.8565\n",
            "\n",
            "Start of epoch 29\n",
            "Training loss (for one batch) at step 0: 0.2546\n",
            "Training loss (for one batch) at step 200: 0.6161\n",
            "Training acc over epoch: 0.8644\n",
            "\n",
            "Start of epoch 30\n",
            "Training loss (for one batch) at step 0: 0.5650\n",
            "Training loss (for one batch) at step 200: 0.3535\n",
            "Training acc over epoch: 0.8672\n",
            "\n",
            "Start of epoch 31\n",
            "Training loss (for one batch) at step 0: 0.3362\n",
            "Training loss (for one batch) at step 200: 0.4429\n",
            "Training acc over epoch: 0.8744\n",
            "\n",
            "Start of epoch 32\n",
            "Training loss (for one batch) at step 0: 0.2961\n",
            "Training loss (for one batch) at step 200: 0.3702\n",
            "Training acc over epoch: 0.8764\n",
            "\n",
            "Start of epoch 33\n",
            "Training loss (for one batch) at step 0: 0.2206\n",
            "Training loss (for one batch) at step 200: 0.2666\n",
            "Training acc over epoch: 0.8807\n",
            "\n",
            "Start of epoch 34\n",
            "Training loss (for one batch) at step 0: 0.3055\n",
            "Training loss (for one batch) at step 200: 0.3875\n",
            "Training acc over epoch: 0.8796\n",
            "\n",
            "Start of epoch 35\n",
            "Training loss (for one batch) at step 0: 0.3763\n",
            "Training loss (for one batch) at step 200: 0.3613\n",
            "Training acc over epoch: 0.8882\n",
            "\n",
            "Start of epoch 36\n",
            "Training loss (for one batch) at step 0: 0.2009\n",
            "Training loss (for one batch) at step 200: 0.3717\n",
            "Training acc over epoch: 0.8886\n",
            "\n",
            "Start of epoch 37\n",
            "Training loss (for one batch) at step 0: 0.3609\n",
            "Training loss (for one batch) at step 200: 0.3374\n",
            "Training acc over epoch: 0.8911\n",
            "\n",
            "Start of epoch 38\n",
            "Training loss (for one batch) at step 0: 0.1698\n",
            "Training loss (for one batch) at step 200: 0.2338\n",
            "Training acc over epoch: 0.8924\n",
            "\n",
            "Start of epoch 39\n",
            "Training loss (for one batch) at step 0: 0.3084\n",
            "Training loss (for one batch) at step 200: 0.4193\n",
            "Training acc over epoch: 0.8969\n",
            "\n",
            "Start of epoch 40\n",
            "Training loss (for one batch) at step 0: 0.3255\n",
            "Training loss (for one batch) at step 200: 0.3926\n",
            "Training acc over epoch: 0.8997\n",
            "\n",
            "Start of epoch 41\n",
            "Training loss (for one batch) at step 0: 0.1630\n",
            "Training loss (for one batch) at step 200: 0.3858\n",
            "Training acc over epoch: 0.8983\n",
            "\n",
            "Start of epoch 42\n",
            "Training loss (for one batch) at step 0: 0.2557\n",
            "Training loss (for one batch) at step 200: 0.2018\n",
            "Training acc over epoch: 0.9065\n",
            "\n",
            "Start of epoch 43\n",
            "Training loss (for one batch) at step 0: 0.1967\n",
            "Training loss (for one batch) at step 200: 0.1893\n",
            "Training acc over epoch: 0.9037\n",
            "\n",
            "Start of epoch 44\n",
            "Training loss (for one batch) at step 0: 0.1932\n",
            "Training loss (for one batch) at step 200: 0.2072\n",
            "Training acc over epoch: 0.9081\n",
            "\n",
            "Start of epoch 45\n",
            "Training loss (for one batch) at step 0: 0.2091\n",
            "Training loss (for one batch) at step 200: 0.3329\n",
            "Training acc over epoch: 0.9063\n",
            "\n",
            "Start of epoch 46\n",
            "Training loss (for one batch) at step 0: 0.1734\n",
            "Training loss (for one batch) at step 200: 0.3038\n",
            "Training acc over epoch: 0.9070\n",
            "\n",
            "Start of epoch 47\n",
            "Training loss (for one batch) at step 0: 0.2643\n",
            "Training loss (for one batch) at step 200: 0.1411\n",
            "Training acc over epoch: 0.9111\n",
            "\n",
            "Start of epoch 48\n",
            "Training loss (for one batch) at step 0: 0.2620\n",
            "Training loss (for one batch) at step 200: 0.2646\n",
            "Training acc over epoch: 0.9110\n",
            "\n",
            "Start of epoch 49\n",
            "Training loss (for one batch) at step 0: 0.1401\n",
            "Training loss (for one batch) at step 200: 0.2575\n",
            "Training acc over epoch: 0.9114\n",
            "Test acc: 0.7613\n",
            "Time taken: 10.09s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_MO6jz2FRNb"
      },
      "source": [
        "## **Experiments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mbem9mAzsFi"
      },
      "source": [
        "## **For CIFAR Dataset**\n",
        "\n",
        "Accuracy using different dropout values\n",
        "**dropout -0.2**\n",
        "Training acc over epoch: **0.8738**\n",
        "Validation acc: **0.7098**\n",
        "\n",
        "**dropout-0.3**\n",
        "Training acc over epoch: **0.8369**\n",
        "Test acc: **0.7333**\n",
        "\n",
        "**dropout-0.4**---model with 3 convolutional layer\n",
        "Training acc over epoch: **0.9114**\n",
        "Test acc: **0.7613**\n",
        "\n",
        "\n",
        "\n",
        "1.   When using dropout after maxpooling, the accuracy is very low, as low as 20%\n",
        "\n",
        "2.   We've had much overfitting with around 30 % better train accuracy than test accuracy, so we added dropout and data augmentation. The results were a little worse for train accuracy, but asignificantly better test accuracy, which was around 20% higher than before.  \n",
        "\n",
        "1.   We tried different layer sizes and came to the conclusion, that more layers help a little bit.  \n",
        "\n",
        "2.   We tried different output channel sizes and came to the conclusion, that more output channels help a little bit\n",
        "\n",
        "1.   We tried different kernel sizes and had some good results with first 7 then 5 then for the remaining layers 3, as the kernel size.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAQcow-dFY9X"
      },
      "source": [
        "\n",
        "# **For MNIST Dataset**\n",
        "1.   For (1,1) filter size,the train accuracy is 86% and the test accuracy is 88%\n",
        "2.   For (28,28) filter size,the train accuracy is 98% and the test accuracy is around 98%\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPtvRvbC5oL6"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "\n",
        "\n",
        "1. Where to use the dropout ??\n",
        "2. What is best dropout to use?\n",
        "\n",
        "\n"
      ]
    }
  ]
}