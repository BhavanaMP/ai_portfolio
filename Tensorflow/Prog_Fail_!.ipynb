{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Prog_Fail_!.ipynb","provenance":[],"authorship_tag":"ABX9TyMtLWYFS2x6lNmiY+kH/ezC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pf-jOTZ2bKVa","executionInfo":{"status":"ok","timestamp":1635685055301,"user_tz":-60,"elapsed":28565,"user":{"displayName":"Bhavana Malla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgW9fuKIX4V4LhhDZMcMHCsHkLgEQvdkoeylzrzwQ=s64","userId":"13569433150164017713"}},"outputId":"0b6572c8-6e94-4052-acc2-f68ab518ba18"},"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 8\n","w_range = 0.4\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0] + layers[-1][1])\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","Loss: 171.59609985351562 Accuracy: 0.0\n","Loss: nan Accuracy: 0.078125\n","Loss: nan Accuracy: 0.046875\n","Loss: nan Accuracy: 0.1328125\n","Loss: nan Accuracy: 0.1015625\n","Loss: nan Accuracy: 0.09375\n","Loss: nan Accuracy: 0.0703125\n","Loss: nan Accuracy: 0.0703125\n","Loss: nan Accuracy: 0.109375\n","Loss: nan Accuracy: 0.09375\n","Loss: nan Accuracy: 0.0703125\n","Loss: nan Accuracy: 0.0859375\n","Loss: nan Accuracy: 0.078125\n","Loss: nan Accuracy: 0.0859375\n","Loss: nan Accuracy: 0.125\n","Loss: nan Accuracy: 0.09375\n","Loss: nan Accuracy: 0.1171875\n","Loss: nan Accuracy: 0.09375\n","Loss: nan Accuracy: 0.078125\n","Loss: nan Accuracy: 0.15625\n","Loss: nan Accuracy: 0.1328125\n","Final test accuracy: 0.09799999743700027\n"]}]}]}