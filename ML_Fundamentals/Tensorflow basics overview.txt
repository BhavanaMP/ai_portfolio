tensorflow
- tf.random.set_seed(1234)
- tf.autograph.set_verbosity(0)
- tf.keras.backend.set_floatx("float64")
- tf.get_logger().setLevel("ERROR")
- tf.linalg.matmul() #matrix multiplication for a vector
- tf.reduce_sum() #performs sum of values in a vector
	#collaborative filter cost function
	error = ((tf.linalg.matmul(X, tf.transpose(W)) + b) - Y) * R
    J = 0.5 * tf.reduce_sum(error ** 2) + (lambda_ / 2) * (tf.reduce_sum(W ** 2) +  tf.reduce_sum(X ** 2))
- tf.Variable #to initialise trainable parameters that are to be optimized like params w, b
	num_features = 100
	num_movies, num_users = Y.shape
	X = tf.Variable(tf.random.normal((num_movies, num_features), dtype=tf.float64), name="X")
	W = tf.Variable(tf.random.normal((num_users, num_features), dtype=tf.float64), name="W")
	b = tf.Variable(tf.random.normal((1, num_users), dtype=tf.float64), name="b")
- Auto diff 
	(performs auto differentation of cost function w.r.to parameters i.e dj/dw, dj/db . we just need to speicfy the cost function in tape to let it watch/record the operations
	with tf.GradientTape(persistent=False, watch_accessed_variables=True) as tape:
		#specify cost function
		# Compute the cost (forward pass included in cost)
        cost_value = cofi_cost_func_v(X, W, b, Ynorm, R, lambda_)
	gradients = tape.gradients(cost_value, [X,W,b]) #dj/dw, dj/db, dj/dx for collaborative filtering
- optimizer to apply gradient inorder to update the params
	optimizer = tf.keras.optimizers.Adam(learning_rate=1e-1)
	optimizer.apply_gradients( zip(grads, [X,W,b]) )
- sorted_idx = tf.argsort(my_predictions, direction="DESCENDING") #sort the indexes based on the value of my_predictions
- tf.nn.sigmoid(probabilitylogits), tf.math.sigmoid(yhatprobabilitylogits)
- tf.nn.softmax(probabilityvalues)
- tensorflowvar.numpy() #to convert the tensor to numpy
- l2 normalize and keras functional api
	num_outputs = 32
	tf.random.set_seed(1)
	user_NN = tf.keras.models.Sequential([
		### START CODE HERE ###   
		tf.keras.layers.Dense(256, activation='relu'),
		tf.keras.layers.Dense(128, activation='relu'),
		tf.keras.layers.Dense(num_outputs, activation='linear'),
		### END CODE HERE ###  
	]) 
	# create the user input and point to the base network
	input_user = tf.keras.layers.Input(shape=(x_user.shape[1]))
	vu = user_NN(input_user)
	vu = tf.linalg.l2_normalize(vu, axis=1) #l2 normalization
	
- Compute the dot product of the two vectors vu and vm
	output = tf.keras.layers.Dot(axes=1)([vu, vm]) #model output vectors
	
- Specify the inputs and output of the model
	model = tf.keras.Model(inputs=[input_user, input_item], outputs=output) #here input_item is another neural network arch input with same number of outputs
- model.summary()
- Setting the cost function and optimizer
  tf.random.set_seed(1)
  cost_fn = tf.keras.losses.MeanSquaredError()
  opt = tf.keras.optimizers.Adam(learning_rate=0.01)
  model.compile(optimizer=opt, loss=cost_fn)
- model.fit([user_train[:, u_s_c_idx:], item_train[:, i_s_c_idx:]], ytrain_scaled, epochs=30) #sending 2 neural network input at the same time, true target
- model.evaluate([user_test[:, u_s_c_idx:], item_test[:, i_s_c_idx:]], ynorm_test) #evaluate the model on validation data if you have, else on test data
- j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R #broadcasts if any of the vector rows or columns are equal, while dot product needs the inner order of 2 matrices to be same
- J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))
- tf.linalg.norm(v1 - v2) #sqrt of sum of squares # this is also equal euclidean distance of v1 and v2
- max = tf.reduce_max(target_q_neuralnetwork(input_vector), axis=-1) #max value from the output of a neural network 
- gather_nd() to access the tensor using indexes, tensor slicing
	import tensorflow as tf
	sess = tf.Session()
	x = [[1,2,3],[4,5,6]] #imagine this as a tensor
	y = tf.gather_nd(x, [[1,1],[1,2]]) #(tensor, indices to access the slices of tensor)
	print(sess.run(y)) # [5, 6]
	[1, 1] -> represents tensor 1 index 1
	[1, 2] -> represents tensor 1 index 2
	
	#another example
	q_values = q_network(states)
    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),
                                                tf.cast(actions, tf.int32)], axis=1))
- keras normalization layer (standard normalization)
	norm_layer = tf.keras.layers.Normalization(axis=-1)
	norm_layer.adapt(X_train) # learns mean, variance
	X_scaled = norm_layer(X_train)
	
	X_test_scaled = norm_layer(X_test)
	y_pred = model.predict(X_test_scaled)
	
- from tensorflow.python.ops.resource_variable_ops import ResourceVariable
  from tensorflow.python.framework.ops import EagerTensor