Python Basics -3
----
Obtaining Timeseries Datasets
- pandas_datareader for extracting info from yahoo Finance for financial market data, World Bank for global development data, and St. Louis Fed (FRED) for economic data for inflation, Tiingo, The Investors Exchange(IEX), Alpha Vantage, Econdb, Enigma, Quandl (pdr.DataReader('GDP', data_source='fred', start=start_time, end=end_time))((population_df.dropna()
                 .sort_values("SP.POP.TOTL")
                 .iloc[-25:]
                 .reset_index()))
- requests library -  Fetching Data Using Web API ( request.get(url), response.status_code, response.json())
- construct a query url for requests library, pd.DataFrame.from_dict(population), bar char of million population
- Generating Synthetic Data Using NumPy, synthetic autoregressive (AR) time-series data, markov process, adding noise to the data, @ to perform dot product, making index of the dataframe as time - synthetic.index = pd.date_range(start="2021-07-01", periods=len(ar_data), freq="D")
- Web crawling of html pages using requests library, beautifulsoup4 with lxml parser or with CSS selectors, lxml with XPath language using document object model (DOM) with xpath() function, etree.HTML(response.text), inspecting the HTML source, response.headers, response.text to get the html data, csv data, textual data from the url, BeautifulSoup(response.text, "lxml")
    (csvtext = response.text
    csv_buffer = StringIO(csvtext)
    csv_df = pd.read_csv(csv_buffer)
-- web crawling using requests -  for reading JSON format data using response.json()
-- web crawling using requests -  for reading binary data using response.content
- Reading Tables on the Web Using Pandas with read_html() 
    specify headers to solve the forbidden access
     # user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
    # headers={'User-Agent':user_agent} 
- Reading Dynamic Content With Selenium, reading webpages written in javascript using selenium by creating or mimicing the browser's behavior, use of Chromedriver, launch the browser in headless mode
- Using urllib.request for webscraping (urllib.request.Request(url, None, headers), urllib.request.urlopen(request))
- web scraping and web crawling, spiders, bots
- Web Crawling using scrapy
- Scrapy Shell (scrapy shell, fetch("url"), view(response), print(response.text)), to extract information from HTML based on css selectors like class, id  using response.css(".title::text").extract() or .extract_first(), response.css("time::attr(title)").extract(), response.css(".score.unvoted::text").extract_first() when we have two classes to the same css element
- Multiprocessing
- Process and Thread
- Multiprocessing with arguments
- multiprocesses with arguments using pool, map and concurrent.futures
- Parallel processing with joblib
- Web Frameworks in Python, popular web frameworks
- Python and the Web
- Flask for Web API applications
- API and endpoint
- Dash for Interactive Widgets
- Dash - Lenet Handwriting recognition
- Polling in Dash, Dash with polling - LeNet 
- Combining Flask and Dash
- Jupyter-Dash
- Python classes , instantiation, constructor, inheritance, __name__ == __main__, sys.argv(), keras call backs
- Command line arguments for running a python script , argparse.ArgumentParser(), Power of command line args
- Alternative to command line arguments - using environment variables, using config files with configParser