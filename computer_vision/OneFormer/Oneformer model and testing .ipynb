{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM7XbdWNZsHmc0Jruvj0KCe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d58146d581e94adb9ca1f1dc106d70e9":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_a4f04f4db6f340829fcfe947b1750f15","IPY_MODEL_d4a703fa18f640efb88eff8ac743164a","IPY_MODEL_e1a843b60cb349779621f5b940470ec9","IPY_MODEL_c2e72d75ec694954a7ef23cbdaffdb08","IPY_MODEL_e70a2d8124274bfeba8d554b2884836f","IPY_MODEL_12dd72e8d1b741daad185a9cc21141e9","IPY_MODEL_47e63821f3534b42bbabff2ae7ab6107","IPY_MODEL_7e34f119725443da9b5fd2ab5e6860cf","IPY_MODEL_433352a6ca63412ca0ab4f050a0092ef"],"layout":"IPY_MODEL_c5b1f28937034aafb555a79022b3e0c8"}},"4b4c5a6cccb84222bc624e6d1af17c39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7671895eec804cfbaa1c98a9a6b21114","placeholder":"​","style":"IPY_MODEL_be4c5338c568429c8b7255e53df10afb","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"4f27513c65e44355a9e5c6bfd1d24dbe":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_7fc89fef78c24255893d005b088c8e63","placeholder":"​","style":"IPY_MODEL_58a18369d69141aaa188c2fcb70daeab","value":""}},"e732eb9a49974d2cb6ced414db1a4d7f":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_9c1a0ac826644d9a89eb0c09821c95d3","style":"IPY_MODEL_f49982094e3245a7b6b922f9114a7b06","value":true}},"a4a8435ce293415187d1e12a089bec50":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_a5dc3b931c3143ce8d3a1349b6e49747","style":"IPY_MODEL_22988131975843caad42bf08268ad4fb","tooltip":""}},"13b3687c098e4c62847a3f5571e8e314":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6b6247ecad34587904074d7f6e92433","placeholder":"​","style":"IPY_MODEL_a05f486554404b3393c5b6e2f3954572","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"c5b1f28937034aafb555a79022b3e0c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"7671895eec804cfbaa1c98a9a6b21114":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be4c5338c568429c8b7255e53df10afb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fc89fef78c24255893d005b088c8e63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58a18369d69141aaa188c2fcb70daeab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c1a0ac826644d9a89eb0c09821c95d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f49982094e3245a7b6b922f9114a7b06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5dc3b931c3143ce8d3a1349b6e49747":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22988131975843caad42bf08268ad4fb":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"f6b6247ecad34587904074d7f6e92433":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a05f486554404b3393c5b6e2f3954572":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"102928dc2f8c4b0c814e8c780a204c0a":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d54d05dd986140c39f958ecf33e5fbfd","placeholder":"​","style":"IPY_MODEL_14c0d5a373204952aa68b2c817f46628","value":"Connecting..."}},"d54d05dd986140c39f958ecf33e5fbfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14c0d5a373204952aa68b2c817f46628":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4f04f4db6f340829fcfe947b1750f15":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0277473538c44fb0b63ffa81cde9a193","placeholder":"​","style":"IPY_MODEL_ed68b20c03b0441384f106e2da82f018","value":"Token is valid (permission: write)."}},"d4a703fa18f640efb88eff8ac743164a":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9716d332f39a4d659082d85166a7e830","placeholder":"​","style":"IPY_MODEL_2dac0d66f97b4bd4a0efdffa8a2cc814","value":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine."}},"e1a843b60cb349779621f5b940470ec9":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87f22e33de6b4309afed320f63ca0665","placeholder":"​","style":"IPY_MODEL_192b5816b29049fab811f9a1e55b7386","value":"You might have to re-authenticate when pushing to the Hugging Face Hub."}},"c2e72d75ec694954a7ef23cbdaffdb08":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_897b066ee3e64b5392055883adadcccd","placeholder":"​","style":"IPY_MODEL_79eaa79a126341408dd0ed6a495d2f0c","value":"Run the following command in your terminal in case you want to set the 'store' credential helper as default."}},"e70a2d8124274bfeba8d554b2884836f":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0976555e01034c6d9f0c77f033e5c617","placeholder":"​","style":"IPY_MODEL_64db188b4f274b27af1dd9112d29e340","value":"git config --global credential.helper store"}},"12dd72e8d1b741daad185a9cc21141e9":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ea941a9866d428fad9b4c28ff23e523","placeholder":"​","style":"IPY_MODEL_542035c36bf8466db42985c5dfe64078","value":"Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m"}},"47e63821f3534b42bbabff2ae7ab6107":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afeb6b507dbc404ab5ea8da381bad272","placeholder":"​","style":"IPY_MODEL_0894b67e92284897b1fc1306d58a8703","value":"Token has not been saved to git credential helper."}},"7e34f119725443da9b5fd2ab5e6860cf":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d23c50894b74d0cabbc95acf4983444","placeholder":"​","style":"IPY_MODEL_7ccd7217bf9f4358b1055546fae97cfa","value":"Your token has been saved to /root/.cache/huggingface/token"}},"433352a6ca63412ca0ab4f050a0092ef":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cfb69a120214afb8c822b92f6e0f0cb","placeholder":"​","style":"IPY_MODEL_c529586d2a7b49f0a63b0fda2d9b7391","value":"Login successful"}},"0277473538c44fb0b63ffa81cde9a193":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed68b20c03b0441384f106e2da82f018":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9716d332f39a4d659082d85166a7e830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dac0d66f97b4bd4a0efdffa8a2cc814":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87f22e33de6b4309afed320f63ca0665":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"192b5816b29049fab811f9a1e55b7386":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"897b066ee3e64b5392055883adadcccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79eaa79a126341408dd0ed6a495d2f0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0976555e01034c6d9f0c77f033e5c617":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64db188b4f274b27af1dd9112d29e340":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ea941a9866d428fad9b4c28ff23e523":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"542035c36bf8466db42985c5dfe64078":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afeb6b507dbc404ab5ea8da381bad272":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0894b67e92284897b1fc1306d58a8703":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d23c50894b74d0cabbc95acf4983444":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ccd7217bf9f4358b1055546fae97cfa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cfb69a120214afb8c822b92f6e0f0cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c529586d2a7b49f0a63b0fda2d9b7391":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"958c20a881fe4d9e8bcbd8ca2c5795a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af483c3e3a534fbbaeeee7d5d3f204d6","IPY_MODEL_7ac7bc44e8db4197a6370d662ce826a6","IPY_MODEL_e5edb5f53d0b40c2a838a2dc3c4fa4ee"],"layout":"IPY_MODEL_f4cacd6e400f4d818b60700020e312a4"}},"af483c3e3a534fbbaeeee7d5d3f204d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2aa1316f52ad403985ea1eb57f71fe05","placeholder":"​","style":"IPY_MODEL_95d2c0fccc374310a7edfbaea8bce0be","value":"Downloading readme: 100%"}},"7ac7bc44e8db4197a6370d662ce826a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3426ae8613c346a2bdf62a784d78868c","max":804,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89e6c152308a4b26ac033ee098d81e13","value":804}},"e5edb5f53d0b40c2a838a2dc3c4fa4ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f9f0f7f44e145649539e977af9957d5","placeholder":"​","style":"IPY_MODEL_967cedf3120b45d49732d3168de7f1a1","value":" 804/804 [00:00&lt;00:00, 33.5kB/s]"}},"f4cacd6e400f4d818b60700020e312a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aa1316f52ad403985ea1eb57f71fe05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d2c0fccc374310a7edfbaea8bce0be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3426ae8613c346a2bdf62a784d78868c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89e6c152308a4b26ac033ee098d81e13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f9f0f7f44e145649539e977af9957d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"967cedf3120b45d49732d3168de7f1a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"201d9e959c5d494ab4ed3b98956ec4d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_05895e250d1648aca8d257142ed6f2fd","IPY_MODEL_d2e3cfb0d7b44c1f9f9b0ac5878eb23e","IPY_MODEL_5ced3a882abe40259a70626a457d89c8"],"layout":"IPY_MODEL_328024c2554f4a62aaf641336f0731cc"}},"05895e250d1648aca8d257142ed6f2fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3011a91553534931b57702e2ecb5ced3","placeholder":"​","style":"IPY_MODEL_76bdf53c11114355949241d2bc19bd79","value":"Downloading data: 100%"}},"d2e3cfb0d7b44c1f9f9b0ac5878eb23e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a470c25cf9242e4bb439ae3c0b56ccd","max":454279741,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f401a9107cad4b9d9e761928bfad880d","value":454279741}},"5ced3a882abe40259a70626a457d89c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6a068ae7c94440f8503179150af3766","placeholder":"​","style":"IPY_MODEL_767786e56b8140f2adf23bb5d0470783","value":" 454M/454M [00:31&lt;00:00, 16.2MB/s]"}},"328024c2554f4a62aaf641336f0731cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3011a91553534931b57702e2ecb5ced3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76bdf53c11114355949241d2bc19bd79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a470c25cf9242e4bb439ae3c0b56ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f401a9107cad4b9d9e761928bfad880d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6a068ae7c94440f8503179150af3766":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"767786e56b8140f2adf23bb5d0470783":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0930c057500243c7b8db7405f39fe45c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_709fd84d15e647e3ac6e0422489bd43f","IPY_MODEL_2ea421b2a109420992168f94b98adbd1","IPY_MODEL_e97e742018d842c98e8c9ad640a11f8b"],"layout":"IPY_MODEL_e7273cda51604344b9f739bd02c6c1e3"}},"709fd84d15e647e3ac6e0422489bd43f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_978e7089d2a14c9cafc5b5fed03addf3","placeholder":"​","style":"IPY_MODEL_bd3db0c2b5b74daab4199bb4f3987ec2","value":"Downloading data: 100%"}},"2ea421b2a109420992168f94b98adbd1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a32b723aae5d485aa125e9d3e6a58e55","max":457886494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_68035a97c80643168a6698769e8b9c36","value":457886494}},"e97e742018d842c98e8c9ad640a11f8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d97ae13da0f44009175a96600567587","placeholder":"​","style":"IPY_MODEL_7f8216de99854f2999ca617d025cfa7b","value":" 458M/458M [00:33&lt;00:00, 12.9MB/s]"}},"e7273cda51604344b9f739bd02c6c1e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"978e7089d2a14c9cafc5b5fed03addf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd3db0c2b5b74daab4199bb4f3987ec2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a32b723aae5d485aa125e9d3e6a58e55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68035a97c80643168a6698769e8b9c36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d97ae13da0f44009175a96600567587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f8216de99854f2999ca617d025cfa7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c02a1f2b6044c39a73c8272ecfd07e1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1da7f95edc404194bd4a3629edaed358","IPY_MODEL_157d0085c75b4439981a39f596d2d166","IPY_MODEL_1dfda86f4e5149ba89a62a501b82b226"],"layout":"IPY_MODEL_3aef08030ff04e30b1775c431b60b8b8"}},"1da7f95edc404194bd4a3629edaed358":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c0e17493aaa4efcb7f3fb7d83fb3257","placeholder":"​","style":"IPY_MODEL_be5f6763fea0493787ea4e6ec74c34ce","value":"Downloading data: 100%"}},"157d0085c75b4439981a39f596d2d166":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abce4f5679784caaa0cc4427ace2fa68","max":452493253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e12f61035a8406cbc4a89b6ac87adec","value":452493253}},"1dfda86f4e5149ba89a62a501b82b226":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d770c05b7c2b4cc1a349eadbcdd32e57","placeholder":"​","style":"IPY_MODEL_6a4a6d7965694de9baaeb09890395668","value":" 452M/452M [00:29&lt;00:00, 14.1MB/s]"}},"3aef08030ff04e30b1775c431b60b8b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c0e17493aaa4efcb7f3fb7d83fb3257":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be5f6763fea0493787ea4e6ec74c34ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abce4f5679784caaa0cc4427ace2fa68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e12f61035a8406cbc4a89b6ac87adec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d770c05b7c2b4cc1a349eadbcdd32e57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a4a6d7965694de9baaeb09890395668":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb2ef2470b604bc5b9aeb62a77185005":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e82bb3786034dd49a1ef907dd18a629","IPY_MODEL_4fc66c9c488540f982924a201b30abf9","IPY_MODEL_fa70b5c44d894700953676981a2def37"],"layout":"IPY_MODEL_94837869b722476e84c9f1ddfa0a049e"}},"6e82bb3786034dd49a1ef907dd18a629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a04edd32306441aa34f0de2cf28b294","placeholder":"​","style":"IPY_MODEL_438fd47312cd416eae2d1be2d371ad9b","value":"Downloading data: 100%"}},"4fc66c9c488540f982924a201b30abf9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_354fcfed5b5f476aba495aeea7c20346","max":448122490,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08c6f82f37514b019afb83538c869440","value":448122490}},"fa70b5c44d894700953676981a2def37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c8f038ce1164d00a161a650ad8ef309","placeholder":"​","style":"IPY_MODEL_d583e1bc8d444af49c4086ef6a706e57","value":" 448M/448M [00:31&lt;00:00, 16.2MB/s]"}},"94837869b722476e84c9f1ddfa0a049e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a04edd32306441aa34f0de2cf28b294":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"438fd47312cd416eae2d1be2d371ad9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"354fcfed5b5f476aba495aeea7c20346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c6f82f37514b019afb83538c869440":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c8f038ce1164d00a161a650ad8ef309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d583e1bc8d444af49c4086ef6a706e57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0d0ab09e6814f9ea8403c52e8f02eb3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cef4e8296ced4eb786f9974d5d6e4982","IPY_MODEL_95d0952d7abb426cb6c53b180a0f8eb4","IPY_MODEL_756f93f28cdd4223a9d0fd180627d701"],"layout":"IPY_MODEL_633c5717b1ae46888f8d7d6b7913b187"}},"cef4e8296ced4eb786f9974d5d6e4982":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c1a0cb6df704f8593e96c1239a8e12c","placeholder":"​","style":"IPY_MODEL_9646411394764161817b5e654e4fc9e4","value":"Downloading data: 100%"}},"95d0952d7abb426cb6c53b180a0f8eb4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b475083f61b42639e757655aee4b87f","max":454594640,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1d1b7199bee24e29b75d4f66645e4c20","value":454594640}},"756f93f28cdd4223a9d0fd180627d701":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc9b39af61a4710948a2dabc3d7481e","placeholder":"​","style":"IPY_MODEL_39ca7cc3cfbe4851844a7c5c24b4c6d2","value":" 455M/455M [00:31&lt;00:00, 16.8MB/s]"}},"633c5717b1ae46888f8d7d6b7913b187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c1a0cb6df704f8593e96c1239a8e12c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9646411394764161817b5e654e4fc9e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b475083f61b42639e757655aee4b87f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d1b7199bee24e29b75d4f66645e4c20":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fc9b39af61a4710948a2dabc3d7481e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39ca7cc3cfbe4851844a7c5c24b4c6d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebcf19bc58d7455e8fd5febd345508f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c6627d48dcc4c3883b29097c7a96b22","IPY_MODEL_72e49a45999e4076aa1c88fbdd3f71d0","IPY_MODEL_ba3a1daf2afd4b378eec8fff480ba108"],"layout":"IPY_MODEL_4433399c6e8746f1a7c37a032cf5567b"}},"9c6627d48dcc4c3883b29097c7a96b22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d2364cef71b4d2a9eef36ae122edf84","placeholder":"​","style":"IPY_MODEL_21ffbe9faf9744058749024111d0543b","value":"Downloading data: 100%"}},"72e49a45999e4076aa1c88fbdd3f71d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24b28462aa4147a2bd86464d6865c571","max":456368368,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85665e8c5b3f4cc6a7a19ff459c4ae9b","value":456368368}},"ba3a1daf2afd4b378eec8fff480ba108":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b99881a80340b28d7649469bb82dda","placeholder":"​","style":"IPY_MODEL_637f262d7a4b477fb0458629b81d19c2","value":" 456M/456M [00:30&lt;00:00, 17.1MB/s]"}},"4433399c6e8746f1a7c37a032cf5567b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d2364cef71b4d2a9eef36ae122edf84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21ffbe9faf9744058749024111d0543b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24b28462aa4147a2bd86464d6865c571":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85665e8c5b3f4cc6a7a19ff459c4ae9b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64b99881a80340b28d7649469bb82dda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"637f262d7a4b477fb0458629b81d19c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b04166124d0b481dad3121f77deb7844":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0c3af7e8b3a45eda1c1758006e7399b","IPY_MODEL_4523534f1c19418197eeefb0eb0f6d22","IPY_MODEL_6159c0c7ebce4c4db338cff1c5f53907"],"layout":"IPY_MODEL_c02a215035ea4c20bec9cd1d68551ac2"}},"d0c3af7e8b3a45eda1c1758006e7399b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2d03a2dd4c14a5691e7d47cc7a77412","placeholder":"​","style":"IPY_MODEL_c4519a029b324245bf47724ed693779f","value":"Downloading data: 100%"}},"4523534f1c19418197eeefb0eb0f6d22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e831e136e444a0389bb50f8d383fb7a","max":452813016,"min":0,"orientation":"horizontal","style":"IPY_MODEL_49b2134a7de74c53bcbca9db07785d89","value":452813016}},"6159c0c7ebce4c4db338cff1c5f53907":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82f72f99bdca4d47bcc27c54ee476de8","placeholder":"​","style":"IPY_MODEL_b2da09679fe944b598962c730023e012","value":" 453M/453M [00:33&lt;00:00, 17.7MB/s]"}},"c02a215035ea4c20bec9cd1d68551ac2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2d03a2dd4c14a5691e7d47cc7a77412":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4519a029b324245bf47724ed693779f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e831e136e444a0389bb50f8d383fb7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49b2134a7de74c53bcbca9db07785d89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82f72f99bdca4d47bcc27c54ee476de8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2da09679fe944b598962c730023e012":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96f736d0224844c699067966e5266639":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f7262ddb3b34888a909e4cc15a8939d","IPY_MODEL_19e6429a29e84f0fa3edd67431b7ebc8","IPY_MODEL_b7cb1dafbcff42659624359dcc2f8e53"],"layout":"IPY_MODEL_b53a7a8f33d147919c6d9fae23f90780"}},"6f7262ddb3b34888a909e4cc15a8939d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc8976f5ef8d496ba138d6310e92a96c","placeholder":"​","style":"IPY_MODEL_8c2524580bae4c3caf244f64dfae3ea9","value":"Downloading data: 100%"}},"19e6429a29e84f0fa3edd67431b7ebc8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_82855a1dbd29441ab05aa3abe94a473b","max":456268689,"min":0,"orientation":"horizontal","style":"IPY_MODEL_855aa2c071a84d46bfaf2e78f96f7b3e","value":456268689}},"b7cb1dafbcff42659624359dcc2f8e53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62c12dc5843940a286ae053b4d5ce930","placeholder":"​","style":"IPY_MODEL_5ec7b6bd240c41faa854f682261aebcf","value":" 456M/456M [00:31&lt;00:00, 15.9MB/s]"}},"b53a7a8f33d147919c6d9fae23f90780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc8976f5ef8d496ba138d6310e92a96c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c2524580bae4c3caf244f64dfae3ea9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"82855a1dbd29441ab05aa3abe94a473b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855aa2c071a84d46bfaf2e78f96f7b3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62c12dc5843940a286ae053b4d5ce930":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ec7b6bd240c41faa854f682261aebcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cde5fc2164614629a7e8f2971339b004":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f23bbebb295400991bca53c4ab07b26","IPY_MODEL_d0557a838dbc41c2a55f4290523c3a28","IPY_MODEL_6e5f4eddd85447b88e02ceb0bab5ef43"],"layout":"IPY_MODEL_15e5fb218bf140638b74d8f8a5649baa"}},"0f23bbebb295400991bca53c4ab07b26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16b929be3a1f48c2abb6590c60a8daf1","placeholder":"​","style":"IPY_MODEL_531a7f4b66384d47a2332e556cde1664","value":"Downloading data: 100%"}},"d0557a838dbc41c2a55f4290523c3a28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea273861a7bf47969545f13c1d72c1c7","max":455440759,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bfcc565748904753a0e91ebd0efd01fd","value":455440759}},"6e5f4eddd85447b88e02ceb0bab5ef43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0683ec8c888f4af1983be1f808102d50","placeholder":"​","style":"IPY_MODEL_01816066754941fd8a554acd3c92e832","value":" 455M/455M [00:33&lt;00:00, 14.5MB/s]"}},"15e5fb218bf140638b74d8f8a5649baa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16b929be3a1f48c2abb6590c60a8daf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"531a7f4b66384d47a2332e556cde1664":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea273861a7bf47969545f13c1d72c1c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfcc565748904753a0e91ebd0efd01fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0683ec8c888f4af1983be1f808102d50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01816066754941fd8a554acd3c92e832":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4c8d06c27324f029965a3dd63b87ef9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc07d1a1cf8a4a4ebe624cf2fd8e516b","IPY_MODEL_fd85b202555c434f905a26adad3a513e","IPY_MODEL_c68c4eec3c674416a031600637d367ba"],"layout":"IPY_MODEL_c4da8b9261f94d828d5b423be2f37c9c"}},"dc07d1a1cf8a4a4ebe624cf2fd8e516b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d11d12c9f314e4185b856f76aa34207","placeholder":"​","style":"IPY_MODEL_7f68858bd8ef4b1bb78deefbf476f90c","value":"Downloading data: 100%"}},"fd85b202555c434f905a26adad3a513e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f27173870623447d9ddb1a6ffcd18d93","max":451437197,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d39a1ed795c42caabf21e8b1bf974f4","value":451437197}},"c68c4eec3c674416a031600637d367ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf0b95a0d3444912b610a38a273b3f95","placeholder":"​","style":"IPY_MODEL_a0f1be2ac4e249e4b6c725835a72865d","value":" 451M/451M [00:31&lt;00:00, 17.0MB/s]"}},"c4da8b9261f94d828d5b423be2f37c9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d11d12c9f314e4185b856f76aa34207":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f68858bd8ef4b1bb78deefbf476f90c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f27173870623447d9ddb1a6ffcd18d93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d39a1ed795c42caabf21e8b1bf974f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf0b95a0d3444912b610a38a273b3f95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f1be2ac4e249e4b6c725835a72865d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98ed7ad4c6b14fd1b23fe5b63ec7f997":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab695c7ca77f4af0894034637ab138b7","IPY_MODEL_e396720e9cbe47ab8ec0c3ad2a9eaed2","IPY_MODEL_6b0c298d7c494c6284c744d94b898540"],"layout":"IPY_MODEL_b90436c9b10a45119c6fc0b38473c7e3"}},"ab695c7ca77f4af0894034637ab138b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f76a2698841847ff89fe286bc4be4281","placeholder":"​","style":"IPY_MODEL_0bb04cf553374e9882d58ad8cef95d43","value":"Downloading data: 100%"}},"e396720e9cbe47ab8ec0c3ad2a9eaed2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a196441574bf433d89f757344415545b","max":458649288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c732ef7832eb41378beedf5ce441ef8a","value":458649288}},"6b0c298d7c494c6284c744d94b898540":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f949e6176c8f4e9bbe9f9d42c4a529cf","placeholder":"​","style":"IPY_MODEL_7aa419d331a345288fef88d70e1fbd82","value":" 459M/459M [00:28&lt;00:00, 17.8MB/s]"}},"b90436c9b10a45119c6fc0b38473c7e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f76a2698841847ff89fe286bc4be4281":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bb04cf553374e9882d58ad8cef95d43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a196441574bf433d89f757344415545b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c732ef7832eb41378beedf5ce441ef8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f949e6176c8f4e9bbe9f9d42c4a529cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7aa419d331a345288fef88d70e1fbd82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cdbaaf2d72e47ff9b6ce911577e2286":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07143d4b3a4544aabd5c49fa76cb1dbc","IPY_MODEL_32174f50ef854b9fba300acdfba6aaac","IPY_MODEL_9f81280021134df5b0a6090f9ee32d3e"],"layout":"IPY_MODEL_3a948744b7cb40e294110f990d4a9cc3"}},"07143d4b3a4544aabd5c49fa76cb1dbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63951f9253f1483fa73b24ce70a3611e","placeholder":"​","style":"IPY_MODEL_1f6af85adae4400e8e3786bbf96cb92f","value":"Downloading data: 100%"}},"32174f50ef854b9fba300acdfba6aaac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_07be19f3aedf45a2babb41bf8338c87b","max":447009408,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ae1cb1d229840a581c9a5f7ff13d843","value":447009408}},"9f81280021134df5b0a6090f9ee32d3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af3f79406a7e4da384f366ad20b37299","placeholder":"​","style":"IPY_MODEL_9cd184d512da4f5996de118b09d306db","value":" 447M/447M [00:30&lt;00:00, 12.8MB/s]"}},"3a948744b7cb40e294110f990d4a9cc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63951f9253f1483fa73b24ce70a3611e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f6af85adae4400e8e3786bbf96cb92f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07be19f3aedf45a2babb41bf8338c87b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ae1cb1d229840a581c9a5f7ff13d843":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af3f79406a7e4da384f366ad20b37299":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cd184d512da4f5996de118b09d306db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2b68ad379cc4d05888b6f1d8b4cbfb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1a5c3b43e9f4bdb90c498c9fc0dfd69","IPY_MODEL_68a804f1470743a29976de1f2d86e87e","IPY_MODEL_4304cc1b0e074d3fa2349a41ab10e847"],"layout":"IPY_MODEL_647472b268754fcb8a9e71780f93f0dd"}},"e1a5c3b43e9f4bdb90c498c9fc0dfd69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c0491b7948e43a98a674c8ed24a8e05","placeholder":"​","style":"IPY_MODEL_fce210efc8764e6fbf6777e8b768c808","value":"Generating data split: 100%"}},"68a804f1470743a29976de1f2d86e87e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f6613643d8648849d021b7eb9d6a988","max":8500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2520b8b26b2241278d9302f803e53c13","value":8500}},"4304cc1b0e074d3fa2349a41ab10e847":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e472927b9424b54ad9c00782718eefe","placeholder":"​","style":"IPY_MODEL_ceaa49ae28594dfca45b7f8764ee5113","value":" 8500/8500 [01:19&lt;00:00, 103.91 examples/s]"}},"647472b268754fcb8a9e71780f93f0dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c0491b7948e43a98a674c8ed24a8e05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fce210efc8764e6fbf6777e8b768c808":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f6613643d8648849d021b7eb9d6a988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2520b8b26b2241278d9302f803e53c13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e472927b9424b54ad9c00782718eefe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ceaa49ae28594dfca45b7f8764ee5113":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ad03020c1114423a65ee61c76360973":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14342fdbba834ffbb9ef9570ff254b31","IPY_MODEL_96389581bd8848149314ad6aa50235bb","IPY_MODEL_3b0bfe57e2b342c289e23c22f38cfee3"],"layout":"IPY_MODEL_01894ffcdb3a4edf9c3da707cdcfa3c2"}},"14342fdbba834ffbb9ef9570ff254b31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c78fa4d69204ea3b67793be887d7bec","placeholder":"​","style":"IPY_MODEL_1a27b6ac1eb44a889a94ea82cc48f805","value":"config.json: 100%"}},"96389581bd8848149314ad6aa50235bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a3229a00fec4595a3efdfcb1467d082","max":77742,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83dac849074a43d2bad33b134b85f469","value":77742}},"3b0bfe57e2b342c289e23c22f38cfee3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b734af7174334237b325e7cdaae94c7e","placeholder":"​","style":"IPY_MODEL_2a4b428325284a4395cdcb3323031278","value":" 77.7k/77.7k [00:00&lt;00:00, 1.44MB/s]"}},"01894ffcdb3a4edf9c3da707cdcfa3c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c78fa4d69204ea3b67793be887d7bec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a27b6ac1eb44a889a94ea82cc48f805":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a3229a00fec4595a3efdfcb1467d082":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83dac849074a43d2bad33b134b85f469":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b734af7174334237b325e7cdaae94c7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a4b428325284a4395cdcb3323031278":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae520bde9a0741e39eede540b16c3c1b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7933e1e987b46aa9c1f008a6fd8f7a9","IPY_MODEL_274485a08e4e4f9da12eea1a48fea1b8","IPY_MODEL_6701034b7cb748dca2a42338e4c4d7f2"],"layout":"IPY_MODEL_2ddae2b99f194821b0895551963ee456"}},"f7933e1e987b46aa9c1f008a6fd8f7a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03770a6221f94126a370b938fe5ef6c5","placeholder":"​","style":"IPY_MODEL_35d09cfe29f646998e150a6335cb35f4","value":"pytorch_model.bin: 100%"}},"274485a08e4e4f9da12eea1a48fea1b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc9f0ac795064d60894b00a5890e4b37","max":879382349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28ee91a62e324dbf8749ef50dab840ae","value":879382349}},"6701034b7cb748dca2a42338e4c4d7f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa625967e52945329cc8e8d7ba8c0539","placeholder":"​","style":"IPY_MODEL_ab5a9d2efb8f4580bc74173ce1101354","value":" 879M/879M [00:18&lt;00:00, 52.9MB/s]"}},"2ddae2b99f194821b0895551963ee456":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03770a6221f94126a370b938fe5ef6c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35d09cfe29f646998e150a6335cb35f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc9f0ac795064d60894b00a5890e4b37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28ee91a62e324dbf8749ef50dab840ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa625967e52945329cc8e8d7ba8c0539":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab5a9d2efb8f4580bc74173ce1101354":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fd53094783d4cb4a2b289ec550a1544":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57ac83994cd44ea9a0f94ec69e004cd1","IPY_MODEL_f2d6d876afc74dbcacc5e167fc1ef945","IPY_MODEL_1ea5e1047ee141149fefd94d9e36d160"],"layout":"IPY_MODEL_2218a868c77f4404b6517bdf93f2c391"}},"57ac83994cd44ea9a0f94ec69e004cd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a83bf36cf98a44dfaa4d63e42a0d30e1","placeholder":"​","style":"IPY_MODEL_9c3e521994f64549898930a098f465ff","value":"labels_info.json: 100%"}},"f2d6d876afc74dbcacc5e167fc1ef945":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e09ebcc9b394f8888c010a481102729","max":1831,"min":0,"orientation":"horizontal","style":"IPY_MODEL_310fa138b6f249d7a68dcd5641b1f658","value":1831}},"1ea5e1047ee141149fefd94d9e36d160":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9b116a1cca54c84914d912b1a580a98","placeholder":"​","style":"IPY_MODEL_60c6c9bf2e514843b8f09d86364d524e","value":" 1.83k/1.83k [00:00&lt;00:00, 70.6kB/s]"}},"2218a868c77f4404b6517bdf93f2c391":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a83bf36cf98a44dfaa4d63e42a0d30e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c3e521994f64549898930a098f465ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e09ebcc9b394f8888c010a481102729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"310fa138b6f249d7a68dcd5641b1f658":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9b116a1cca54c84914d912b1a580a98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60c6c9bf2e514843b8f09d86364d524e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a54fc691e85248559ada5b03bb8c0ffc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8441000ece04219b7596c23817d535e","IPY_MODEL_fc3638d3bfec49d2bec3e49e586920f8","IPY_MODEL_12b8e83cd31a4f4a8ff12395fc5d9bed"],"layout":"IPY_MODEL_51d1008d841c49778f2c1a57e9266476"}},"b8441000ece04219b7596c23817d535e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_11dcd814aa194df688cfd31b20e1807c","placeholder":"​","style":"IPY_MODEL_227988af636546d6b62147effee2a448","value":"config.json: 100%"}},"fc3638d3bfec49d2bec3e49e586920f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_827e9ca396a74fdba8eed8898d869e3e","max":77742,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e4094894fd74027b667b2a789ed6b7b","value":77742}},"12b8e83cd31a4f4a8ff12395fc5d9bed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ade8ced35f434ac7a725813642400d6e","placeholder":"​","style":"IPY_MODEL_cea4866c9ce84b42b75604e63df11f38","value":" 77.7k/77.7k [00:00&lt;00:00, 1.54MB/s]"}},"51d1008d841c49778f2c1a57e9266476":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11dcd814aa194df688cfd31b20e1807c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227988af636546d6b62147effee2a448":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"827e9ca396a74fdba8eed8898d869e3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e4094894fd74027b667b2a789ed6b7b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ade8ced35f434ac7a725813642400d6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cea4866c9ce84b42b75604e63df11f38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76e11b9f773347b2a26ae92a8ea6c28f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_adad701229ae48bc82b3f729c9eeddef","IPY_MODEL_effb6fe1bc03457cbfeef92b292b8653","IPY_MODEL_754bf6f6698b4b61a4d03e367edf9e36"],"layout":"IPY_MODEL_95654d5d5f0a424bac86d2b757601dca"}},"adad701229ae48bc82b3f729c9eeddef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3436d33164a644cababbcc2fb6c1bec2","placeholder":"​","style":"IPY_MODEL_d2d9d5e515e241b9959e60b5ce29a795","value":"pytorch_model.bin: 100%"}},"effb6fe1bc03457cbfeef92b292b8653":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_633dcbd47c544221a0b656486b13c1f2","max":879382349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac76707ce1364864873b08e9492c7de6","value":879382349}},"754bf6f6698b4b61a4d03e367edf9e36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a1b951b02304fd689e7f440dbaa3ce1","placeholder":"​","style":"IPY_MODEL_b93aa7592fab413ea9dd8d491b347475","value":" 879M/879M [00:16&lt;00:00, 64.3MB/s]"}},"95654d5d5f0a424bac86d2b757601dca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3436d33164a644cababbcc2fb6c1bec2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2d9d5e515e241b9959e60b5ce29a795":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"633dcbd47c544221a0b656486b13c1f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac76707ce1364864873b08e9492c7de6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a1b951b02304fd689e7f440dbaa3ce1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b93aa7592fab413ea9dd8d491b347475":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa3d0422209341dbb1c2cb3df7c9f337":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77310f43383d4e37ac804e262c46915e","IPY_MODEL_f9aacf42ee384d23a3758db479e62b58","IPY_MODEL_4925bdece9b44221a0f523253d9b8194"],"layout":"IPY_MODEL_2f54b05f221743a087c7e9f5cf7806ee"}},"77310f43383d4e37ac804e262c46915e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e569a348e6e4b2e8bc929887c274e2a","placeholder":"​","style":"IPY_MODEL_e0eee41cd5f84ffabe0cb17fe4feb4cd","value":"preprocessor_config.json: 100%"}},"f9aacf42ee384d23a3758db479e62b58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05de8fbc14ea4aeb80091c774187abbb","max":1525,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef5523cff1fe4b5dbb992871a193a7fd","value":1525}},"4925bdece9b44221a0f523253d9b8194":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd4167cfbf32433c91e9ae0932750ee1","placeholder":"​","style":"IPY_MODEL_b559bbb878c84ed2992c5da3d98d8f2a","value":" 1.52k/1.52k [00:00&lt;00:00, 95.5kB/s]"}},"2f54b05f221743a087c7e9f5cf7806ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e569a348e6e4b2e8bc929887c274e2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0eee41cd5f84ffabe0cb17fe4feb4cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05de8fbc14ea4aeb80091c774187abbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef5523cff1fe4b5dbb992871a193a7fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd4167cfbf32433c91e9ae0932750ee1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b559bbb878c84ed2992c5da3d98d8f2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b11d61fb7c614a43b8613aeceef34dad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e60d0bcbda144ec8ad81089d90b3635a","IPY_MODEL_751b65f6244f4c1180f9d9d0a491a9ad","IPY_MODEL_2e38560ff08945cd9820e3781ad89690"],"layout":"IPY_MODEL_8d38e58461f746a6a4963b5dd9802255"}},"e60d0bcbda144ec8ad81089d90b3635a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_123f81146d534da9bf049d1a25be496b","placeholder":"​","style":"IPY_MODEL_55d71579522844febaeedd38b2a20613","value":"cityscapes_panoptic.json: 100%"}},"751b65f6244f4c1180f9d9d0a491a9ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95bf480fbe6e4ea7973e12c9e1bb92cf","max":838,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f0b15ed6d9c4bd4b09ecb396093212b","value":838}},"2e38560ff08945cd9820e3781ad89690":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35ba3fa3c6d44e249ebfb2ed28c91aa5","placeholder":"​","style":"IPY_MODEL_afb5e14bb65945dab4e9566961a1cd80","value":" 838/838 [00:00&lt;00:00, 55.6kB/s]"}},"8d38e58461f746a6a4963b5dd9802255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"123f81146d534da9bf049d1a25be496b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55d71579522844febaeedd38b2a20613":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95bf480fbe6e4ea7973e12c9e1bb92cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f0b15ed6d9c4bd4b09ecb396093212b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35ba3fa3c6d44e249ebfb2ed28c91aa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afb5e14bb65945dab4e9566961a1cd80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db93a9908f19458fb1ab24e487da9dc6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ccb229087154c5f982ed9a457600b2f","IPY_MODEL_33516225315d45ea8089623599fa6377","IPY_MODEL_6e6acdfae88046bba786f9b2396bbee1"],"layout":"IPY_MODEL_33c30445c8fd40e19277885e6f579318"}},"1ccb229087154c5f982ed9a457600b2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1682d7828b19482d90f30407287d0e05","placeholder":"​","style":"IPY_MODEL_4167194f0227405b86dd262983387c25","value":"tokenizer_config.json: 100%"}},"33516225315d45ea8089623599fa6377":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56121314be03475bbe1cb330cc2e4864","max":812,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b801d0aec0ee47828c248dd17f5c7bfd","value":812}},"6e6acdfae88046bba786f9b2396bbee1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98bd9af33c274c6ebc209875044d8fda","placeholder":"​","style":"IPY_MODEL_7a34b79c32544236b138692e249f726c","value":" 812/812 [00:00&lt;00:00, 49.4kB/s]"}},"33c30445c8fd40e19277885e6f579318":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1682d7828b19482d90f30407287d0e05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4167194f0227405b86dd262983387c25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56121314be03475bbe1cb330cc2e4864":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b801d0aec0ee47828c248dd17f5c7bfd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98bd9af33c274c6ebc209875044d8fda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a34b79c32544236b138692e249f726c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"734784dafb7d476a83e0ed2555400e3f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8774a89dc65e41e9b7bc6ff3bf9a12d6","IPY_MODEL_718830fe1be742e99935df459e2d40d8","IPY_MODEL_39afb548c9874c16b59c1b1679f14cd4"],"layout":"IPY_MODEL_2f25724062e04ca3b7e753c28c8060aa"}},"8774a89dc65e41e9b7bc6ff3bf9a12d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a060f86e48d446a0a8bc377bf6034346","placeholder":"​","style":"IPY_MODEL_345649f9350f408aab65984e6f39d520","value":"vocab.json: 100%"}},"718830fe1be742e99935df459e2d40d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcb423bc1fd04ead9268adf842a0d0d9","max":1059962,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9c7b911084f4acf8766e94cffd2eb50","value":1059962}},"39afb548c9874c16b59c1b1679f14cd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f90d35918cd4d14a183ecf9cec1374b","placeholder":"​","style":"IPY_MODEL_ef3edba10d184d3c908dfaca0604f1cb","value":" 1.06M/1.06M [00:00&lt;00:00, 22.4MB/s]"}},"2f25724062e04ca3b7e753c28c8060aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a060f86e48d446a0a8bc377bf6034346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"345649f9350f408aab65984e6f39d520":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcb423bc1fd04ead9268adf842a0d0d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9c7b911084f4acf8766e94cffd2eb50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f90d35918cd4d14a183ecf9cec1374b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef3edba10d184d3c908dfaca0604f1cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae2c33f69d4a464fa8f5db68a6a124ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d344e41d0d5b465c8d0b6e86c16df806","IPY_MODEL_afdcf27a088b4468a21192fd34c26c6a","IPY_MODEL_449228a23b144f02b4904bde190c7785"],"layout":"IPY_MODEL_cb4f99383b9f474da6b5b8943f67c624"}},"d344e41d0d5b465c8d0b6e86c16df806":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5081d8c8b3ce4b97895ce2d8ecaa54ba","placeholder":"​","style":"IPY_MODEL_37f9517b6fc647ebaec61801e6dd5f65","value":"merges.txt: 100%"}},"afdcf27a088b4468a21192fd34c26c6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_627a4e3d16764366bcfd7887be568299","max":524619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f089156c09e84318a33bac72ab26d48d","value":524619}},"449228a23b144f02b4904bde190c7785":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0318611f960a40229f34e2c5cd4160cb","placeholder":"​","style":"IPY_MODEL_f8e8bc20f7d64b3e916a5ff613c41aa2","value":" 525k/525k [00:00&lt;00:00, 28.1MB/s]"}},"cb4f99383b9f474da6b5b8943f67c624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5081d8c8b3ce4b97895ce2d8ecaa54ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37f9517b6fc647ebaec61801e6dd5f65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"627a4e3d16764366bcfd7887be568299":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f089156c09e84318a33bac72ab26d48d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0318611f960a40229f34e2c5cd4160cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8e8bc20f7d64b3e916a5ff613c41aa2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86d549447568497d8cc89c752eccda36":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef85e993d2ee4272af0d8d8b461e41d0","IPY_MODEL_60cc796801424aeda0850346e39b13e7","IPY_MODEL_eee5df4b34b64d629359c49bd9bb67cc"],"layout":"IPY_MODEL_0b5aae12e3854022a0a8169acc79300e"}},"ef85e993d2ee4272af0d8d8b461e41d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0aaed17b974a422c8794e237612abc92","placeholder":"​","style":"IPY_MODEL_2e072fd4d2ad4f1aab1d57a5c572f334","value":"special_tokens_map.json: 100%"}},"60cc796801424aeda0850346e39b13e7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d53125acfba04541aa8d051c7a2bb364","max":472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18d4d427fac649a3916e15464669b4f5","value":472}},"eee5df4b34b64d629359c49bd9bb67cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_feeeba152e364f9bbc46c325291735d0","placeholder":"​","style":"IPY_MODEL_67fc24cc5d26498cb7a01846bbdc907e","value":" 472/472 [00:00&lt;00:00, 19.7kB/s]"}},"0b5aae12e3854022a0a8169acc79300e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0aaed17b974a422c8794e237612abc92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e072fd4d2ad4f1aab1d57a5c572f334":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d53125acfba04541aa8d051c7a2bb364":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18d4d427fac649a3916e15464669b4f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"feeeba152e364f9bbc46c325291735d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67fc24cc5d26498cb7a01846bbdc907e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acb66233eb614a27b9fa4addbe29f8e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16bdae5d3fe74799a23b37806458e7e8","IPY_MODEL_4d07a24fc36b4699b85a041044d27844","IPY_MODEL_c7ce2c12224c4bceaf48fda55b78cb6e"],"layout":"IPY_MODEL_2a9c0aa1cc2b4889bbd2d802e98cc4d8"}},"16bdae5d3fe74799a23b37806458e7e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bc543c48d5348fd9ae5d2ae6c623a5c","placeholder":"​","style":"IPY_MODEL_481df29be11e43a89712cd1ec50581e8","value":"Downloading readme: 100%"}},"4d07a24fc36b4699b85a041044d27844":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_afaef9b1c8fa4a2e89429dd167309b31","max":804,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d5d0a0a792c49feb2ac3e446ccf278f","value":804}},"c7ce2c12224c4bceaf48fda55b78cb6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd74cda8113643a2b3f19e6a815f2dcd","placeholder":"​","style":"IPY_MODEL_7a6c46452cf648658f00ec383bd0deb1","value":" 804/804 [00:00&lt;00:00, 19.3kB/s]"}},"2a9c0aa1cc2b4889bbd2d802e98cc4d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bc543c48d5348fd9ae5d2ae6c623a5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"481df29be11e43a89712cd1ec50581e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afaef9b1c8fa4a2e89429dd167309b31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d5d0a0a792c49feb2ac3e446ccf278f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd74cda8113643a2b3f19e6a815f2dcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a6c46452cf648658f00ec383bd0deb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a384f37852be4840a8290df6465969c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e61d3bfca35489c8ec3081277da78fe","IPY_MODEL_8f3bec7ac075422f998cdbaa1acf9a92","IPY_MODEL_403f209f59b44f77abf27b10542c2e94"],"layout":"IPY_MODEL_51a8b19999b74a6384b56d7b211e144a"}},"7e61d3bfca35489c8ec3081277da78fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b771d0d4c9014bedad0767f8402399c8","placeholder":"​","style":"IPY_MODEL_583bc594411d4e29b719769fc915b846","value":"Downloading data: 100%"}},"8f3bec7ac075422f998cdbaa1acf9a92":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41512a00532046c48a79d3f35910dae9","max":454279741,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89edddd0912945eb9da82fc2c46f29f0","value":454279741}},"403f209f59b44f77abf27b10542c2e94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f52c7682c9344defb3cb39b57df5cdd3","placeholder":"​","style":"IPY_MODEL_cb240772432c425bb2ff7169b4dc554d","value":" 454M/454M [00:17&lt;00:00, 30.5MB/s]"}},"51a8b19999b74a6384b56d7b211e144a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b771d0d4c9014bedad0767f8402399c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"583bc594411d4e29b719769fc915b846":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41512a00532046c48a79d3f35910dae9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89edddd0912945eb9da82fc2c46f29f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f52c7682c9344defb3cb39b57df5cdd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb240772432c425bb2ff7169b4dc554d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26fa44a0f34f4599a91e8410ffaad99f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77c9f50aab1147e48f1a475a12bf218f","IPY_MODEL_2544f0d786874be08a80365cb40822d8","IPY_MODEL_e0d5f25fa5184d78aa3949329e7bb367"],"layout":"IPY_MODEL_54c0fcb3187143a7b43844db86b21c9a"}},"77c9f50aab1147e48f1a475a12bf218f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9db137601afc4dfcb4e7b466e6f4d8bb","placeholder":"​","style":"IPY_MODEL_109dc7ce8ffd4d01983bf79a2d2cf51c","value":"Downloading data: 100%"}},"2544f0d786874be08a80365cb40822d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae1980a7858c4d1bb963d874f9699f59","max":457886494,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8631342c0d784a3bac1e82a4665e67d1","value":457886494}},"e0d5f25fa5184d78aa3949329e7bb367":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7154a2699bb4a20840973ea826c8dd7","placeholder":"​","style":"IPY_MODEL_44b1b65a1c4b4b66bea3e35ea0ef0281","value":" 458M/458M [00:19&lt;00:00, 21.9MB/s]"}},"54c0fcb3187143a7b43844db86b21c9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9db137601afc4dfcb4e7b466e6f4d8bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"109dc7ce8ffd4d01983bf79a2d2cf51c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae1980a7858c4d1bb963d874f9699f59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8631342c0d784a3bac1e82a4665e67d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7154a2699bb4a20840973ea826c8dd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44b1b65a1c4b4b66bea3e35ea0ef0281":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4968045158dc4503ad1b821cf2ae075f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9c845b9d82b493e908bcd33e21bb970","IPY_MODEL_889e7eb757084b20ba649dbecd8322ba","IPY_MODEL_5760c6fa5ddd483797bbb7f3b5bd80f0"],"layout":"IPY_MODEL_8e51477d7eae49e482577e67517dbfee"}},"f9c845b9d82b493e908bcd33e21bb970":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e7774c29f91419b862bebcabd34b742","placeholder":"​","style":"IPY_MODEL_a305c0c8f75e470ead60f2d6abb4d3e1","value":"Downloading data: 100%"}},"889e7eb757084b20ba649dbecd8322ba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_acca7312f9534f97a50fd6e71d5c7f25","max":452493253,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad233f87fb4d41e5a0c6d0fc89a888a0","value":452493253}},"5760c6fa5ddd483797bbb7f3b5bd80f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4e5eb813d0d4d90b575fe4e0ed0c9fc","placeholder":"​","style":"IPY_MODEL_c8c14b675908421fb5c95a3762164299","value":" 452M/452M [00:48&lt;00:00, 15.8MB/s]"}},"8e51477d7eae49e482577e67517dbfee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e7774c29f91419b862bebcabd34b742":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a305c0c8f75e470ead60f2d6abb4d3e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acca7312f9534f97a50fd6e71d5c7f25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad233f87fb4d41e5a0c6d0fc89a888a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4e5eb813d0d4d90b575fe4e0ed0c9fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8c14b675908421fb5c95a3762164299":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc81faede80e4d229da12e02fa2ee660":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31f32437b88f4a4ba14770b9fdf968dd","IPY_MODEL_6eb0cacc1775427bbe75b54560d1a3c0","IPY_MODEL_cb15d2739e9748a1a487479c7b576572"],"layout":"IPY_MODEL_e335005cc7614af9816708b73c947fad"}},"31f32437b88f4a4ba14770b9fdf968dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_233ab8c63dbe4306b47540d5aab6f1fd","placeholder":"​","style":"IPY_MODEL_276c01d048ea46ed920daf3edd510fce","value":"Downloading data: 100%"}},"6eb0cacc1775427bbe75b54560d1a3c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_17d45bace43a4c03aca7aa90302f7e57","max":448122490,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ced578f7fa4a47789bf0f3060efa54c8","value":448122490}},"cb15d2739e9748a1a487479c7b576572":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f550d2242a424577a60ccf0b4cf88617","placeholder":"​","style":"IPY_MODEL_a5389071c1b64609a497a7e3ff6adcaf","value":" 448M/448M [00:16&lt;00:00, 30.4MB/s]"}},"e335005cc7614af9816708b73c947fad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"233ab8c63dbe4306b47540d5aab6f1fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"276c01d048ea46ed920daf3edd510fce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17d45bace43a4c03aca7aa90302f7e57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ced578f7fa4a47789bf0f3060efa54c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f550d2242a424577a60ccf0b4cf88617":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5389071c1b64609a497a7e3ff6adcaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"144e93305074473cbcb536f773dc27cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62e6f3e6308b4a5ba0a8a00ed25919e0","IPY_MODEL_226b45aba8e24e4abc646e408bf89b81","IPY_MODEL_8920453de84740bcb6eee9e27a438d61"],"layout":"IPY_MODEL_44acb765a6974f3389eb47a493aaba55"}},"62e6f3e6308b4a5ba0a8a00ed25919e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7867cf749a0f493994ba9e077366721e","placeholder":"​","style":"IPY_MODEL_659d39962958421a8f97f0a4af40ee2d","value":"Downloading data: 100%"}},"226b45aba8e24e4abc646e408bf89b81":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_240683e35f4c431f9eb96db4d1f731bd","max":454594640,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84a0dbae4f76441096e9b751e9741c05","value":454594640}},"8920453de84740bcb6eee9e27a438d61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b3c640c23024097ae04cdf9747454e9","placeholder":"​","style":"IPY_MODEL_3db8d97892d64462a72966eba93a852f","value":" 455M/455M [00:16&lt;00:00, 37.6MB/s]"}},"44acb765a6974f3389eb47a493aaba55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7867cf749a0f493994ba9e077366721e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"659d39962958421a8f97f0a4af40ee2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"240683e35f4c431f9eb96db4d1f731bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84a0dbae4f76441096e9b751e9741c05":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b3c640c23024097ae04cdf9747454e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3db8d97892d64462a72966eba93a852f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c782862f10084c128f3ec8422a4e1923":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b029a52481a942f28e707c583986546c","IPY_MODEL_e1eedc54557b45429ac04335692a5901","IPY_MODEL_3bf333b2c59f42d2be89cc37490a5cca"],"layout":"IPY_MODEL_2a6348aec7c64832bda11df2c971afac"}},"b029a52481a942f28e707c583986546c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc26b66b3f70458b93d0cd261728c174","placeholder":"​","style":"IPY_MODEL_a9628fc293724e97837f9d40f31f0af5","value":"Downloading data: 100%"}},"e1eedc54557b45429ac04335692a5901":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1e66baa31e04a52922085268c2e8970","max":456368368,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac0410b0400a4ff0bac8f88521746837","value":456368368}},"3bf333b2c59f42d2be89cc37490a5cca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b77948265ed54e2cba32db23849905d4","placeholder":"​","style":"IPY_MODEL_13dda8430f7842ba95cace5f6124da24","value":" 456M/456M [00:18&lt;00:00, 20.8MB/s]"}},"2a6348aec7c64832bda11df2c971afac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc26b66b3f70458b93d0cd261728c174":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9628fc293724e97837f9d40f31f0af5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1e66baa31e04a52922085268c2e8970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac0410b0400a4ff0bac8f88521746837":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b77948265ed54e2cba32db23849905d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13dda8430f7842ba95cace5f6124da24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bd33a202bb84dca84e570ff8db3d30f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfd7a3fcedfd4fe1adfd93184a0339dd","IPY_MODEL_347a210e5ced4a5494959e4417fe1635","IPY_MODEL_9f30b68f0469406a9b944bd837d9ad0e"],"layout":"IPY_MODEL_a810f9b79e1b4583a9147dd658169824"}},"bfd7a3fcedfd4fe1adfd93184a0339dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94c33bb538284a62abca56fc6314ccfb","placeholder":"​","style":"IPY_MODEL_f0572f3825664d4d9604e97bcc15061a","value":"Downloading data: 100%"}},"347a210e5ced4a5494959e4417fe1635":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdba53703ff2454bb55ee74114c1e1f0","max":452813016,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e31692625e3841d0a9bad27094f2963d","value":452813016}},"9f30b68f0469406a9b944bd837d9ad0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b799bdbe08b4bb0bf1f70d8a4ba1920","placeholder":"​","style":"IPY_MODEL_6a3f54deb23f43b2b06e3612301fe1cc","value":" 453M/453M [00:23&lt;00:00, 17.3MB/s]"}},"a810f9b79e1b4583a9147dd658169824":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94c33bb538284a62abca56fc6314ccfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0572f3825664d4d9604e97bcc15061a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdba53703ff2454bb55ee74114c1e1f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e31692625e3841d0a9bad27094f2963d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b799bdbe08b4bb0bf1f70d8a4ba1920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a3f54deb23f43b2b06e3612301fe1cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68f69048a7b341b186e7a4d1da51dd30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c2213d0b7d24915bcb9b8e282237643","IPY_MODEL_41360c6b2a234e7582fd234fb5ea5d04","IPY_MODEL_6381193a1bb54a52a5ed7ebab45cb2ef"],"layout":"IPY_MODEL_76065a5791d5483889c0542726fca2bc"}},"2c2213d0b7d24915bcb9b8e282237643":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e79792f16d44d0890875850bb3e32a8","placeholder":"​","style":"IPY_MODEL_0b97085f2614428aa2c56c079fde881e","value":"Downloading data: 100%"}},"41360c6b2a234e7582fd234fb5ea5d04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ccd2a4be4f24ea384d0e932b0a692c6","max":456268689,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d58d1fb55da645eca4762f2c20bd077b","value":456268689}},"6381193a1bb54a52a5ed7ebab45cb2ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77314328fc8743139e23ca9e9460c2e5","placeholder":"​","style":"IPY_MODEL_2905cd8126b24191820dd2ab9905e43e","value":" 456M/456M [00:15&lt;00:00, 33.5MB/s]"}},"76065a5791d5483889c0542726fca2bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e79792f16d44d0890875850bb3e32a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b97085f2614428aa2c56c079fde881e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ccd2a4be4f24ea384d0e932b0a692c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d58d1fb55da645eca4762f2c20bd077b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77314328fc8743139e23ca9e9460c2e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2905cd8126b24191820dd2ab9905e43e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"346ac103e3a04effbfe21c6f7b2b05a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6a2516064f843dea9d451f430d7cccf","IPY_MODEL_d34923bb23a64511960c6c5b76826797","IPY_MODEL_8edc94df7fc4404e82855ae4d2f57559"],"layout":"IPY_MODEL_390677d470a04892887a9c16afe9a47c"}},"f6a2516064f843dea9d451f430d7cccf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2966d4dd4f454136ab8075389088898b","placeholder":"​","style":"IPY_MODEL_2b53fc272bf3454b8b12bc21ad366729","value":"Downloading data: 100%"}},"d34923bb23a64511960c6c5b76826797":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6da16a4433004120a296bb870cff4e2b","max":455440759,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec2e767b7ad4403ab24da27aa90a85ea","value":455440759}},"8edc94df7fc4404e82855ae4d2f57559":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b42a24aed5e42a18c460402f8e392d5","placeholder":"​","style":"IPY_MODEL_21b0ee607fd747d8aee76d40f2a4940e","value":" 455M/455M [00:14&lt;00:00, 30.5MB/s]"}},"390677d470a04892887a9c16afe9a47c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2966d4dd4f454136ab8075389088898b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b53fc272bf3454b8b12bc21ad366729":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6da16a4433004120a296bb870cff4e2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec2e767b7ad4403ab24da27aa90a85ea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b42a24aed5e42a18c460402f8e392d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21b0ee607fd747d8aee76d40f2a4940e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"329426deb8bc477b9df02d077a5801f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c9e184c18c7424d891feb629fa74cc9","IPY_MODEL_2d8fd5b5ad0c46f4bea3f662b5275292","IPY_MODEL_bd498ae6b7fb4849a13d1dfd9a212a71"],"layout":"IPY_MODEL_57be65c4b2384a05a40aad798673979f"}},"6c9e184c18c7424d891feb629fa74cc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb2fdfb450cd435e84336468907cd5ce","placeholder":"​","style":"IPY_MODEL_f6cf67e62fac45c7a30a68c9794e6b79","value":"Downloading data: 100%"}},"2d8fd5b5ad0c46f4bea3f662b5275292":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8bb33317b814cb5844b2e16916c235f","max":451437197,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec8d5ecd87ee4defbe8a517a9cab90bd","value":451437197}},"bd498ae6b7fb4849a13d1dfd9a212a71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22d256b7c18943ad8abf611ec5acd793","placeholder":"​","style":"IPY_MODEL_32bcb4aaac844ce4a35420c090207773","value":" 451M/451M [00:16&lt;00:00, 26.1MB/s]"}},"57be65c4b2384a05a40aad798673979f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb2fdfb450cd435e84336468907cd5ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6cf67e62fac45c7a30a68c9794e6b79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8bb33317b814cb5844b2e16916c235f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec8d5ecd87ee4defbe8a517a9cab90bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"22d256b7c18943ad8abf611ec5acd793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32bcb4aaac844ce4a35420c090207773":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1ef404acc5d4d34979a7a134a5f5c97":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3379eecdec464be0a751ae6f029a49fc","IPY_MODEL_9893e5d77a6b4f30be4fc31d57eeb233","IPY_MODEL_a6196503429744de962613c351113306"],"layout":"IPY_MODEL_2e36d3cbf3c0402aa2986209dbce5543"}},"3379eecdec464be0a751ae6f029a49fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e12fd43256e425d8dd8486d800e994b","placeholder":"​","style":"IPY_MODEL_8af611ef15f54ca0b9d4cc6c46684b4a","value":"Downloading data: 100%"}},"9893e5d77a6b4f30be4fc31d57eeb233":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_35ec797e155d4140b7bd45b13da3d509","max":458649288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3400be871c254be3bb5dfbd630b9aeb9","value":458649288}},"a6196503429744de962613c351113306":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e40e6479514d49d5a4176002144b9171","placeholder":"​","style":"IPY_MODEL_7c340702d44742d9aa9a915d92f4e22d","value":" 459M/459M [00:16&lt;00:00, 33.4MB/s]"}},"2e36d3cbf3c0402aa2986209dbce5543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e12fd43256e425d8dd8486d800e994b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8af611ef15f54ca0b9d4cc6c46684b4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35ec797e155d4140b7bd45b13da3d509":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3400be871c254be3bb5dfbd630b9aeb9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e40e6479514d49d5a4176002144b9171":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c340702d44742d9aa9a915d92f4e22d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc83795d415e4e45ad249aaeb7ab33e0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffce499bd7774baeadd47af2d9a5521a","IPY_MODEL_fb81855dd3084aaca424ec6866d34ad3","IPY_MODEL_a746e283a3344460b7b5b52c75c1e1b5"],"layout":"IPY_MODEL_7dabf0c0b1be40fab6098c808bc87e86"}},"ffce499bd7774baeadd47af2d9a5521a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8949672bd6224236b1a25ab91f1cd3d7","placeholder":"​","style":"IPY_MODEL_55774be35b824db08cabdff6f0fcc6ad","value":"Downloading data: 100%"}},"fb81855dd3084aaca424ec6866d34ad3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a8b5df591034968a746bae636bc48f6","max":447009408,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9232f897dd264d1a81773cc5819a5ae2","value":447009408}},"a746e283a3344460b7b5b52c75c1e1b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3ac42e708ad432191dfb01936f41fb1","placeholder":"​","style":"IPY_MODEL_5f216a9b1c8c4cb5a401b67a11b69e9d","value":" 447M/447M [00:15&lt;00:00, 30.2MB/s]"}},"7dabf0c0b1be40fab6098c808bc87e86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8949672bd6224236b1a25ab91f1cd3d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55774be35b824db08cabdff6f0fcc6ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a8b5df591034968a746bae636bc48f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9232f897dd264d1a81773cc5819a5ae2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3ac42e708ad432191dfb01936f41fb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f216a9b1c8c4cb5a401b67a11b69e9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3dad10e24c24d859777da8eaf2eb2f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c570753b03e4cad8c39c0adbf03167b","IPY_MODEL_839d1bb6270f46c78e166988293484ff","IPY_MODEL_fea52f5ac70c4c18b03ce2b91eb76807"],"layout":"IPY_MODEL_446b4c048dc54b2589cd0a3e7c19c213"}},"6c570753b03e4cad8c39c0adbf03167b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15eb373876744eaa8ed09c4db61f0ee9","placeholder":"​","style":"IPY_MODEL_f4b61f3942ec434fac60bd3f9088dd55","value":"Generating data split: 100%"}},"839d1bb6270f46c78e166988293484ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6432cbf5d16344d8b77f824954e7e7e7","max":8500,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50442b688b63428ca32cb0342c182195","value":8500}},"fea52f5ac70c4c18b03ce2b91eb76807":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aed77354ec624543972846033b3455de","placeholder":"​","style":"IPY_MODEL_c11be314812b4abf9d9505c54a5e9bed","value":" 8500/8500 [01:17&lt;00:00, 111.80 examples/s]"}},"446b4c048dc54b2589cd0a3e7c19c213":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15eb373876744eaa8ed09c4db61f0ee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4b61f3942ec434fac60bd3f9088dd55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6432cbf5d16344d8b77f824954e7e7e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50442b688b63428ca32cb0342c182195":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aed77354ec624543972846033b3455de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c11be314812b4abf9d9505c54a5e9bed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c70b17f4799941daa8ae118ba91d75d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b38d3012d7545999b636d665f6e9d28","IPY_MODEL_8a219a6c88f24a1ea67121b6c29ff44b","IPY_MODEL_51564aaaa5994b35839edcbfce5148b5"],"layout":"IPY_MODEL_122832b37b394569848a3e62b1dc3447"}},"2b38d3012d7545999b636d665f6e9d28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f444f0ca8bcc45c18ed170de8c631a2e","placeholder":"​","style":"IPY_MODEL_2b589b4f5d9f4e159e80ea8b1b9816ba","value":"Downloading builder script: 100%"}},"8a219a6c88f24a1ea67121b6c29ff44b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53683e9085684faf87e5d2f6f5b0fb5b","max":13077,"min":0,"orientation":"horizontal","style":"IPY_MODEL_88f0e134a1214b91b8dafae923d465e5","value":13077}},"51564aaaa5994b35839edcbfce5148b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ad7236737714bee9a357fe77cd94870","placeholder":"​","style":"IPY_MODEL_062a4ef225ce4f4ba76f9955ecff5879","value":" 13.1k/13.1k [00:00&lt;00:00, 670kB/s]"}},"122832b37b394569848a3e62b1dc3447":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f444f0ca8bcc45c18ed170de8c631a2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b589b4f5d9f4e159e80ea8b1b9816ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53683e9085684faf87e5d2f6f5b0fb5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88f0e134a1214b91b8dafae923d465e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ad7236737714bee9a357fe77cd94870":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"062a4ef225ce4f4ba76f9955ecff5879":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e71bf5cf09d474e9836a692f518d4c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ed19b134a234973ae6dad5ca75313e9","IPY_MODEL_28a6d0a437dc45378d499d350a1dea6a","IPY_MODEL_b31000a22e074c73bc675bff9730ef9e"],"layout":"IPY_MODEL_94b804b4daf5444b84f4ffe23a027ec5"}},"1ed19b134a234973ae6dad5ca75313e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_decb57633893453cb6739f0ea384ba9f","placeholder":"​","style":"IPY_MODEL_4517bbb5fbf4441c9ade373b9e9da899","value":"100%"}},"28a6d0a437dc45378d499d350a1dea6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac69f7a25fcd4583b93eca1fe4a46e10","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_480b117d39bc443b843e5a1aadf6ac83","value":2}},"b31000a22e074c73bc675bff9730ef9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfa7fe6d930d4a55b88468e5582dfd18","placeholder":"​","style":"IPY_MODEL_9218217026af4fa8b7ffaf93b4cc3085","value":" 2/2 [04:07&lt;00:00, 123.77s/it]"}},"94b804b4daf5444b84f4ffe23a027ec5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"decb57633893453cb6739f0ea384ba9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4517bbb5fbf4441c9ade373b9e9da899":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac69f7a25fcd4583b93eca1fe4a46e10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"480b117d39bc443b843e5a1aadf6ac83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfa7fe6d930d4a55b88468e5582dfd18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9218217026af4fa8b7ffaf93b4cc3085":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77b355d3b5134f229a187606f2fd8ecc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b52479baf50442889bf177f4c1197f4f","IPY_MODEL_5351c0efcb4a4688a9d5c3fa5df944d6","IPY_MODEL_6fb5a4252d7e4063b464c73704548262"],"layout":"IPY_MODEL_551a13f70fd44365b92cbe4b8ef6cfc0"}},"b52479baf50442889bf177f4c1197f4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b08f40110f9430f9de5bcc0aebf5b84","placeholder":"​","style":"IPY_MODEL_bf20a796a2e34e329472e519aba7cfe3","value":"100%"}},"5351c0efcb4a4688a9d5c3fa5df944d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96b376e622ef42ba95426fc8310f2fc4","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ec3061465fb4716916b08a900c43a96","value":4}},"6fb5a4252d7e4063b464c73704548262":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c066e176bfcd4ee6a724d328270be69d","placeholder":"​","style":"IPY_MODEL_cb1e0868ff064802b7b07a7ea5ba99e6","value":" 4/4 [01:03&lt;00:00, 16.08s/it]"}},"551a13f70fd44365b92cbe4b8ef6cfc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b08f40110f9430f9de5bcc0aebf5b84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf20a796a2e34e329472e519aba7cfe3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96b376e622ef42ba95426fc8310f2fc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ec3061465fb4716916b08a900c43a96":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c066e176bfcd4ee6a724d328270be69d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb1e0868ff064802b7b07a7ea5ba99e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0b88e7e6b9a4172bfaaa61a02c140f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf88725ff8264c7fb5535e6bc3846c6f","IPY_MODEL_3f313d62c13d4e3289949d0357ee0c47","IPY_MODEL_6cfd43bb17ec4cff9658293fe67beada"],"layout":"IPY_MODEL_ecfc76ed7ded4d7f996702bac02af37a"}},"bf88725ff8264c7fb5535e6bc3846c6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06a15119b78244b288c7c84ed69288b0","placeholder":"​","style":"IPY_MODEL_c09f2b83e3d7415586dcc4555329d332","value":"100%"}},"3f313d62c13d4e3289949d0357ee0c47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a865c0f2d51c4c4b88679b27432df5b5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39be3817181a49fd8bf686a63d33e8c2","value":2}},"6cfd43bb17ec4cff9658293fe67beada":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_170e7c1162e643c3a1399f33520ea586","placeholder":"​","style":"IPY_MODEL_ecc23be158244fc893a7597ea6f660e7","value":" 2/2 [00:31&lt;00:00, 15.87s/it]"}},"ecfc76ed7ded4d7f996702bac02af37a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06a15119b78244b288c7c84ed69288b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c09f2b83e3d7415586dcc4555329d332":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a865c0f2d51c4c4b88679b27432df5b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39be3817181a49fd8bf686a63d33e8c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"170e7c1162e643c3a1399f33520ea586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecc23be158244fc893a7597ea6f660e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f2ff019e15f4dccb4fb14d03c9a48d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc688151c18d4929ae6002003e01fa28","IPY_MODEL_bb58692154a5421897aaa5b1efe4750c","IPY_MODEL_7f6629575bb549f882055a9873bdf2d3"],"layout":"IPY_MODEL_fa4bf27ef6b44d93aa0bea897a8333b7"}},"bc688151c18d4929ae6002003e01fa28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f4a873b38ac4b55a883a063dad5cee0","placeholder":"​","style":"IPY_MODEL_314867ec7deb4d7dacfbc8f7f572ec7a","value":"100%"}},"bb58692154a5421897aaa5b1efe4750c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac4b31fb561b4030af638765e8e5dfc8","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7a9bd13a37f4f74a8f457472c76417b","value":4}},"7f6629575bb549f882055a9873bdf2d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73c9fde8532144d08c5c2c58357403c0","placeholder":"​","style":"IPY_MODEL_dcaac50f620c4d408c98a05fc70ea669","value":" 4/4 [01:03&lt;00:00, 15.88s/it]"}},"fa4bf27ef6b44d93aa0bea897a8333b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f4a873b38ac4b55a883a063dad5cee0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"314867ec7deb4d7dacfbc8f7f572ec7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac4b31fb561b4030af638765e8e5dfc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7a9bd13a37f4f74a8f457472c76417b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73c9fde8532144d08c5c2c58357403c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcaac50f620c4d408c98a05fc70ea669":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c916bd6e5f4a4a018745cd9fd89beb80":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75b2f8fc1e5042a4910a0a0f0dc0a054","IPY_MODEL_d2bc8eaba2be41638eb4885d26f587f1","IPY_MODEL_4807876841e0491986fa8340b5bfe534"],"layout":"IPY_MODEL_c4f055f29b604c42a16f3e3c69dd651d"}},"75b2f8fc1e5042a4910a0a0f0dc0a054":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab61550561724ffc90b7aa93b93f36d5","placeholder":"​","style":"IPY_MODEL_1c71c562a6034c1e8f53438da61efefa","value":"100%"}},"d2bc8eaba2be41638eb4885d26f587f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af4986724938470984a0491084afa9a4","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81a96af4445142aab84a28f6f659e2e0","value":2}},"4807876841e0491986fa8340b5bfe534":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac79644bdd434980900d74b31559aab8","placeholder":"​","style":"IPY_MODEL_0f17b8a432e74b009e6e48598441a2aa","value":" 2/2 [00:33&lt;00:00, 16.88s/it]"}},"c4f055f29b604c42a16f3e3c69dd651d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab61550561724ffc90b7aa93b93f36d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c71c562a6034c1e8f53438da61efefa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af4986724938470984a0491084afa9a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81a96af4445142aab84a28f6f659e2e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac79644bdd434980900d74b31559aab8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f17b8a432e74b009e6e48598441a2aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e95d6c76a89444089d2f23c2e699cd4d":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_9c75bd90c33b4fe3a9eefa62cde622b6","IPY_MODEL_d9d1bcaceecc455ea90487e5b5936af6"],"layout":"IPY_MODEL_572850b6f55748dc976362ae5efb750d"}},"9c75bd90c33b4fe3a9eefa62cde622b6":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06c11dc7e2b445928ef2361b63659e10","placeholder":"​","style":"IPY_MODEL_a05bf836439e444c82360c49a785457d","value":"0.000 MB of 0.000 MB uploaded\r"}},"d9d1bcaceecc455ea90487e5b5936af6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aa2f942d67f459eaf0bc11c7c5f4f43","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e22485b3aa541c0a9808fb9eac76cad","value":1}},"572850b6f55748dc976362ae5efb750d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06c11dc7e2b445928ef2361b63659e10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a05bf836439e444c82360c49a785457d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1aa2f942d67f459eaf0bc11c7c5f4f43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e22485b3aa541c0a9808fb9eac76cad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e935d3eba194ee794733bf3d4365148":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_96f471ca881349ba95f745137a3b5b05","IPY_MODEL_ae9c2b2167e24cf188867156c18b686f"],"layout":"IPY_MODEL_9c628b0e3447482ba5f53963fe4fbef0"}},"96f471ca881349ba95f745137a3b5b05":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f308264465214dec89e156535d213a9f","placeholder":"​","style":"IPY_MODEL_d1825ba49004423f8b7236e10de203dc","value":"0.000 MB of 0.000 MB uploaded\r"}},"ae9c2b2167e24cf188867156c18b686f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_04a216d14d884718ab1deb80167460be","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_447b647550bf40f5b0a8a3a1e8ade7f7","value":1}},"9c628b0e3447482ba5f53963fe4fbef0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f308264465214dec89e156535d213a9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1825ba49004423f8b7236e10de203dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04a216d14d884718ab1deb80167460be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447b647550bf40f5b0a8a3a1e8ade7f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l4rfESY-BqBg","executionInfo":{"status":"ok","timestamp":1706696450823,"user_tz":-60,"elapsed":16309,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"f051a32a-7dca-4f06-90d5-53098bd8662f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting datasets\n","  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: dill, multiprocess, datasets\n","Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n"]}],"source":["!pip install transformers datasets"]},{"cell_type":"code","source":["import transformers, datasets\n","from transformers import OneFormerForUniversalSegmentation, OneFormerModel, OneFormerPreTrainedModel\n","from transformers import OneFormerConfig, OneFormerImageProcessor, OneFormerProcessor\n","from datasets import load_dataset\n","from huggingface_hub import notebook_login"],"metadata":{"id":"jtRCJpegCLgV","executionInfo":{"status":"ok","timestamp":1706782753270,"user_tz":-60,"elapsed":324,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305,"referenced_widgets":["d58146d581e94adb9ca1f1dc106d70e9","4b4c5a6cccb84222bc624e6d1af17c39","4f27513c65e44355a9e5c6bfd1d24dbe","e732eb9a49974d2cb6ced414db1a4d7f","a4a8435ce293415187d1e12a089bec50","13b3687c098e4c62847a3f5571e8e314","c5b1f28937034aafb555a79022b3e0c8","7671895eec804cfbaa1c98a9a6b21114","be4c5338c568429c8b7255e53df10afb","7fc89fef78c24255893d005b088c8e63","58a18369d69141aaa188c2fcb70daeab","9c1a0ac826644d9a89eb0c09821c95d3","f49982094e3245a7b6b922f9114a7b06","a5dc3b931c3143ce8d3a1349b6e49747","22988131975843caad42bf08268ad4fb","f6b6247ecad34587904074d7f6e92433","a05f486554404b3393c5b6e2f3954572","102928dc2f8c4b0c814e8c780a204c0a","d54d05dd986140c39f958ecf33e5fbfd","14c0d5a373204952aa68b2c817f46628","a4f04f4db6f340829fcfe947b1750f15","d4a703fa18f640efb88eff8ac743164a","e1a843b60cb349779621f5b940470ec9","c2e72d75ec694954a7ef23cbdaffdb08","e70a2d8124274bfeba8d554b2884836f","12dd72e8d1b741daad185a9cc21141e9","47e63821f3534b42bbabff2ae7ab6107","7e34f119725443da9b5fd2ab5e6860cf","433352a6ca63412ca0ab4f050a0092ef","0277473538c44fb0b63ffa81cde9a193","ed68b20c03b0441384f106e2da82f018","9716d332f39a4d659082d85166a7e830","2dac0d66f97b4bd4a0efdffa8a2cc814","87f22e33de6b4309afed320f63ca0665","192b5816b29049fab811f9a1e55b7386","897b066ee3e64b5392055883adadcccd","79eaa79a126341408dd0ed6a495d2f0c","0976555e01034c6d9f0c77f033e5c617","64db188b4f274b27af1dd9112d29e340","1ea941a9866d428fad9b4c28ff23e523","542035c36bf8466db42985c5dfe64078","afeb6b507dbc404ab5ea8da381bad272","0894b67e92284897b1fc1306d58a8703","1d23c50894b74d0cabbc95acf4983444","7ccd7217bf9f4358b1055546fae97cfa","4cfb69a120214afb8c822b92f6e0f0cb","c529586d2a7b49f0a63b0fda2d9b7391"]},"id":"XQkCTPf3FnMJ","executionInfo":{"status":"ok","timestamp":1706697362506,"user_tz":-60,"elapsed":405,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"2ea8b948-8372-44e5-b123-845d60ac705d"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d58146d581e94adb9ca1f1dc106d70e9"}},"metadata":{}}]},{"cell_type":"code","source":["dataset = load_dataset(\"BhavanaMalla/railsem19-semantic-expanded\")\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":569,"referenced_widgets":["958c20a881fe4d9e8bcbd8ca2c5795a5","af483c3e3a534fbbaeeee7d5d3f204d6","7ac7bc44e8db4197a6370d662ce826a6","e5edb5f53d0b40c2a838a2dc3c4fa4ee","f4cacd6e400f4d818b60700020e312a4","2aa1316f52ad403985ea1eb57f71fe05","95d2c0fccc374310a7edfbaea8bce0be","3426ae8613c346a2bdf62a784d78868c","89e6c152308a4b26ac033ee098d81e13","6f9f0f7f44e145649539e977af9957d5","967cedf3120b45d49732d3168de7f1a1","201d9e959c5d494ab4ed3b98956ec4d3","05895e250d1648aca8d257142ed6f2fd","d2e3cfb0d7b44c1f9f9b0ac5878eb23e","5ced3a882abe40259a70626a457d89c8","328024c2554f4a62aaf641336f0731cc","3011a91553534931b57702e2ecb5ced3","76bdf53c11114355949241d2bc19bd79","9a470c25cf9242e4bb439ae3c0b56ccd","f401a9107cad4b9d9e761928bfad880d","c6a068ae7c94440f8503179150af3766","767786e56b8140f2adf23bb5d0470783","0930c057500243c7b8db7405f39fe45c","709fd84d15e647e3ac6e0422489bd43f","2ea421b2a109420992168f94b98adbd1","e97e742018d842c98e8c9ad640a11f8b","e7273cda51604344b9f739bd02c6c1e3","978e7089d2a14c9cafc5b5fed03addf3","bd3db0c2b5b74daab4199bb4f3987ec2","a32b723aae5d485aa125e9d3e6a58e55","68035a97c80643168a6698769e8b9c36","5d97ae13da0f44009175a96600567587","7f8216de99854f2999ca617d025cfa7b","2c02a1f2b6044c39a73c8272ecfd07e1","1da7f95edc404194bd4a3629edaed358","157d0085c75b4439981a39f596d2d166","1dfda86f4e5149ba89a62a501b82b226","3aef08030ff04e30b1775c431b60b8b8","5c0e17493aaa4efcb7f3fb7d83fb3257","be5f6763fea0493787ea4e6ec74c34ce","abce4f5679784caaa0cc4427ace2fa68","2e12f61035a8406cbc4a89b6ac87adec","d770c05b7c2b4cc1a349eadbcdd32e57","6a4a6d7965694de9baaeb09890395668","fb2ef2470b604bc5b9aeb62a77185005","6e82bb3786034dd49a1ef907dd18a629","4fc66c9c488540f982924a201b30abf9","fa70b5c44d894700953676981a2def37","94837869b722476e84c9f1ddfa0a049e","4a04edd32306441aa34f0de2cf28b294","438fd47312cd416eae2d1be2d371ad9b","354fcfed5b5f476aba495aeea7c20346","08c6f82f37514b019afb83538c869440","0c8f038ce1164d00a161a650ad8ef309","d583e1bc8d444af49c4086ef6a706e57","e0d0ab09e6814f9ea8403c52e8f02eb3","cef4e8296ced4eb786f9974d5d6e4982","95d0952d7abb426cb6c53b180a0f8eb4","756f93f28cdd4223a9d0fd180627d701","633c5717b1ae46888f8d7d6b7913b187","6c1a0cb6df704f8593e96c1239a8e12c","9646411394764161817b5e654e4fc9e4","5b475083f61b42639e757655aee4b87f","1d1b7199bee24e29b75d4f66645e4c20","0fc9b39af61a4710948a2dabc3d7481e","39ca7cc3cfbe4851844a7c5c24b4c6d2","ebcf19bc58d7455e8fd5febd345508f7","9c6627d48dcc4c3883b29097c7a96b22","72e49a45999e4076aa1c88fbdd3f71d0","ba3a1daf2afd4b378eec8fff480ba108","4433399c6e8746f1a7c37a032cf5567b","8d2364cef71b4d2a9eef36ae122edf84","21ffbe9faf9744058749024111d0543b","24b28462aa4147a2bd86464d6865c571","85665e8c5b3f4cc6a7a19ff459c4ae9b","64b99881a80340b28d7649469bb82dda","637f262d7a4b477fb0458629b81d19c2","b04166124d0b481dad3121f77deb7844","d0c3af7e8b3a45eda1c1758006e7399b","4523534f1c19418197eeefb0eb0f6d22","6159c0c7ebce4c4db338cff1c5f53907","c02a215035ea4c20bec9cd1d68551ac2","d2d03a2dd4c14a5691e7d47cc7a77412","c4519a029b324245bf47724ed693779f","9e831e136e444a0389bb50f8d383fb7a","49b2134a7de74c53bcbca9db07785d89","82f72f99bdca4d47bcc27c54ee476de8","b2da09679fe944b598962c730023e012","96f736d0224844c699067966e5266639","6f7262ddb3b34888a909e4cc15a8939d","19e6429a29e84f0fa3edd67431b7ebc8","b7cb1dafbcff42659624359dcc2f8e53","b53a7a8f33d147919c6d9fae23f90780","bc8976f5ef8d496ba138d6310e92a96c","8c2524580bae4c3caf244f64dfae3ea9","82855a1dbd29441ab05aa3abe94a473b","855aa2c071a84d46bfaf2e78f96f7b3e","62c12dc5843940a286ae053b4d5ce930","5ec7b6bd240c41faa854f682261aebcf","cde5fc2164614629a7e8f2971339b004","0f23bbebb295400991bca53c4ab07b26","d0557a838dbc41c2a55f4290523c3a28","6e5f4eddd85447b88e02ceb0bab5ef43","15e5fb218bf140638b74d8f8a5649baa","16b929be3a1f48c2abb6590c60a8daf1","531a7f4b66384d47a2332e556cde1664","ea273861a7bf47969545f13c1d72c1c7","bfcc565748904753a0e91ebd0efd01fd","0683ec8c888f4af1983be1f808102d50","01816066754941fd8a554acd3c92e832","f4c8d06c27324f029965a3dd63b87ef9","dc07d1a1cf8a4a4ebe624cf2fd8e516b","fd85b202555c434f905a26adad3a513e","c68c4eec3c674416a031600637d367ba","c4da8b9261f94d828d5b423be2f37c9c","6d11d12c9f314e4185b856f76aa34207","7f68858bd8ef4b1bb78deefbf476f90c","f27173870623447d9ddb1a6ffcd18d93","4d39a1ed795c42caabf21e8b1bf974f4","bf0b95a0d3444912b610a38a273b3f95","a0f1be2ac4e249e4b6c725835a72865d","98ed7ad4c6b14fd1b23fe5b63ec7f997","ab695c7ca77f4af0894034637ab138b7","e396720e9cbe47ab8ec0c3ad2a9eaed2","6b0c298d7c494c6284c744d94b898540","b90436c9b10a45119c6fc0b38473c7e3","f76a2698841847ff89fe286bc4be4281","0bb04cf553374e9882d58ad8cef95d43","a196441574bf433d89f757344415545b","c732ef7832eb41378beedf5ce441ef8a","f949e6176c8f4e9bbe9f9d42c4a529cf","7aa419d331a345288fef88d70e1fbd82","4cdbaaf2d72e47ff9b6ce911577e2286","07143d4b3a4544aabd5c49fa76cb1dbc","32174f50ef854b9fba300acdfba6aaac","9f81280021134df5b0a6090f9ee32d3e","3a948744b7cb40e294110f990d4a9cc3","63951f9253f1483fa73b24ce70a3611e","1f6af85adae4400e8e3786bbf96cb92f","07be19f3aedf45a2babb41bf8338c87b","7ae1cb1d229840a581c9a5f7ff13d843","af3f79406a7e4da384f366ad20b37299","9cd184d512da4f5996de118b09d306db","b2b68ad379cc4d05888b6f1d8b4cbfb1","e1a5c3b43e9f4bdb90c498c9fc0dfd69","68a804f1470743a29976de1f2d86e87e","4304cc1b0e074d3fa2349a41ab10e847","647472b268754fcb8a9e71780f93f0dd","7c0491b7948e43a98a674c8ed24a8e05","fce210efc8764e6fbf6777e8b768c808","0f6613643d8648849d021b7eb9d6a988","2520b8b26b2241278d9302f803e53c13","2e472927b9424b54ad9c00782718eefe","ceaa49ae28594dfca45b7f8764ee5113"]},"id":"TTzJw2ZkFJaK","executionInfo":{"status":"ok","timestamp":1706697851894,"user_tz":-60,"elapsed":460230,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"a59b8f46-f191-4e21-98b2-dd045dceeba4"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/804 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"958c20a881fe4d9e8bcbd8ca2c5795a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/454M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"201d9e959c5d494ab4ed3b98956ec4d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/458M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0930c057500243c7b8db7405f39fe45c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/452M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c02a1f2b6044c39a73c8272ecfd07e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/448M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb2ef2470b604bc5b9aeb62a77185005"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/455M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0d0ab09e6814f9ea8403c52e8f02eb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/456M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebcf19bc58d7455e8fd5febd345508f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/453M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b04166124d0b481dad3121f77deb7844"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/456M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f736d0224844c699067966e5266639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/455M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde5fc2164614629a7e8f2971339b004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/451M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c8d06c27324f029965a3dd63b87ef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/459M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98ed7ad4c6b14fd1b23fe5b63ec7f997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/447M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cdbaaf2d72e47ff9b6ce911577e2286"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating data split:   0%|          | 0/8500 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b68ad379cc4d05888b6f1d8b4cbfb1"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    data: Dataset({\n","        features: ['image', 'semantic_mask_label', 'objs_labels', 'objs_bboxes', 'objs_polygons', 'objs_lpolyline', 'objs_polyline-pair', 'img_Height', 'img_Width', 'img_Name'],\n","        num_rows: 8500\n","    })\n","})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_cityscapes_swin_large\", is_training=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240,"referenced_widgets":["6ad03020c1114423a65ee61c76360973","14342fdbba834ffbb9ef9570ff254b31","96389581bd8848149314ad6aa50235bb","3b0bfe57e2b342c289e23c22f38cfee3","01894ffcdb3a4edf9c3da707cdcfa3c2","4c78fa4d69204ea3b67793be887d7bec","1a27b6ac1eb44a889a94ea82cc48f805","1a3229a00fec4595a3efdfcb1467d082","83dac849074a43d2bad33b134b85f469","b734af7174334237b325e7cdaae94c7e","2a4b428325284a4395cdcb3323031278","ae520bde9a0741e39eede540b16c3c1b","f7933e1e987b46aa9c1f008a6fd8f7a9","274485a08e4e4f9da12eea1a48fea1b8","6701034b7cb748dca2a42338e4c4d7f2","2ddae2b99f194821b0895551963ee456","03770a6221f94126a370b938fe5ef6c5","35d09cfe29f646998e150a6335cb35f4","fc9f0ac795064d60894b00a5890e4b37","28ee91a62e324dbf8749ef50dab840ae","aa625967e52945329cc8e8d7ba8c0539","ab5a9d2efb8f4580bc74173ce1101354"]},"id":"jkVJyF7iF7Rb","executionInfo":{"status":"ok","timestamp":1706782788744,"user_tz":-60,"elapsed":25740,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"015a3f1d-5d15-4bea-bdeb-d5dbdee224eb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/77.7k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad03020c1114423a65ee61c76360973"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/879M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae520bde9a0741e39eede540b16c3c1b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized: ['model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_projector.layers.1.0.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMUL-w65HxEd","executionInfo":{"status":"ok","timestamp":1706782794472,"user_tz":-60,"elapsed":331,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"5cc89935-348c-430e-db75-40d71068807f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerForUniversalSegmentation(\n","  (model): OneFormerModel(\n","    (pixel_level_module): OneFormerPixelLevelModule(\n","      (encoder): SwinBackbone(\n","        (embeddings): SwinEmbeddings(\n","          (patch_embeddings): SwinPatchEmbeddings(\n","            (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n","          )\n","          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (encoder): SwinEncoder(\n","          (layers): ModuleList(\n","            (0): SwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x SwinLayer(\n","                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (attention): SwinAttention(\n","                    (self): SwinSelfAttention(\n","                      (query): Linear(in_features=192, out_features=192, bias=True)\n","                      (key): Linear(in_features=192, out_features=192, bias=True)\n","                      (value): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): SwinSelfOutput(\n","                      (dense): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): SwinDropPath(p=0.3)\n","                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): SwinIntermediate(\n","                    (dense): Linear(in_features=192, out_features=768, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): SwinOutput(\n","                    (dense): Linear(in_features=768, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): SwinPatchMerging(\n","                (reduction): Linear(in_features=768, out_features=384, bias=False)\n","                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (1): SwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x SwinLayer(\n","                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (attention): SwinAttention(\n","                    (self): SwinSelfAttention(\n","                      (query): Linear(in_features=384, out_features=384, bias=True)\n","                      (key): Linear(in_features=384, out_features=384, bias=True)\n","                      (value): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): SwinSelfOutput(\n","                      (dense): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): SwinDropPath(p=0.3)\n","                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): SwinIntermediate(\n","                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): SwinOutput(\n","                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): SwinPatchMerging(\n","                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n","                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (2): SwinStage(\n","              (blocks): ModuleList(\n","                (0-17): 18 x SwinLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): SwinAttention(\n","                    (self): SwinSelfAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): SwinSelfOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): SwinDropPath(p=0.3)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): SwinIntermediate(\n","                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): SwinOutput(\n","                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): SwinPatchMerging(\n","                (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n","                (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (3): SwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x SwinLayer(\n","                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (attention): SwinAttention(\n","                    (self): SwinSelfAttention(\n","                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): SwinSelfOutput(\n","                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): SwinDropPath(p=0.3)\n","                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): SwinIntermediate(\n","                    (dense): Linear(in_features=1536, out_features=6144, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): SwinOutput(\n","                    (dense): Linear(in_features=6144, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","            )\n","          )\n","        )\n","        (hidden_states_norms): ModuleDict(\n","          (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (decoder): OneFormerPixelDecoder(\n","        (position_embedding): OneFormerSinePositionEmbedding()\n","        (input_projections): ModuleList(\n","          (0): Sequential(\n","            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","          (1): Sequential(\n","            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","          (2): Sequential(\n","            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","        )\n","        (encoder): OneFormerPixelDecoderEncoderOnly(\n","          (layers): ModuleList(\n","            (0-5): 6 x OneFormerPixelDecoderEncoderLayer(\n","              (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","              )\n","              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","        )\n","        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (adapter_1): Sequential(\n","          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        )\n","        (layer_1): Sequential(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          (2): ReLU()\n","        )\n","      )\n","    )\n","    (transformer_module): OneFormerTransformerModule(\n","      (position_embedder): OneFormerSinePositionEmbedding()\n","      (queries_embedder): Embedding(250, 256)\n","      (decoder): OneFormerTransformerDecoder(\n","        (query_transformer): OneFormerTransformerDecoderQueryTransformer(\n","          (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(\n","            (layers): ModuleList(\n","              (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(\n","                (self_attn): MultiheadAttention(\n","                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","                )\n","                (multihead_attn): MultiheadAttention(\n","                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","                )\n","                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","                (dropout1): Dropout(p=0.1, inplace=False)\n","                (dropout2): Dropout(p=0.1, inplace=False)\n","                (dropout3): Dropout(p=0.1, inplace=False)\n","                (activation): ReLU()\n","              )\n","            )\n","            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (layers): ModuleList(\n","          (0-8): 9 x OneFormerTransformerDecoderLayer(\n","            (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(\n","              (multihead_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","              (activation): ReLU()\n","            )\n","            (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(\n","              (self_attn): OneFormerAttention(\n","                (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","              )\n","              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","              (activation): ReLU()\n","            )\n","            (ffn): OneFormerTransformerDecoderFFNLayer(\n","              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (activation): ReLU()\n","            )\n","          )\n","        )\n","        (query_input_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (class_embed): Linear(in_features=256, out_features=20, bias=True)\n","        (mask_embed): OneFormerMLPPredictionHead(\n","          (layers): Sequential(\n","            (0): PredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): ReLU()\n","            )\n","            (1): PredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): ReLU()\n","            )\n","            (2): PredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): Identity()\n","            )\n","          )\n","        )\n","      )\n","      (level_embed): Embedding(3, 256)\n","    )\n","    (task_encoder): OneFormerTaskModel(\n","      (task_mlp): OneFormerMLPPredictionHead(\n","        (layers): Sequential(\n","          (0): PredictionBlock(\n","            (0): Linear(in_features=77, out_features=256, bias=True)\n","            (1): ReLU()\n","          )\n","          (1): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Identity()\n","          )\n","        )\n","      )\n","    )\n","    (text_mapper): OneFormerTextMapper(\n","      (text_encoder): OneFormerTextEncoder(\n","        (transformer): OneFormerTextTransformer(\n","          (layers): Sequential(\n","            (0): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (1): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (2): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (3): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (4): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (5): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","        )\n","        (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (token_embedding): Embedding(49408, 256)\n","      )\n","      (text_projector): OneFormerMLPPredictionHead(\n","        (layers): Sequential(\n","          (0): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): ReLU()\n","          )\n","          (1): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Identity()\n","          )\n","        )\n","      )\n","      (prompt_ctx): Embedding(16, 256)\n","    )\n","  )\n","  (matcher): OneFormerHungarianMatcher()\n","  (criterion): OneFormerLoss(\n","    (matcher): OneFormerHungarianMatcher()\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["model.model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZK-iatPISOQ","executionInfo":{"status":"ok","timestamp":1706782808607,"user_tz":-60,"elapsed":287,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d8c40b57-b970-4b8d-a23b-d6e6ddf5146a"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerModel(\n","  (pixel_level_module): OneFormerPixelLevelModule(\n","    (encoder): SwinBackbone(\n","      (embeddings): SwinEmbeddings(\n","        (patch_embeddings): SwinPatchEmbeddings(\n","          (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n","        )\n","        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): SwinEncoder(\n","        (layers): ModuleList(\n","          (0): SwinStage(\n","            (blocks): ModuleList(\n","              (0-1): 2 x SwinLayer(\n","                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                (attention): SwinAttention(\n","                  (self): SwinSelfAttention(\n","                    (query): Linear(in_features=192, out_features=192, bias=True)\n","                    (key): Linear(in_features=192, out_features=192, bias=True)\n","                    (value): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): SwinSelfOutput(\n","                    (dense): Linear(in_features=192, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (drop_path): SwinDropPath(p=0.3)\n","                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                (intermediate): SwinIntermediate(\n","                  (dense): Linear(in_features=192, out_features=768, bias=True)\n","                  (intermediate_act_fn): GELUActivation()\n","                )\n","                (output): SwinOutput(\n","                  (dense): Linear(in_features=768, out_features=192, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","            )\n","            (downsample): SwinPatchMerging(\n","              (reduction): Linear(in_features=768, out_features=384, bias=False)\n","              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","          (1): SwinStage(\n","            (blocks): ModuleList(\n","              (0-1): 2 x SwinLayer(\n","                (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (attention): SwinAttention(\n","                  (self): SwinSelfAttention(\n","                    (query): Linear(in_features=384, out_features=384, bias=True)\n","                    (key): Linear(in_features=384, out_features=384, bias=True)\n","                    (value): Linear(in_features=384, out_features=384, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): SwinSelfOutput(\n","                    (dense): Linear(in_features=384, out_features=384, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (drop_path): SwinDropPath(p=0.3)\n","                (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                (intermediate): SwinIntermediate(\n","                  (dense): Linear(in_features=384, out_features=1536, bias=True)\n","                  (intermediate_act_fn): GELUActivation()\n","                )\n","                (output): SwinOutput(\n","                  (dense): Linear(in_features=1536, out_features=384, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","            )\n","            (downsample): SwinPatchMerging(\n","              (reduction): Linear(in_features=1536, out_features=768, bias=False)\n","              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","          (2): SwinStage(\n","            (blocks): ModuleList(\n","              (0-17): 18 x SwinLayer(\n","                (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (attention): SwinAttention(\n","                  (self): SwinSelfAttention(\n","                    (query): Linear(in_features=768, out_features=768, bias=True)\n","                    (key): Linear(in_features=768, out_features=768, bias=True)\n","                    (value): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): SwinSelfOutput(\n","                    (dense): Linear(in_features=768, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (drop_path): SwinDropPath(p=0.3)\n","                (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (intermediate): SwinIntermediate(\n","                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                  (intermediate_act_fn): GELUActivation()\n","                )\n","                (output): SwinOutput(\n","                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","            )\n","            (downsample): SwinPatchMerging(\n","              (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n","              (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","          (3): SwinStage(\n","            (blocks): ModuleList(\n","              (0-1): 2 x SwinLayer(\n","                (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                (attention): SwinAttention(\n","                  (self): SwinSelfAttention(\n","                    (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                    (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                    (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                  (output): SwinSelfOutput(\n","                    (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (drop_path): SwinDropPath(p=0.3)\n","                (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                (intermediate): SwinIntermediate(\n","                  (dense): Linear(in_features=1536, out_features=6144, bias=True)\n","                  (intermediate_act_fn): GELUActivation()\n","                )\n","                (output): SwinOutput(\n","                  (dense): Linear(in_features=6144, out_features=1536, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","            )\n","          )\n","        )\n","      )\n","      (hidden_states_norms): ModuleDict(\n","        (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","        (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","        (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (decoder): OneFormerPixelDecoder(\n","      (position_embedding): OneFormerSinePositionEmbedding()\n","      (input_projections): ModuleList(\n","        (0): Sequential(\n","          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        )\n","        (1): Sequential(\n","          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        )\n","        (2): Sequential(\n","          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        )\n","      )\n","      (encoder): OneFormerPixelDecoderEncoderOnly(\n","        (layers): ModuleList(\n","          (0-5): 6 x OneFormerPixelDecoderEncoderLayer(\n","            (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","              (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","              (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","              (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","            )\n","            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (adapter_1): Sequential(\n","        (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      )\n","      (layer_1): Sequential(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        (2): ReLU()\n","      )\n","    )\n","  )\n","  (transformer_module): OneFormerTransformerModule(\n","    (position_embedder): OneFormerSinePositionEmbedding()\n","    (queries_embedder): Embedding(250, 256)\n","    (decoder): OneFormerTransformerDecoder(\n","      (query_transformer): OneFormerTransformerDecoderQueryTransformer(\n","        (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(\n","          (layers): ModuleList(\n","            (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (multihead_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (dropout1): Dropout(p=0.1, inplace=False)\n","              (dropout2): Dropout(p=0.1, inplace=False)\n","              (dropout3): Dropout(p=0.1, inplace=False)\n","              (activation): ReLU()\n","            )\n","          )\n","          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (layers): ModuleList(\n","        (0-8): 9 x OneFormerTransformerDecoderLayer(\n","          (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","            (activation): ReLU()\n","          )\n","          (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(\n","            (self_attn): OneFormerAttention(\n","              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","            )\n","            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","            (activation): ReLU()\n","          )\n","          (ffn): OneFormerTransformerDecoderFFNLayer(\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (activation): ReLU()\n","          )\n","        )\n","      )\n","      (query_input_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (class_embed): Linear(in_features=256, out_features=20, bias=True)\n","      (mask_embed): OneFormerMLPPredictionHead(\n","        (layers): Sequential(\n","          (0): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): ReLU()\n","          )\n","          (1): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): ReLU()\n","          )\n","          (2): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Identity()\n","          )\n","        )\n","      )\n","    )\n","    (level_embed): Embedding(3, 256)\n","  )\n","  (task_encoder): OneFormerTaskModel(\n","    (task_mlp): OneFormerMLPPredictionHead(\n","      (layers): Sequential(\n","        (0): PredictionBlock(\n","          (0): Linear(in_features=77, out_features=256, bias=True)\n","          (1): ReLU()\n","        )\n","        (1): PredictionBlock(\n","          (0): Linear(in_features=256, out_features=256, bias=True)\n","          (1): Identity()\n","        )\n","      )\n","    )\n","  )\n","  (text_mapper): OneFormerTextMapper(\n","    (text_encoder): OneFormerTextEncoder(\n","      (transformer): OneFormerTextTransformer(\n","        (layers): Sequential(\n","          (0): OneFormerTextTransformerLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (mlp): OneFormerTextMLP(\n","              (activation_fn): QuickGELUActivation()\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","            )\n","            (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (1): OneFormerTextTransformerLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (mlp): OneFormerTextMLP(\n","              (activation_fn): QuickGELUActivation()\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","            )\n","            (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (2): OneFormerTextTransformerLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (mlp): OneFormerTextMLP(\n","              (activation_fn): QuickGELUActivation()\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","            )\n","            (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (3): OneFormerTextTransformerLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (mlp): OneFormerTextMLP(\n","              (activation_fn): QuickGELUActivation()\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","            )\n","            (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (4): OneFormerTextTransformerLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (mlp): OneFormerTextMLP(\n","              (activation_fn): QuickGELUActivation()\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","            )\n","            (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (5): OneFormerTextTransformerLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (mlp): OneFormerTextMLP(\n","              (activation_fn): QuickGELUActivation()\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","            )\n","            (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (token_embedding): Embedding(49408, 256)\n","    )\n","    (text_projector): OneFormerMLPPredictionHead(\n","      (layers): Sequential(\n","        (0): PredictionBlock(\n","          (0): Linear(in_features=256, out_features=256, bias=True)\n","          (1): ReLU()\n","        )\n","        (1): PredictionBlock(\n","          (0): Linear(in_features=256, out_features=256, bias=True)\n","          (1): Identity()\n","        )\n","      )\n","    )\n","    (prompt_ctx): Embedding(16, 256)\n","  )\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["model.model.pixel_level_module"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Hk_UsE-HyP_","executionInfo":{"status":"ok","timestamp":1706698012263,"user_tz":-60,"elapsed":247,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"5cd1a3f1-e1c8-44ce-ee5c-5c21cbed0c5a"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerPixelLevelModule(\n","  (encoder): SwinBackbone(\n","    (embeddings): SwinEmbeddings(\n","      (patch_embeddings): SwinPatchEmbeddings(\n","        (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n","      )\n","      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): SwinEncoder(\n","      (layers): ModuleList(\n","        (0): SwinStage(\n","          (blocks): ModuleList(\n","            (0-1): 2 x SwinLayer(\n","              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","              (attention): SwinAttention(\n","                (self): SwinSelfAttention(\n","                  (query): Linear(in_features=192, out_features=192, bias=True)\n","                  (key): Linear(in_features=192, out_features=192, bias=True)\n","                  (value): Linear(in_features=192, out_features=192, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): SwinSelfOutput(\n","                  (dense): Linear(in_features=192, out_features=192, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (drop_path): SwinDropPath(p=0.3)\n","              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","              (intermediate): SwinIntermediate(\n","                (dense): Linear(in_features=192, out_features=768, bias=True)\n","                (intermediate_act_fn): GELUActivation()\n","              )\n","              (output): SwinOutput(\n","                (dense): Linear(in_features=768, out_features=192, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","          )\n","          (downsample): SwinPatchMerging(\n","            (reduction): Linear(in_features=768, out_features=384, bias=False)\n","            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (1): SwinStage(\n","          (blocks): ModuleList(\n","            (0-1): 2 x SwinLayer(\n","              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (attention): SwinAttention(\n","                (self): SwinSelfAttention(\n","                  (query): Linear(in_features=384, out_features=384, bias=True)\n","                  (key): Linear(in_features=384, out_features=384, bias=True)\n","                  (value): Linear(in_features=384, out_features=384, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): SwinSelfOutput(\n","                  (dense): Linear(in_features=384, out_features=384, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (drop_path): SwinDropPath(p=0.3)\n","              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              (intermediate): SwinIntermediate(\n","                (dense): Linear(in_features=384, out_features=1536, bias=True)\n","                (intermediate_act_fn): GELUActivation()\n","              )\n","              (output): SwinOutput(\n","                (dense): Linear(in_features=1536, out_features=384, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","          )\n","          (downsample): SwinPatchMerging(\n","            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n","            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (2): SwinStage(\n","          (blocks): ModuleList(\n","            (0-17): 18 x SwinLayer(\n","              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (attention): SwinAttention(\n","                (self): SwinSelfAttention(\n","                  (query): Linear(in_features=768, out_features=768, bias=True)\n","                  (key): Linear(in_features=768, out_features=768, bias=True)\n","                  (value): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): SwinSelfOutput(\n","                  (dense): Linear(in_features=768, out_features=768, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (drop_path): SwinDropPath(p=0.3)\n","              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (intermediate): SwinIntermediate(\n","                (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                (intermediate_act_fn): GELUActivation()\n","              )\n","              (output): SwinOutput(\n","                (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","          )\n","          (downsample): SwinPatchMerging(\n","            (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n","            (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (3): SwinStage(\n","          (blocks): ModuleList(\n","            (0-1): 2 x SwinLayer(\n","              (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","              (attention): SwinAttention(\n","                (self): SwinSelfAttention(\n","                  (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                  (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                  (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","                (output): SwinSelfOutput(\n","                  (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                  (dropout): Dropout(p=0.0, inplace=False)\n","                )\n","              )\n","              (drop_path): SwinDropPath(p=0.3)\n","              (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","              (intermediate): SwinIntermediate(\n","                (dense): Linear(in_features=1536, out_features=6144, bias=True)\n","                (intermediate_act_fn): GELUActivation()\n","              )\n","              (output): SwinOutput(\n","                (dense): Linear(in_features=6144, out_features=1536, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (hidden_states_norms): ModuleDict(\n","      (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","      (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (decoder): OneFormerPixelDecoder(\n","    (position_embedding): OneFormerSinePositionEmbedding()\n","    (input_projections): ModuleList(\n","      (0): Sequential(\n","        (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      )\n","      (1): Sequential(\n","        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      )\n","      (2): Sequential(\n","        (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      )\n","    )\n","    (encoder): OneFormerPixelDecoderEncoderOnly(\n","      (layers): ModuleList(\n","        (0-5): 6 x OneFormerPixelDecoderEncoderLayer(\n","          (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","            (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","            (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (adapter_1): Sequential(\n","      (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","    )\n","    (layer_1): Sequential(\n","      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","      (2): ReLU()\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["model.model.pixel_level_module.encoder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tw1EbKz4ILcp","executionInfo":{"status":"ok","timestamp":1706699807910,"user_tz":-60,"elapsed":244,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"4f591b05-4b30-4cd6-fa37-447b3dc75fc0"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SwinBackbone(\n","  (embeddings): SwinEmbeddings(\n","    (patch_embeddings): SwinPatchEmbeddings(\n","      (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n","    )\n","    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.0, inplace=False)\n","  )\n","  (encoder): SwinEncoder(\n","    (layers): ModuleList(\n","      (0): SwinStage(\n","        (blocks): ModuleList(\n","          (0-1): 2 x SwinLayer(\n","            (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","            (attention): SwinAttention(\n","              (self): SwinSelfAttention(\n","                (query): Linear(in_features=192, out_features=192, bias=True)\n","                (key): Linear(in_features=192, out_features=192, bias=True)\n","                (value): Linear(in_features=192, out_features=192, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): SwinSelfOutput(\n","                (dense): Linear(in_features=192, out_features=192, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (drop_path): SwinDropPath(p=0.3)\n","            (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","            (intermediate): SwinIntermediate(\n","              (dense): Linear(in_features=192, out_features=768, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): SwinOutput(\n","              (dense): Linear(in_features=768, out_features=192, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","        (downsample): SwinPatchMerging(\n","          (reduction): Linear(in_features=768, out_features=384, bias=False)\n","          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (1): SwinStage(\n","        (blocks): ModuleList(\n","          (0-1): 2 x SwinLayer(\n","            (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","            (attention): SwinAttention(\n","              (self): SwinSelfAttention(\n","                (query): Linear(in_features=384, out_features=384, bias=True)\n","                (key): Linear(in_features=384, out_features=384, bias=True)\n","                (value): Linear(in_features=384, out_features=384, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): SwinSelfOutput(\n","                (dense): Linear(in_features=384, out_features=384, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (drop_path): SwinDropPath(p=0.3)\n","            (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","            (intermediate): SwinIntermediate(\n","              (dense): Linear(in_features=384, out_features=1536, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): SwinOutput(\n","              (dense): Linear(in_features=1536, out_features=384, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","        (downsample): SwinPatchMerging(\n","          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n","          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (2): SwinStage(\n","        (blocks): ModuleList(\n","          (0-17): 18 x SwinLayer(\n","            (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (attention): SwinAttention(\n","              (self): SwinSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): SwinSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (drop_path): SwinDropPath(p=0.3)\n","            (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (intermediate): SwinIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): SwinOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","        (downsample): SwinPatchMerging(\n","          (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n","          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (3): SwinStage(\n","        (blocks): ModuleList(\n","          (0-1): 2 x SwinLayer(\n","            (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","            (attention): SwinAttention(\n","              (self): SwinSelfAttention(\n","                (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): SwinSelfOutput(\n","                (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (drop_path): SwinDropPath(p=0.3)\n","            (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","            (intermediate): SwinIntermediate(\n","              (dense): Linear(in_features=1536, out_features=6144, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): SwinOutput(\n","              (dense): Linear(in_features=6144, out_features=1536, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (hidden_states_norms): ModuleDict(\n","    (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","    (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","    (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","  )\n",")"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["model.model.pixel_level_module.encoder.encoder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dQHIHjFeOeGe","executionInfo":{"status":"ok","timestamp":1706699656072,"user_tz":-60,"elapsed":320,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d5bf6d8f-170e-4708-85b6-4386a5193ed6"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SwinEncoder(\n","  (layers): ModuleList(\n","    (0): SwinStage(\n","      (blocks): ModuleList(\n","        (0-1): 2 x SwinLayer(\n","          (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (attention): SwinAttention(\n","            (self): SwinSelfAttention(\n","              (query): Linear(in_features=192, out_features=192, bias=True)\n","              (key): Linear(in_features=192, out_features=192, bias=True)\n","              (value): Linear(in_features=192, out_features=192, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): SwinSelfOutput(\n","              (dense): Linear(in_features=192, out_features=192, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (drop_path): SwinDropPath(p=0.3)\n","          (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (intermediate): SwinIntermediate(\n","            (dense): Linear(in_features=192, out_features=768, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): SwinOutput(\n","            (dense): Linear(in_features=768, out_features=192, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","      (downsample): SwinPatchMerging(\n","        (reduction): Linear(in_features=768, out_features=384, bias=False)\n","        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (1): SwinStage(\n","      (blocks): ModuleList(\n","        (0-1): 2 x SwinLayer(\n","          (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (attention): SwinAttention(\n","            (self): SwinSelfAttention(\n","              (query): Linear(in_features=384, out_features=384, bias=True)\n","              (key): Linear(in_features=384, out_features=384, bias=True)\n","              (value): Linear(in_features=384, out_features=384, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): SwinSelfOutput(\n","              (dense): Linear(in_features=384, out_features=384, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (drop_path): SwinDropPath(p=0.3)\n","          (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (intermediate): SwinIntermediate(\n","            (dense): Linear(in_features=384, out_features=1536, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): SwinOutput(\n","            (dense): Linear(in_features=1536, out_features=384, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","      (downsample): SwinPatchMerging(\n","        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n","        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (2): SwinStage(\n","      (blocks): ModuleList(\n","        (0-17): 18 x SwinLayer(\n","          (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (attention): SwinAttention(\n","            (self): SwinSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): SwinSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (drop_path): SwinDropPath(p=0.3)\n","          (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (intermediate): SwinIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): SwinOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","      (downsample): SwinPatchMerging(\n","        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n","        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (3): SwinStage(\n","      (blocks): ModuleList(\n","        (0-1): 2 x SwinLayer(\n","          (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","          (attention): SwinAttention(\n","            (self): SwinSelfAttention(\n","              (query): Linear(in_features=1536, out_features=1536, bias=True)\n","              (key): Linear(in_features=1536, out_features=1536, bias=True)\n","              (value): Linear(in_features=1536, out_features=1536, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): SwinSelfOutput(\n","              (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (drop_path): SwinDropPath(p=0.3)\n","          (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","          (intermediate): SwinIntermediate(\n","            (dense): Linear(in_features=1536, out_features=6144, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): SwinOutput(\n","            (dense): Linear(in_features=6144, out_features=1536, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["model.model.pixel_level_module.encoder.hidden_states_norms"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02hPE5QYxYpy","executionInfo":{"status":"ok","timestamp":1706708804510,"user_tz":-60,"elapsed":224,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"c10a8146-a9f9-4c89-8f18-de7318915853"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ModuleDict(\n","  (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","  (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","  (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",")"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["model.model.pixel_level_module.decoder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oP_nlDHFImCD","executionInfo":{"status":"ok","timestamp":1706698145930,"user_tz":-60,"elapsed":10,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"c61cffb2-da98-45e4-8a6e-a94090abe8b2"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerPixelDecoder(\n","  (position_embedding): OneFormerSinePositionEmbedding()\n","  (input_projections): ModuleList(\n","    (0): Sequential(\n","      (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","    )\n","    (1): Sequential(\n","      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","    )\n","    (2): Sequential(\n","      (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n","      (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","    )\n","  )\n","  (encoder): OneFormerPixelDecoderEncoderOnly(\n","    (layers): ModuleList(\n","      (0-5): 6 x OneFormerPixelDecoderEncoderLayer(\n","        (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","          (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","          (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","          (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","  (adapter_1): Sequential(\n","    (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","  )\n","  (layer_1): Sequential(\n","    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","    (2): ReLU()\n","  )\n",")"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["model.model.pixel_level_module.decoder.encoder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zw2h7JbnPRdn","executionInfo":{"status":"ok","timestamp":1706699930037,"user_tz":-60,"elapsed":251,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"8846f773-7553-4557-fcb8-b499b35fb429"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerPixelDecoderEncoderOnly(\n","  (layers): ModuleList(\n","    (0-5): 6 x OneFormerPixelDecoderEncoderLayer(\n","      (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","        (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","        (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","        (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","        (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","      )\n","      (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","      (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","      (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["model.model.pixel_level_module.decoder.mask_projection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMUr79PbUT07","executionInfo":{"status":"ok","timestamp":1706701193007,"user_tz":-60,"elapsed":8,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d0a46c2e-b0e6-4b6d-a668-1633e3235c6e"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["model.model.pixel_level_module.decoder.adapter_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTyebccqUY_7","executionInfo":{"status":"ok","timestamp":1706701206084,"user_tz":-60,"elapsed":245,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"846c8deb-884b-4b42-a522-a07cafaf9d6f"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","  (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",")"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["model.model.pixel_level_module.decoder.layer_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-hRQYrdUdsb","executionInfo":{"status":"ok","timestamp":1706701225818,"user_tz":-60,"elapsed":228,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"5337a038-1e7f-487d-8fbe-6ed0f3bc0199"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","  (2): ReLU()\n",")"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["model.model.transformer_module #transformer decoder N=250 i.e Q is 250, 256"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9dJH6PrIv4S","executionInfo":{"status":"ok","timestamp":1706698428349,"user_tz":-60,"elapsed":350,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"cc58790c-f8e4-4cab-d56f-85e4e72676c0"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTransformerModule(\n","  (position_embedder): OneFormerSinePositionEmbedding()\n","  (queries_embedder): Embedding(250, 256)\n","  (decoder): OneFormerTransformerDecoder(\n","    (query_transformer): OneFormerTransformerDecoderQueryTransformer(\n","      (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(\n","        (layers): ModuleList(\n","          (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(\n","            (self_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (multihead_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (dropout1): Dropout(p=0.1, inplace=False)\n","            (dropout2): Dropout(p=0.1, inplace=False)\n","            (dropout3): Dropout(p=0.1, inplace=False)\n","            (activation): ReLU()\n","          )\n","        )\n","        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    (layers): ModuleList(\n","      (0-8): 9 x OneFormerTransformerDecoderLayer(\n","        (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(\n","          (multihead_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (activation): ReLU()\n","        )\n","        (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(\n","          (self_attn): OneFormerAttention(\n","            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (activation): ReLU()\n","        )\n","        (ffn): OneFormerTransformerDecoderFFNLayer(\n","          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (activation): ReLU()\n","        )\n","      )\n","    )\n","    (query_input_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (class_embed): Linear(in_features=256, out_features=20, bias=True)\n","    (mask_embed): OneFormerMLPPredictionHead(\n","      (layers): Sequential(\n","        (0): PredictionBlock(\n","          (0): Linear(in_features=256, out_features=256, bias=True)\n","          (1): ReLU()\n","        )\n","        (1): PredictionBlock(\n","          (0): Linear(in_features=256, out_features=256, bias=True)\n","          (1): ReLU()\n","        )\n","        (2): PredictionBlock(\n","          (0): Linear(in_features=256, out_features=256, bias=True)\n","          (1): Identity()\n","        )\n","      )\n","    )\n","  )\n","  (level_embed): Embedding(3, 256)\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["model.model.transformer_module.position_embedder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8PbtK_vQ5ch","executionInfo":{"status":"ok","timestamp":1706700298802,"user_tz":-60,"elapsed":360,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"0751c8eb-7f89-4e5f-af42-e412b4161482"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerSinePositionEmbedding()"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["model.model.transformer_module.queries_embedder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BVUM9rFVRBPd","executionInfo":{"status":"ok","timestamp":1706700342231,"user_tz":-60,"elapsed":219,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"0b161962-6269-40a7-8dfe-1ec44d0404c0"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(250, 256)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["model.model.transformer_module.decoder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IsLAlYgRJ0vj","executionInfo":{"status":"ok","timestamp":1706706866276,"user_tz":-60,"elapsed":371,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"248a4049-a06a-4bdf-ad99-ab8ed28e8297"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTransformerDecoder(\n","  (query_transformer): OneFormerTransformerDecoderQueryTransformer(\n","    (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(\n","      (layers): ModuleList(\n","        (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (multihead_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (dropout1): Dropout(p=0.1, inplace=False)\n","          (dropout2): Dropout(p=0.1, inplace=False)\n","          (dropout3): Dropout(p=0.1, inplace=False)\n","          (activation): ReLU()\n","        )\n","      )\n","      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (layers): ModuleList(\n","    (0-8): 9 x OneFormerTransformerDecoderLayer(\n","      (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (activation): ReLU()\n","      )\n","      (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(\n","        (self_attn): OneFormerAttention(\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (activation): ReLU()\n","      )\n","      (ffn): OneFormerTransformerDecoderFFNLayer(\n","        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (activation): ReLU()\n","      )\n","    )\n","  )\n","  (query_input_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","  (class_embed): Linear(in_features=256, out_features=20, bias=True)\n","  (mask_embed): OneFormerMLPPredictionHead(\n","    (layers): Sequential(\n","      (0): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): ReLU()\n","      )\n","      (2): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): Identity()\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["model.model.transformer_module.level_embed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNFjwMjfSlQV","executionInfo":{"status":"ok","timestamp":1706700901728,"user_tz":-60,"elapsed":220,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"8294bfe3-54b9-48bc-ca99-054b052c51c8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(3, 256)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["model.model.transformer_module.decoder.query_transformer # this is 2 layer transformer between 1/4 flattened and object queries Q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6ddHCsKRPUK","executionInfo":{"status":"ok","timestamp":1706700387303,"user_tz":-60,"elapsed":22,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"3018fd23-66df-477a-f9d8-0f264d673de3"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTransformerDecoderQueryTransformer(\n","  (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(\n","    (layers): ModuleList(\n","      (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (multihead_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","        (dropout3): Dropout(p=0.1, inplace=False)\n","        (activation): ReLU()\n","      )\n","    )\n","    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  )\n",")"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["model.model.transformer_module.decoder.decoder_norm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-5jMjPURXiu","executionInfo":{"status":"ok","timestamp":1706700512139,"user_tz":-60,"elapsed":280,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"6cf99c72-5ff1-40c7-a0b4-9a90b76afbfd"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LayerNorm((256,), eps=1e-05, elementwise_affine=True)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["model.model.transformer_module.decoder.layers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqjNO2KTRq9d","executionInfo":{"status":"ok","timestamp":1706700618676,"user_tz":-60,"elapsed":30,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"97ce820e-ebc0-4d75-af07-315d49ac9b35"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ModuleList(\n","  (0-8): 9 x OneFormerTransformerDecoderLayer(\n","    (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(\n","      (multihead_attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (activation): ReLU()\n","    )\n","    (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(\n","      (self_attn): OneFormerAttention(\n","        (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","        (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","        (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","      )\n","      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (activation): ReLU()\n","    )\n","    (ffn): OneFormerTransformerDecoderFFNLayer(\n","      (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","      (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (activation): ReLU()\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["model.model.transformer_module.decoder.query_input_projection"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bx4nCFN-SclP","executionInfo":{"status":"ok","timestamp":1706700711844,"user_tz":-60,"elapsed":227,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"45ce4589-f579-4a65-d955-71060dc14b4c"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["model.model.transformer_module.decoder.class_embed #classification layer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjKgThQqSj3m","executionInfo":{"status":"ok","timestamp":1706700759337,"user_tz":-60,"elapsed":312,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"b5c050a0-6ffd-430c-f1f5-88e15b5bc6ec"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=256, out_features=20, bias=True)"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["model.model.transformer_module.decoder.mask_embed #mask prediction layer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8yQwYlRSkod","executionInfo":{"status":"ok","timestamp":1706700767352,"user_tz":-60,"elapsed":16,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"0d4e3fc1-cb36-4e78-c18e-286feda6fec4"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerMLPPredictionHead(\n","  (layers): Sequential(\n","    (0): PredictionBlock(\n","      (0): Linear(in_features=256, out_features=256, bias=True)\n","      (1): ReLU()\n","    )\n","    (1): PredictionBlock(\n","      (0): Linear(in_features=256, out_features=256, bias=True)\n","      (1): ReLU()\n","    )\n","    (2): PredictionBlock(\n","      (0): Linear(in_features=256, out_features=256, bias=True)\n","      (1): Identity()\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["model.model.task_encoder #MLP that encodes task input to create Q task.. the task input is padded to 77 tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U1LINwUWKTOb","executionInfo":{"status":"ok","timestamp":1706698643829,"user_tz":-60,"elapsed":270,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"9ca6bbf4-8d9f-4800-a9a7-a3ffd171a78d"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTaskModel(\n","  (task_mlp): OneFormerMLPPredictionHead(\n","    (layers): Sequential(\n","      (0): PredictionBlock(\n","        (0): Linear(in_features=77, out_features=256, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): Identity()\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Text Mapper.\n","\n","we iterate over the set of masks to create a list of text (Tlist)\n","with a template “a photo with a {CLS}”, where CLS is the\n","class name for the corresponding binary mask. The number\n","of binary masks per sample varies over the dataset. Therefor\n","we pad Tlist with “a/an {task} photo” entries to obtain\n","a padded list (Tpad) of constant length Ntext, with padded entries representing no-object masks\n","\n","```\n","We tokenize and then encode the input text list (Tpad) using a 6-layer transformer text encoder to obtain a set of Ntext embeddings. We concatenate a set of Nctx learnable embeddings to the encoded representations to obtain the final N text queries (Qtext). The N text queries stand for a text based representation of the objects present in an image.\n","    ```\n","\n","During training, we use two sets of queries in our architecture: text queries (Qtext) and object queries (Q). Qtext is the text-based representation for the segments in the image, while Q is the image-based representation.\n","To obtain Qtext, we first tokenize the text entries Tpad and pass the tokenized representations through a text encoder, which is a 6-layer transformer. The encoded Ntext text embeddings represent the number of binary masks and their corresponding classes in the input image.We further concatenate a set of Nctx learnable text context embeddings (Qctx) to the encoded text embeddings to obtain the final N text queries (Qtext), as shown in Fig. 4. Our motivation behind using Qctx is to learn a unified textual context  for a sample image.\n","\n","To obtain Q, we first initialize the object queries (Q' ) as a N −1 times repetitions of the task-token (Qtask). Then, we update Q' with guidance from the flattened 1/4-scale features inside a 2-layer transformer. The updated Q' from the transformer (rich with image-contextual information) is concatenated with Qtask to obtain a task-conditioned representation of N queries, Q. Unlike the vanilla all-zeros or random initialization, the task-guided initialization of the queries and the concatenation with Qtask is critical for the model to learn multiple segmentation tasks\n","\n","we set N = 250 and Nctx = 16"],"metadata":{"id":"Yc06uHi-2LrS"}},{"cell_type":"code","source":["model.model.text_mapper\n","#49408 is the total vocab size,  dimensionality of the embedding space i.e  token in the vocabulary will be represented as a vector of length 256 in this embedding space\n","#Qtext is created from here, text list is first padded to have list length of 256 to mathc with Q=object queries 256, and then the text is tokenized and, and then encoded using 6 layer transformer to created emebddings of Ntext,, Now a context embeddings Qctx of 16, 256 is added to N text to get Q text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R5aKZgnhU-f4","executionInfo":{"status":"ok","timestamp":1706701359653,"user_tz":-60,"elapsed":13,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"80d20dd8-cfe7-433d-fbdc-28c9da775d1b"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTextMapper(\n","  (text_encoder): OneFormerTextEncoder(\n","    (transformer): OneFormerTextTransformer(\n","      (layers): Sequential(\n","        (0): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    (token_embedding): Embedding(49408, 256)\n","  )\n","  (text_projector): OneFormerMLPPredictionHead(\n","    (layers): Sequential(\n","      (0): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): Identity()\n","      )\n","    )\n","  )\n","  (prompt_ctx): Embedding(16, 256)\n",")"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["model.model.text_mapper.text_encoder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvLdkRcXVGix","executionInfo":{"status":"ok","timestamp":1706701394871,"user_tz":-60,"elapsed":225,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"9c0e2da4-6543-4031-eb18-33cc594963ee"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTextEncoder(\n","  (transformer): OneFormerTextTransformer(\n","    (layers): Sequential(\n","      (0): OneFormerTextTransformerLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): OneFormerTextMLP(\n","          (activation_fn): QuickGELUActivation()\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        )\n","        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): OneFormerTextTransformerLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): OneFormerTextMLP(\n","          (activation_fn): QuickGELUActivation()\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        )\n","        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): OneFormerTextTransformerLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): OneFormerTextMLP(\n","          (activation_fn): QuickGELUActivation()\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        )\n","        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (3): OneFormerTextTransformerLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): OneFormerTextMLP(\n","          (activation_fn): QuickGELUActivation()\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        )\n","        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (4): OneFormerTextTransformerLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): OneFormerTextMLP(\n","          (activation_fn): QuickGELUActivation()\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        )\n","        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (5): OneFormerTextTransformerLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","        )\n","        (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): OneFormerTextMLP(\n","          (activation_fn): QuickGELUActivation()\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","        )\n","        (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (token_embedding): Embedding(49408, 256)\n",")"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["model.model.text_mapper.text_encoder.transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AsDv_PyWcDR","executionInfo":{"status":"ok","timestamp":1706701769167,"user_tz":-60,"elapsed":6,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"fce140f4-1906-447c-ade9-518b0f0d3034"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTextTransformer(\n","  (layers): Sequential(\n","    (0): OneFormerTextTransformerLayer(\n","      (self_attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (mlp): OneFormerTextMLP(\n","        (activation_fn): QuickGELUActivation()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","      )\n","      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (1): OneFormerTextTransformerLayer(\n","      (self_attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (mlp): OneFormerTextMLP(\n","        (activation_fn): QuickGELUActivation()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","      )\n","      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (2): OneFormerTextTransformerLayer(\n","      (self_attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (mlp): OneFormerTextMLP(\n","        (activation_fn): QuickGELUActivation()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","      )\n","      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (3): OneFormerTextTransformerLayer(\n","      (self_attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (mlp): OneFormerTextMLP(\n","        (activation_fn): QuickGELUActivation()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","      )\n","      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (4): OneFormerTextTransformerLayer(\n","      (self_attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (mlp): OneFormerTextMLP(\n","        (activation_fn): QuickGELUActivation()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","      )\n","      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (5): OneFormerTextTransformerLayer(\n","      (self_attn): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (mlp): OneFormerTextMLP(\n","        (activation_fn): QuickGELUActivation()\n","        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","      )\n","      (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["model.model.text_mapper.text_encoder.ln_final"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QtppIoU_Wfof","executionInfo":{"status":"ok","timestamp":1706701784314,"user_tz":-60,"elapsed":263,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"4907e4dc-d105-4a31-ca0c-524c86448e8e"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LayerNorm((256,), eps=1e-05, elementwise_affine=True)"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["model.model.text_mapper.text_encoder.token_embedding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EStmZjjWiuM","executionInfo":{"status":"ok","timestamp":1706701786792,"user_tz":-60,"elapsed":221,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"855f27c3-7d9b-46ff-de6b-4128dcecab6a"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(49408, 256)"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["model.model.text_mapper.text_projector"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwfG1oxPVLwc","executionInfo":{"status":"ok","timestamp":1706701797951,"user_tz":-60,"elapsed":247,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"1dce7f56-4771-472a-94e6-c1436b025f41"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerMLPPredictionHead(\n","  (layers): Sequential(\n","    (0): PredictionBlock(\n","      (0): Linear(in_features=256, out_features=256, bias=True)\n","      (1): ReLU()\n","    )\n","    (1): PredictionBlock(\n","      (0): Linear(in_features=256, out_features=256, bias=True)\n","      (1): Identity()\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["model.matcher"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fRmRCJrDVXJW","executionInfo":{"status":"ok","timestamp":1706701669633,"user_tz":-60,"elapsed":294,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"4255107e-d7fa-4da7-fdaa-3b57fab25eea"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerHungarianMatcher()"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["model.criterion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMPthp1WWNRV","executionInfo":{"status":"ok","timestamp":1706701680288,"user_tz":-60,"elapsed":316,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"eca44f3d-ec37-4063-ee4c-f49a74c5223f"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerLoss(\n","  (matcher): OneFormerHungarianMatcher()\n",")"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tslYtMMrKylz","executionInfo":{"status":"ok","timestamp":1706782977778,"user_tz":-60,"elapsed":7207,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"fe264942-a652-497a-bc9e-0d00f48d0c2a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","source":["import torchinfo\n","torchinfo.summary(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXUnWi0-z6ds","executionInfo":{"status":"ok","timestamp":1706782994931,"user_tz":-60,"elapsed":1666,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d7521b8e-5654-45eb-936c-73826e483828"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["=============================================================================================================================\n","Layer (type:depth-idx)                                                                               Param #\n","=============================================================================================================================\n","OneFormerForUniversalSegmentation                                                                    --\n","├─OneFormerModel: 1-1                                                                                --\n","│    └─OneFormerPixelLevelModule: 2-1                                                                --\n","│    │    └─SwinBackbone: 3-1                                                                        195,201,204\n","│    │    └─OneFormerPixelDecoder: 3-2                                                               5,790,144\n","│    └─OneFormerTransformerModule: 2-2                                                               --\n","│    │    └─OneFormerSinePositionEmbedding: 3-3                                                      --\n","│    │    └─Embedding: 3-4                                                                           64,000\n","│    │    └─OneFormerTransformerDecoder: 3-5                                                         17,635,604\n","│    │    └─Embedding: 3-6                                                                           768\n","│    └─OneFormerTaskModel: 2-3                                                                       --\n","│    │    └─OneFormerMLPPredictionHead: 3-7                                                          85,760\n","│    └─OneFormerTextMapper: 2-4                                                                      --\n","│    │    └─OneFormerTextEncoder: 3-8                                                                17,407,232\n","│    │    └─OneFormerMLPPredictionHead: 3-9                                                          131,584\n","│    │    └─Embedding: 3-10                                                                          4,096\n","├─OneFormerHungarianMatcher: 1-2                                                                     --\n","├─OneFormerLoss: 1-3                                                                                 1\n","│    └─OneFormerHungarianMatcher: 2-5                                                                --\n","=============================================================================================================================\n","Total params: 236,320,393\n","Trainable params: 236,320,393\n","Non-trainable params: 0\n","============================================================================================================================="]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["model.model.task_encoder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAVbHZiWMdUt","executionInfo":{"status":"ok","timestamp":1706783068273,"user_tz":-60,"elapsed":332,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"474f5dd9-d00f-4d96-a6b8-cde4e50f2f2f"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTaskModel(\n","  (task_mlp): OneFormerMLPPredictionHead(\n","    (layers): Sequential(\n","      (0): PredictionBlock(\n","        (0): Linear(in_features=77, out_features=256, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): Identity()\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["model.model.text_mapper"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owiUh4-ZM2hb","executionInfo":{"status":"ok","timestamp":1706783119041,"user_tz":-60,"elapsed":317,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"4506071d-1f99-4ce9-905f-b6f5d0f7562e"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerTextMapper(\n","  (text_encoder): OneFormerTextEncoder(\n","    (transformer): OneFormerTextTransformer(\n","      (layers): Sequential(\n","        (0): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): OneFormerTextTransformerLayer(\n","          (self_attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (mlp): OneFormerTextMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    (token_embedding): Embedding(49408, 256)\n","  )\n","  (text_projector): OneFormerMLPPredictionHead(\n","    (layers): Sequential(\n","      (0): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): PredictionBlock(\n","        (0): Linear(in_features=256, out_features=256, bias=True)\n","        (1): Identity()\n","      )\n","    )\n","  )\n","  (prompt_ctx): Embedding(16, 256)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from transformers import Cli"],"metadata":{"id":"d3L7JzenQcvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoProcessor\n","processor = AutoProcessor.from_pretrained(\n","        \"shi-labs/oneformer_cityscapes_swin_large\",\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9xCibHX_kfO","executionInfo":{"status":"ok","timestamp":1706724263347,"user_tz":-60,"elapsed":826,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"c0497c3d-bd20-4fab-bd14-bdabed77ccbf"},"execution_count":164,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:426: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["processor.tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jMBEgJj2M3YV","executionInfo":{"status":"ok","timestamp":1706724267410,"user_tz":-60,"elapsed":641,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"37e4bf57-a9e9-4935-dfe2-0614438331f7"},"execution_count":165,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CLIPTokenizerFast(name_or_path='shi-labs/oneformer_cityscapes_swin_large', vocab_size=49408, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n","}"]},"metadata":{},"execution_count":165}]},{"cell_type":"code","source":["processor.image_processor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z6dIPgW9AAp7","executionInfo":{"status":"ok","timestamp":1706724277502,"user_tz":-60,"elapsed":279,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"b88851a9-3e87-4317-8505-c0ff352376da"},"execution_count":166,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerImageProcessor {\n","  \"_max_size\": 2048,\n","  \"class_info_file\": \"cityscapes_panoptic.json\",\n","  \"do_normalize\": true,\n","  \"do_reduce_labels\": false,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"ignore_index\": 255,\n","  \"image_mean\": [\n","    0.48500001430511475,\n","    0.4560000002384186,\n","    0.4059999883174896\n","  ],\n","  \"image_processor_type\": \"OneFormerImageProcessor\",\n","  \"image_std\": [\n","    0.2290000021457672,\n","    0.2239999920129776,\n","    0.22499999403953552\n","  ],\n","  \"metadata\": {\n","    \"0\": \"road\",\n","    \"1\": \"building\",\n","    \"10\": \"sky\",\n","    \"11\": \"person\",\n","    \"12\": \"rider\",\n","    \"13\": \"car\",\n","    \"14\": \"truck\",\n","    \"15\": \"bus\",\n","    \"16\": \"train\",\n","    \"17\": \"motorcycle\",\n","    \"18\": \"bicycle\",\n","    \"2\": \"sidewalk\",\n","    \"3\": \"wall\",\n","    \"4\": \"fence\",\n","    \"5\": \"pole\",\n","    \"6\": \"traffic light\",\n","    \"7\": \"traffic sign\",\n","    \"8\": \"vegetation\",\n","    \"9\": \"terrain\",\n","    \"class_names\": [\n","      \"road\",\n","      \"building\",\n","      \"sidewalk\",\n","      \"wall\",\n","      \"fence\",\n","      \"pole\",\n","      \"traffic light\",\n","      \"traffic sign\",\n","      \"vegetation\",\n","      \"terrain\",\n","      \"sky\",\n","      \"person\",\n","      \"rider\",\n","      \"car\",\n","      \"truck\",\n","      \"bus\",\n","      \"train\",\n","      \"motorcycle\",\n","      \"bicycle\"\n","    ],\n","    \"thing_ids\": [\n","      11,\n","      12,\n","      13,\n","      14,\n","      15,\n","      16,\n","      17,\n","      18\n","    ]\n","  },\n","  \"num_labels\": 19,\n","  \"num_text\": null,\n","  \"processor_class\": \"OneFormerProcessor\",\n","  \"repo_path\": \"shi-labs/oneformer_demo\",\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"longest_edge\": 2048,\n","    \"shortest_edge\": 1024\n","  }\n","}"]},"metadata":{},"execution_count":166}]},{"cell_type":"code","source":["processor.image_processor.to_dict().keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9lJ54Q1jAGQS","executionInfo":{"status":"ok","timestamp":1706712744797,"user_tz":-60,"elapsed":9,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"5738a61a-e089-446d-bae3-959882192c55"},"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['_max_size', '_processor_class', 'image_processor_type', 'metadata', 'num_labels', 'do_resize', 'size', 'resample', 'do_rescale', 'rescale_factor', 'do_normalize', 'image_mean', 'image_std', 'ignore_index', 'do_reduce_labels', 'class_info_file', 'repo_path', 'num_text'])"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["processor.image_processor.class_info_file #shi-labs/oneformer_demo/cityscapes_panoptic.json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"t9pAjsAyAPTI","executionInfo":{"status":"ok","timestamp":1706712775449,"user_tz":-60,"elapsed":249,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"181ebe5f-6980-4171-9084-5592f5070a23"},"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cityscapes_panoptic.json'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["from pathlib import Path\n","from huggingface_hub import hf_hub_download\n","import json\n","\n","def get_labels():\n","    data_directory = Path(\".\")\n","    json_file_path = data_directory / \"labels_info.json\"\n","    if json_file_path.is_file():\n","        print(f\"[INFO]: Found {json_file_path}.Skipping Download...\")\n","    else:\n","        print(\"[INFO]: Downloading labels_info.json from hub\")\n","        json_file_path = hf_hub_download(\n","            repo_id=\"BhavanaMalla/railsem19-semantic-expanded\",\n","            filename=\"labels_info.json\",\n","            repo_type=\"dataset\",\n","            local_dir=data_directory\n","        )\n","    with open(json_file_path, \"r\") as f:\n","        labels_info = json.load(f)\n","    return labels_info"],"metadata":{"id":"GXIJ8-JhNC4K","executionInfo":{"status":"ok","timestamp":1706716121955,"user_tz":-60,"elapsed":237,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["from transformers import OneFormerForUniversalSegmentation, AutoProcessor\n","\n","labels_info = get_labels()\n","labels = labels_info['labels']\n","id2label = labels_info[\"id2label\"]\n","\n","ckpt = \"shi-labs/oneformer_cityscapes_swin_large\"\n","model = OneFormerForUniversalSegmentation.from_pretrained(\n","        ckpt,\n","        is_training=True,\n","        id2label=id2label,\n","        ignore_mismatched_sizes=True\n","    )\n","\n","processor = AutoProcessor.from_pretrained(\n","    ckpt,\n","    do_reduce_labels=False,\n","    do_normalize=False,\n","    do_rescale=False,\n","    do_resize=False,\n","\n",")\n","\n","processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx\n","\n","#repo_path: Optional[str] = \"shi-labs/oneformer_demo\",\n","#class_info_file: str = cityscapes_panoptic.json\n","#self.metadata = prepare_metadata(load_metadata(repo_path, class_info_file))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dlzik2wNAMH","executionInfo":{"status":"ok","timestamp":1706722277211,"user_tz":-60,"elapsed":9680,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"30b4459b-a8ba-48a1-a82e-d7feeb385900"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO]: Found labels_info.json.Skipping Download...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized: ['model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_projector.layers.1.0.weight', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:426: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["processor.image_processor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5H0tEXsnNZQ2","executionInfo":{"status":"ok","timestamp":1706716346195,"user_tz":-60,"elapsed":265,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"78d39d58-6be1-49e1-9202-7a889992caf9"},"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerImageProcessor {\n","  \"_max_size\": 2048,\n","  \"class_info_file\": \"cityscapes_panoptic.json\",\n","  \"do_normalize\": false,\n","  \"do_reduce_labels\": false,\n","  \"do_rescale\": false,\n","  \"do_resize\": false,\n","  \"ignore_index\": 255,\n","  \"image_mean\": [\n","    0.48500001430511475,\n","    0.4560000002384186,\n","    0.4059999883174896\n","  ],\n","  \"image_processor_type\": \"OneFormerImageProcessor\",\n","  \"image_std\": [\n","    0.2290000021457672,\n","    0.2239999920129776,\n","    0.22499999403953552\n","  ],\n","  \"metadata\": {\n","    \"0\": \"road\",\n","    \"1\": \"building\",\n","    \"10\": \"sky\",\n","    \"11\": \"person\",\n","    \"12\": \"rider\",\n","    \"13\": \"car\",\n","    \"14\": \"truck\",\n","    \"15\": \"bus\",\n","    \"16\": \"train\",\n","    \"17\": \"motorcycle\",\n","    \"18\": \"bicycle\",\n","    \"2\": \"sidewalk\",\n","    \"3\": \"wall\",\n","    \"4\": \"fence\",\n","    \"5\": \"pole\",\n","    \"6\": \"traffic light\",\n","    \"7\": \"traffic sign\",\n","    \"8\": \"vegetation\",\n","    \"9\": \"terrain\",\n","    \"class_names\": [\n","      \"road\",\n","      \"building\",\n","      \"sidewalk\",\n","      \"wall\",\n","      \"fence\",\n","      \"pole\",\n","      \"traffic light\",\n","      \"traffic sign\",\n","      \"vegetation\",\n","      \"terrain\",\n","      \"sky\",\n","      \"person\",\n","      \"rider\",\n","      \"car\",\n","      \"truck\",\n","      \"bus\",\n","      \"train\",\n","      \"motorcycle\",\n","      \"bicycle\"\n","    ],\n","    \"thing_ids\": [\n","      11,\n","      12,\n","      13,\n","      14,\n","      15,\n","      16,\n","      17,\n","      18\n","    ]\n","  },\n","  \"num_labels\": 19,\n","  \"num_text\": 234,\n","  \"processor_class\": \"OneFormerProcessor\",\n","  \"repo_path\": \"shi-labs/oneformer_demo\",\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"longest_edge\": 2048,\n","    \"shortest_edge\": 1024\n","  }\n","}"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["type(processor.image_processor.metadata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7xOXDvVX0uj","executionInfo":{"status":"ok","timestamp":1706718884313,"user_tz":-60,"elapsed":402,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"f468b7c0-8f77-477d-ae58-a223c3007b12"},"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["#class_info_file - repo_path= ,\n","processor.image_processor.metadata.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QaQCq5uDVRy6","executionInfo":{"status":"ok","timestamp":1706718362615,"user_tz":-60,"elapsed":287,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"c0fb7156-daa0-4676-d4a5-be8eb8f53281"},"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', 'thing_ids', 'class_names'])"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["processor.image_processor.metadata[\"0\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"mwKe_74uYYPY","executionInfo":{"status":"ok","timestamp":1706719031859,"user_tz":-60,"elapsed":298,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"91ccadcd-59b2-4a2c-be5e-7f994486b639"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'road'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["id2label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t5GelcHdNyeR","executionInfo":{"status":"ok","timestamp":1706716250045,"user_tz":-60,"elapsed":338,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"998a53cc-f8e2-40ea-977a-39df9253e1fa"},"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'0': 'road',\n"," '1': 'sidewalk',\n"," '2': 'construction',\n"," '3': 'tram-track',\n"," '4': 'fence',\n"," '5': 'pole',\n"," '6': 'traffic-light',\n"," '7': 'traffic-sign',\n"," '8': 'vegetation',\n"," '9': 'terrain',\n"," '10': 'sky',\n"," '11': 'human',\n"," '12': 'rail-track',\n"," '13': 'car',\n"," '14': 'truck',\n"," '15': 'trackbed',\n"," '16': 'on-rails',\n"," '17': 'rail-raised',\n"," '18': 'rail-embedded'}"]},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ppHHTAHxk0ns","executionInfo":{"status":"ok","timestamp":1706722284930,"user_tz":-60,"elapsed":9,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"29a53c3b-6997-4142-9424-5c5b20323c94"},"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['road',\n"," 'sidewalk',\n"," 'construction',\n"," 'tram-track',\n"," 'fence',\n"," 'pole',\n"," 'traffic-light',\n"," 'traffic-sign',\n"," 'vegetation',\n"," 'terrain',\n"," 'sky',\n"," 'human',\n"," 'rail-track',\n"," 'car',\n"," 'truck',\n"," 'trackbed',\n"," 'on-rails',\n"," 'rail-raised',\n"," 'rail-embedded']"]},"metadata":{},"execution_count":127}]},{"cell_type":"code","source":["id2label[\"255\"] = \"background\"\n","labels.append(\"background\")"],"metadata":{"id":"lKkYUkgCk60T","executionInfo":{"status":"ok","timestamp":1706722311098,"user_tz":-60,"elapsed":256,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":128,"outputs":[]},{"cell_type":"code","source":["metadata = id2label\n","metadata[\"thing_ids\"] = [11, 12, 13, 14]\n","metadata[\"class_names\"] = labels\n","processor.image_processor.metadata = metadata\n","processor.image_processor.num_labels = 20"],"metadata":{"id":"vDE6bb9BXUn_","executionInfo":{"status":"ok","timestamp":1706722367801,"user_tz":-60,"elapsed":247,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":131,"outputs":[]},{"cell_type":"code","source":["processor.image_processor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4UtvbfddN42n","executionInfo":{"status":"ok","timestamp":1706722369169,"user_tz":-60,"elapsed":231,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"e3f69b88-cca7-4f15-ee80-e4fd720c1248"},"execution_count":132,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerImageProcessor {\n","  \"_max_size\": 2048,\n","  \"class_info_file\": \"cityscapes_panoptic.json\",\n","  \"do_normalize\": false,\n","  \"do_reduce_labels\": false,\n","  \"do_rescale\": false,\n","  \"do_resize\": false,\n","  \"ignore_index\": 255,\n","  \"image_mean\": [\n","    0.48500001430511475,\n","    0.4560000002384186,\n","    0.4059999883174896\n","  ],\n","  \"image_processor_type\": \"OneFormerImageProcessor\",\n","  \"image_std\": [\n","    0.2290000021457672,\n","    0.2239999920129776,\n","    0.22499999403953552\n","  ],\n","  \"metadata\": {\n","    \"0\": \"road\",\n","    \"1\": \"sidewalk\",\n","    \"10\": \"sky\",\n","    \"11\": \"human\",\n","    \"12\": \"rail-track\",\n","    \"13\": \"car\",\n","    \"14\": \"truck\",\n","    \"15\": \"trackbed\",\n","    \"16\": \"on-rails\",\n","    \"17\": \"rail-raised\",\n","    \"18\": \"rail-embedded\",\n","    \"2\": \"construction\",\n","    \"255\": \"background\",\n","    \"3\": \"tram-track\",\n","    \"4\": \"fence\",\n","    \"5\": \"pole\",\n","    \"6\": \"traffic-light\",\n","    \"7\": \"traffic-sign\",\n","    \"8\": \"vegetation\",\n","    \"9\": \"terrain\",\n","    \"class_names\": [\n","      \"road\",\n","      \"sidewalk\",\n","      \"construction\",\n","      \"tram-track\",\n","      \"fence\",\n","      \"pole\",\n","      \"traffic-light\",\n","      \"traffic-sign\",\n","      \"vegetation\",\n","      \"terrain\",\n","      \"sky\",\n","      \"human\",\n","      \"rail-track\",\n","      \"car\",\n","      \"truck\",\n","      \"trackbed\",\n","      \"on-rails\",\n","      \"rail-raised\",\n","      \"rail-embedded\",\n","      \"background\"\n","    ],\n","    \"thing_ids\": [\n","      11,\n","      12,\n","      13,\n","      14\n","    ]\n","  },\n","  \"num_labels\": 20,\n","  \"num_text\": 234,\n","  \"processor_class\": \"OneFormerProcessor\",\n","  \"repo_path\": \"shi-labs/oneformer_demo\",\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"longest_edge\": 2048,\n","    \"shortest_edge\": 1024\n","  }\n","}"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","source":["model.config.to_dict().keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDx4xYZLiuf0","executionInfo":{"status":"ok","timestamp":1706722395085,"user_tz":-60,"elapsed":363,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"7f23d376-5b56-4f7b-c780-99c46504f748"},"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['backbone_config', 'ignore_value', 'num_queries', 'no_object_weight', 'class_weight', 'mask_weight', 'dice_weight', 'contrastive_weight', 'contrastive_temperature', 'train_num_points', 'oversample_ratio', 'importance_sample_ratio', 'init_std', 'init_xavier_std', 'layer_norm_eps', 'is_training', 'use_auxiliary_loss', 'output_auxiliary_logits', 'strides', 'task_seq_len', 'text_encoder_width', 'text_encoder_context_length', 'text_encoder_num_layers', 'text_encoder_vocab_size', 'text_encoder_proj_layers', 'text_encoder_n_ctx', 'conv_dim', 'mask_dim', 'hidden_dim', 'encoder_feedforward_dim', 'norm', 'encoder_layers', 'decoder_layers', 'use_task_norm', 'num_attention_heads', 'dropout', 'dim_feedforward', 'pre_norm', 'enforce_input_proj', 'query_dec_layers', 'common_stride', 'num_hidden_layers', 'return_dict', 'output_hidden_states', 'output_attentions', 'torchscript', 'torch_dtype', 'use_bfloat16', 'tf_legacy_loss', 'pruned_heads', 'tie_word_embeddings', 'is_encoder_decoder', 'is_decoder', 'cross_attention_hidden_size', 'add_cross_attention', 'tie_encoder_decoder', 'max_length', 'min_length', 'do_sample', 'early_stopping', 'num_beams', 'num_beam_groups', 'diversity_penalty', 'temperature', 'top_k', 'top_p', 'typical_p', 'repetition_penalty', 'length_penalty', 'no_repeat_ngram_size', 'encoder_no_repeat_ngram_size', 'bad_words_ids', 'num_return_sequences', 'chunk_size_feed_forward', 'output_scores', 'return_dict_in_generate', 'forced_bos_token_id', 'forced_eos_token_id', 'remove_invalid_values', 'exponential_decay_length_penalty', 'suppress_tokens', 'begin_suppress_tokens', 'architectures', 'finetuning_task', 'id2label', 'label2id', 'tokenizer_class', 'prefix', 'bos_token_id', 'pad_token_id', 'eos_token_id', 'sep_token_id', 'decoder_start_token_id', 'task_specific_params', 'problem_type', '_name_or_path', 'transformers_version', 'max_seq_len', 'model_type', 'num_classes'])"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":["model.config.backbone_config.to_dict().keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpwhAEpfnjbF","executionInfo":{"status":"ok","timestamp":1706892021218,"user_tz":-60,"elapsed":295,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"51aa296e-f3f5-447e-8465-b768b32ec350"},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['return_dict', 'output_hidden_states', 'output_attentions', 'torchscript', 'torch_dtype', 'use_bfloat16', 'tf_legacy_loss', 'pruned_heads', 'tie_word_embeddings', 'is_encoder_decoder', 'is_decoder', 'cross_attention_hidden_size', 'add_cross_attention', 'tie_encoder_decoder', 'max_length', 'min_length', 'do_sample', 'early_stopping', 'num_beams', 'num_beam_groups', 'diversity_penalty', 'temperature', 'top_k', 'top_p', 'typical_p', 'repetition_penalty', 'length_penalty', 'no_repeat_ngram_size', 'encoder_no_repeat_ngram_size', 'bad_words_ids', 'num_return_sequences', 'chunk_size_feed_forward', 'output_scores', 'return_dict_in_generate', 'forced_bos_token_id', 'forced_eos_token_id', 'remove_invalid_values', 'exponential_decay_length_penalty', 'suppress_tokens', 'begin_suppress_tokens', 'architectures', 'finetuning_task', 'id2label', 'label2id', 'tokenizer_class', 'prefix', 'bos_token_id', 'pad_token_id', 'eos_token_id', 'sep_token_id', 'decoder_start_token_id', 'task_specific_params', 'problem_type', '_name_or_path', 'transformers_version', 'hidden_size', 'model_type', 'num_layers', 'path_norm', 'stage_names', 'image_size', 'patch_size', 'num_channels', 'embed_dim', 'depths', 'num_heads', 'window_size', 'mlp_ratio', 'qkv_bias', 'hidden_dropout_prob', 'attention_probs_dropout_prob', 'drop_path_rate', 'hidden_act', 'use_absolute_embeddings', 'layer_norm_eps', 'initializer_range', 'encoder_stride', 'out_features', 'out_indices'])"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["model.config.backbone_config.patch_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gCKooTMLsW2W","executionInfo":{"status":"ok","timestamp":1706892035565,"user_tz":-60,"elapsed":346,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"472caf86-1f72-4af1-e188-a1ae39367f83"},"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["new_dict = {key: value for key, value in model.config.to_dict().items() if key != \"backbone_config\"}\n","new_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQxqaqBomoJa","executionInfo":{"status":"ok","timestamp":1706723163313,"user_tz":-60,"elapsed":344,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"f3c4b380-7d4a-4a60-9fdc-7402b0857683"},"execution_count":148,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ignore_value': 255,\n"," 'num_queries': 250,\n"," 'no_object_weight': 0.1,\n"," 'class_weight': 2.0,\n"," 'mask_weight': 5.0,\n"," 'dice_weight': 5.0,\n"," 'contrastive_weight': 0.5,\n"," 'contrastive_temperature': 0.07,\n"," 'train_num_points': 12544,\n"," 'oversample_ratio': 3.0,\n"," 'importance_sample_ratio': 0.75,\n"," 'init_std': 0.02,\n"," 'init_xavier_std': 1.0,\n"," 'layer_norm_eps': 1e-05,\n"," 'is_training': True,\n"," 'use_auxiliary_loss': True,\n"," 'output_auxiliary_logits': True,\n"," 'strides': [4, 8, 16, 32],\n"," 'task_seq_len': 77,\n"," 'text_encoder_width': 256,\n"," 'text_encoder_context_length': 77,\n"," 'text_encoder_num_layers': 6,\n"," 'text_encoder_vocab_size': 49408,\n"," 'text_encoder_proj_layers': 2,\n"," 'text_encoder_n_ctx': 16,\n"," 'conv_dim': 256,\n"," 'mask_dim': 256,\n"," 'hidden_dim': 256,\n"," 'encoder_feedforward_dim': 1024,\n"," 'norm': 'GN',\n"," 'encoder_layers': 6,\n"," 'decoder_layers': 10,\n"," 'use_task_norm': True,\n"," 'num_attention_heads': 8,\n"," 'dropout': 0.1,\n"," 'dim_feedforward': 2048,\n"," 'pre_norm': False,\n"," 'enforce_input_proj': False,\n"," 'query_dec_layers': 2,\n"," 'common_stride': 4,\n"," 'num_hidden_layers': 10,\n"," 'return_dict': True,\n"," 'output_hidden_states': True,\n"," 'output_attentions': True,\n"," 'torchscript': False,\n"," 'torch_dtype': 'float32',\n"," 'use_bfloat16': False,\n"," 'tf_legacy_loss': False,\n"," 'pruned_heads': {},\n"," 'tie_word_embeddings': True,\n"," 'is_encoder_decoder': False,\n"," 'is_decoder': False,\n"," 'cross_attention_hidden_size': None,\n"," 'add_cross_attention': False,\n"," 'tie_encoder_decoder': False,\n"," 'max_length': 20,\n"," 'min_length': 0,\n"," 'do_sample': False,\n"," 'early_stopping': False,\n"," 'num_beams': 1,\n"," 'num_beam_groups': 1,\n"," 'diversity_penalty': 0.0,\n"," 'temperature': 1.0,\n"," 'top_k': 50,\n"," 'top_p': 1.0,\n"," 'typical_p': 1.0,\n"," 'repetition_penalty': 1.0,\n"," 'length_penalty': 1.0,\n"," 'no_repeat_ngram_size': 0,\n"," 'encoder_no_repeat_ngram_size': 0,\n"," 'bad_words_ids': None,\n"," 'num_return_sequences': 1,\n"," 'chunk_size_feed_forward': 0,\n"," 'output_scores': False,\n"," 'return_dict_in_generate': False,\n"," 'forced_bos_token_id': None,\n"," 'forced_eos_token_id': None,\n"," 'remove_invalid_values': False,\n"," 'exponential_decay_length_penalty': None,\n"," 'suppress_tokens': None,\n"," 'begin_suppress_tokens': None,\n"," 'architectures': ['OneFormerForUniversalSegmentation'],\n"," 'finetuning_task': None,\n"," 'id2label': {'0': 'road',\n","  '1': 'sidewalk',\n","  '2': 'construction',\n","  '3': 'tram-track',\n","  '4': 'fence',\n","  '5': 'pole',\n","  '6': 'traffic-light',\n","  '7': 'traffic-sign',\n","  '8': 'vegetation',\n","  '9': 'terrain',\n","  '10': 'sky',\n","  '11': 'human',\n","  '12': 'rail-track',\n","  '13': 'car',\n","  '14': 'truck',\n","  '15': 'trackbed',\n","  '16': 'on-rails',\n","  '17': 'rail-raised',\n","  '18': 'rail-embedded',\n","  '255': 'background'},\n"," 'label2id': {'bicycle': 18,\n","  'building': 2,\n","  'bus': 15,\n","  'car': 13,\n","  'fence': 4,\n","  'motorcycle': 17,\n","  'person': 11,\n","  'pole': 5,\n","  'rider': 12,\n","  'road': 0,\n","  'sidewalk': 1,\n","  'sky': 10,\n","  'terrain': 9,\n","  'traffic light': 6,\n","  'traffic sign': 7,\n","  'train': 16,\n","  'truck': 14,\n","  'vegetation': 8,\n","  'wall': 3},\n"," 'tokenizer_class': None,\n"," 'prefix': None,\n"," 'bos_token_id': None,\n"," 'pad_token_id': None,\n"," 'eos_token_id': None,\n"," 'sep_token_id': None,\n"," 'decoder_start_token_id': None,\n"," 'task_specific_params': None,\n"," 'problem_type': None,\n"," '_name_or_path': 'shi-labs/oneformer_cityscapes_swin_large',\n"," 'transformers_version': '4.35.2',\n"," 'max_seq_len': 77,\n"," 'model_type': 'oneformer',\n"," 'num_classes': 19}"]},"metadata":{},"execution_count":148}]},{"cell_type":"code","source":["model.config.num_classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R9EZjWtMKVSF","executionInfo":{"status":"ok","timestamp":1706722397348,"user_tz":-60,"elapsed":259,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"9033d3be-10d7-4123-c0a7-27f250820adc"},"execution_count":134,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{},"execution_count":134}]},{"cell_type":"code","source":["model.config.num_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQXYrZKHokqz","executionInfo":{"status":"ok","timestamp":1706723273546,"user_tz":-60,"elapsed":6,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"6370af0c-d88b-4b3f-8f0c-c87b3bd7a291"},"execution_count":149,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{},"execution_count":149}]},{"cell_type":"code","source":["label2id = labels_info['label2id']\n","label2id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHLGL6NkpYU7","executionInfo":{"status":"ok","timestamp":1706723492291,"user_tz":-60,"elapsed":273,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"3ec8ae36-fbd2-4e72-da3f-8890e028d355"},"execution_count":151,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'road': 0,\n"," 'sidewalk': 1,\n"," 'construction': 2,\n"," 'tram-track': 3,\n"," 'fence': 4,\n"," 'pole': 5,\n"," 'traffic-light': 6,\n"," 'traffic-sign': 7,\n"," 'vegetation': 8,\n"," 'terrain': 9,\n"," 'sky': 10,\n"," 'human': 11,\n"," 'rail-track': 12,\n"," 'car': 13,\n"," 'truck': 14,\n"," 'trackbed': 15,\n"," 'on-rails': 16,\n"," 'rail-raised': 17,\n"," 'rail-embedded': 18}"]},"metadata":{},"execution_count":151}]},{"cell_type":"code","source":["from transformers import OneFormerForUniversalSegmentation, AutoProcessor\n","from copy import deepcopy\n","\n","labels_info = get_labels()\n","labels = labels_info['labels']\n","id2label = labels_info[\"id2label\"]\n","label2id = labels_info['label2id']\n","#no need to include bg in the num of classes\n","#id2label[\"255\"] = \"background\"\n","#label2id[\"background\"] = 19\n","# if \"background\" not in labels:\n","#     labels.append(\"background\")\n","\n","ckpt = \"shi-labs/oneformer_cityscapes_swin_large\"\n","model = OneFormerForUniversalSegmentation.from_pretrained(\n","        ckpt,\n","        is_training=True,\n","        id2label=id2label,\n","        label2id=label2id,\n","        ignore_mismatched_sizes=True\n","    )\n","\n","processor = AutoProcessor.from_pretrained(\n","    ckpt,\n","    ignore_index=255,\n","    do_reduce_labels=False,\n","    do_normalize=False,\n","    do_rescale=False,\n","    do_resize=False,\n","\n",")\n","\n","metadata = deepcopy(id2label)\n","metadata[\"thing_ids\"] = [11, 12, 13, 14]\n","metadata[\"class_names\"] = labels\n","processor.image_processor.metadata = metadata\n","processor.image_processor.num_labels = len(id2label)\n","processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4LdoAbPsjfq7","executionInfo":{"status":"ok","timestamp":1706724334378,"user_tz":-60,"elapsed":9691,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"9d14a2d1-a848-4d38-9706-7e8f856aacbd"},"execution_count":167,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO]: Found labels_info.json.Skipping Download...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized: ['model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_projector.layers.1.0.weight', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:426: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["len(id2label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbCOVPxvqCJX","executionInfo":{"status":"ok","timestamp":1706724338186,"user_tz":-60,"elapsed":267,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"f74004ca-2148-4109-bbef-e65751865df4"},"execution_count":168,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{},"execution_count":168}]},{"cell_type":"code","source":["model.config.num_classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHIRnP7pl_ah","executionInfo":{"status":"ok","timestamp":1706724342423,"user_tz":-60,"elapsed":232,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"eba99007-9648-41a0-f3a0-be83897bf12e"},"execution_count":169,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{},"execution_count":169}]},{"cell_type":"code","source":["model.config.num_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MDPkpbAmHe_","executionInfo":{"status":"ok","timestamp":1706724368735,"user_tz":-60,"elapsed":324,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"6d5508bc-113d-4ab0-c051-b5d269e1fa6b"},"execution_count":170,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{},"execution_count":170}]},{"cell_type":"code","source":["!pip install transformers datasets evaluate wandb torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDPC5ittu2Lf","executionInfo":{"status":"ok","timestamp":1707040077421,"user_tz":-60,"elapsed":25119,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"faaaea68-1769-49a8-9d65-adce9c551a13"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting datasets\n","  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchmetrics\n","  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.40.0-py2.py3-none-any.whl (257 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, lightning-utilities, docker-pycreds, dill, responses, multiprocess, gitdb, torchmetrics, GitPython, wandb, datasets, evaluate\n","Successfully installed GitPython-3.1.41 datasets-2.16.1 dill-0.3.7 docker-pycreds-0.4.0 evaluate-0.4.1 gitdb-4.0.11 lightning-utilities-0.10.1 multiprocess-0.70.15 responses-0.18.0 sentry-sdk-1.40.0 setproctitle-1.3.3 smmap-5.0.1 torchmetrics-1.3.0.post0 wandb-0.16.2\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.cuda.amp import GradScaler, autocast\n","from transformers import AutoModelForUniversalSegmentation, AutoProcessor\n","from torchmetrics.classification import JaccardIndex\n","import albumentations as A\n","from tqdm.auto import tqdm\n","from huggingface_hub import notebook_login\n","from datasets import load_dataset\n","import wandb\n","import evaluate\n","import statistics\n","from copy import deepcopy\n","\n","import numpy as np\n","import pandas as pd\n","import random\n","import requests\n","from pathlib import Path\n","import os\n","from typing import List, Dict, Tuple\n","import warnings\n","from PIL import Image as PILImage\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","#warnings.filterwarnings(\"ignore\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.empty_cache()\n","    torch.backends.cudnn.benchmark = True\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.enabled = True\n","\n","from pathlib import Path\n","from huggingface_hub import hf_hub_download\n","import json\n","\n","def get_labels():\n","    data_directory = Path(\".\")\n","    json_file_path = data_directory / \"labels_info.json\"\n","    if json_file_path.is_file():\n","        print(f\"[INFO]: Found {json_file_path}.Skipping Download...\")\n","    else:\n","        print(\"[INFO]: Downloading labels_info.json from hub\")\n","        json_file_path = hf_hub_download(\n","            repo_id=\"BhavanaMalla/railsem19-semantic-expanded\",\n","            filename=\"labels_info.json\",\n","            repo_type=\"dataset\",\n","            local_dir=data_directory\n","        )\n","    with open(json_file_path, \"r\") as f:\n","        labels_info = json.load(f)\n","    return labels_info\n","\n","def get_model_and_processor(ckpt_dataset_name, ckpt_name,\n","                            id2label=None, label2id=None, labels=None,\n","                            is_train=False):\n","    print(\"[INFO]: Prepare the Model and the Processor...\")\n","    model_ckpt = f\"shi-labs/oneformer_{ckpt_dataset_name}_{ckpt_name}_large\"\n","    if is_train:\n","        print(\"[INFO]: Model is in training mode...\")\n","\n","        model = AutoModelForUniversalSegmentation.from_pretrained(\n","            model_ckpt, is_training=is_train,\n","            id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n","        )\n","    # Processor\n","    processor = AutoProcessor.from_pretrained(\n","        model_ckpt,\n","        ignore_index=255,\n","        do_reduce_labels=False,\n","        do_normalize=False,\n","        do_rescale=False,\n","        do_resize=False\n","    )\n","    metadata = deepcopy(id2label)\n","    metadata[\"thing_ids\"] = [11, 12, 13, 14]\n","    metadata[\"class_names\"] = labels\n","    processor.image_processor.metadata = metadata\n","    processor.image_processor.num_labels = len(id2label)\n","    processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx\n","\n","    # Parameter Freezing\n","    if is_train:\n","        # Freeze the model backbone params\n","        for name, params in model.named_parameters():\n","            if name.startswith(\"model.pixel_level_module\") or\\\n","                    name.startswith(\"model.task_encoder\"):\n","                params.requires_grad = False\n","            if name.startswith(\"model.transformer_module\"):\n","                mdl_keywords = [\"query_input_projection\", \"class_embed\",\n","                                \"mask_embed\", \"level_embed\"]\n","                if not any(keyword in name for keyword in mdl_keywords):\n","                    params.requires_grad = False\n","    return model, processor\n","\n","def get_transforms():\n","    # Transforms\n","    #include remap\n","    imagenet_mean = np.array([0.485, 0.456, 0.406])\n","    imagenet_std = np.array([0.229, 0.224, 0.225])\n","    train_transforms = A.Compose([\n","        A.LongestMaxSize(max_size=1333),\n","        A.RandomCrop(width=512, height=512),\n","        A.HorizontalFlip(p=0.5),\n","        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n","    ])\n","    val_transforms = A.Compose([\n","        A.Resize(width=512, height=512),\n","        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n","    ])\n","    return train_transforms, val_transforms\n","\n","\n","# util function for generating interactive image mask from components\n","def wandb_mask_log(images, true_seg_masks, pred_seg_masks, id2label, run):\n","    new_id2label = {int(key): value for key, value in id2label.items()}\n","    table_data = []\n","    for img, gt_mask, pred_mask in zip(images, true_seg_masks, pred_seg_masks):\n","        # Unnormalizing the images\n","        img = (img).astype(np.uint8)\n","        gt_mask = (gt_mask).astype(np.uint8)\n","        pred_mask = (pred_mask).astype(np.uint8)\n","        pil_image = PILImage.fromarray(img, mode=\"RGB\")\n","        table_data.append(wandb.Image(\n","            pil_image, masks={\n","                \"ground truth\": {\n","                    \"mask_data\": gt_mask,\n","                    \"class_labels\": new_id2label\n","                },\n","                \"prediction\": {\n","                    \"mask_data\": pred_mask,\n","                    \"class_labels\": new_id2label\n","                }\n","            }\n","        ))\n","        return table_data\n","\n","def load_railsem_dataset(device):\n","    print(f\"[INFO]: Extracting Railsem19 dataset from hub on device {device}\")\n","    railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-expanded\")\n","    # BhavanaMalla/railsem19-semantic-split355-expanded\n","    return railsem_ds\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image = self.dataset[idx][\"image\"]\n","        semantic_mask = self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image,\n","                                          mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask"],"metadata":{"id":"wHfkNbyHrqdP","executionInfo":{"status":"ok","timestamp":1707040098514,"user_tz":-60,"elapsed":21113,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rozMzZGGOax5","executionInfo":{"status":"ok","timestamp":1707040117891,"user_tz":-60,"elapsed":19401,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"0516635e-4c31-4642-d37f-ad28df35191f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["labelsinfo = get_labels()\n","id2label = labelsinfo[\"id2label\"]\n","label2id = labelsinfo[\"label2id\"]\n","labels = labelsinfo[\"labels\"]\n","\n","# Model\n","model_ckpt = f\"shi-labs/oneformer_cityscapes_swin_large\"\n","model = AutoModelForUniversalSegmentation.from_pretrained(model_ckpt, is_training=True,\n","                                                          id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n","# Processor\n","processor = AutoProcessor.from_pretrained(model_ckpt, ignore_index=255, do_reduce_labels=False, do_normalize=False, do_rescale=False, do_resize=False)\n","\n","metadata = deepcopy(id2label)\n","metadata[\"thing_ids\"] = [11, 12, 13, 14]\n","metadata[\"class_names\"] = labels\n","processor.image_processor.metadata = metadata\n","processor.image_processor.num_labels = len(id2label)\n","processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx\n","\n","def _collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","    # # Find unique values in each list using NumPy\n","    # unique_values_in_each_list = [np.unique(lst) for lst in segmentation_maps]\n","\n","    # # Print the result\n","    # for i, unique_values in enumerate(unique_values_in_each_list):\n","    #     print(f\"List {i + 1}: {unique_values}\")\n","    batch = processor(images, segmentation_maps=segmentation_maps, task_inputs=[\"semantic\"] * len(images), return_tensors=\"pt\")\n","\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","def _prepare_dataloader(dataset: torch.utils.data.Dataset, batch_size: int, is_train):\n","    sampler = None\n","    dataloader = DataLoader(dataset, batch_size=batch_size,\n","                            shuffle=(is_train and sampler is None),\n","                            sampler=sampler, collate_fn=_collate_fn)\n","    return dataloader\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481,"referenced_widgets":["6fd53094783d4cb4a2b289ec550a1544","57ac83994cd44ea9a0f94ec69e004cd1","f2d6d876afc74dbcacc5e167fc1ef945","1ea5e1047ee141149fefd94d9e36d160","2218a868c77f4404b6517bdf93f2c391","a83bf36cf98a44dfaa4d63e42a0d30e1","9c3e521994f64549898930a098f465ff","2e09ebcc9b394f8888c010a481102729","310fa138b6f249d7a68dcd5641b1f658","c9b116a1cca54c84914d912b1a580a98","60c6c9bf2e514843b8f09d86364d524e","a54fc691e85248559ada5b03bb8c0ffc","b8441000ece04219b7596c23817d535e","fc3638d3bfec49d2bec3e49e586920f8","12b8e83cd31a4f4a8ff12395fc5d9bed","51d1008d841c49778f2c1a57e9266476","11dcd814aa194df688cfd31b20e1807c","227988af636546d6b62147effee2a448","827e9ca396a74fdba8eed8898d869e3e","7e4094894fd74027b667b2a789ed6b7b","ade8ced35f434ac7a725813642400d6e","cea4866c9ce84b42b75604e63df11f38","76e11b9f773347b2a26ae92a8ea6c28f","adad701229ae48bc82b3f729c9eeddef","effb6fe1bc03457cbfeef92b292b8653","754bf6f6698b4b61a4d03e367edf9e36","95654d5d5f0a424bac86d2b757601dca","3436d33164a644cababbcc2fb6c1bec2","d2d9d5e515e241b9959e60b5ce29a795","633dcbd47c544221a0b656486b13c1f2","ac76707ce1364864873b08e9492c7de6","6a1b951b02304fd689e7f440dbaa3ce1","b93aa7592fab413ea9dd8d491b347475","fa3d0422209341dbb1c2cb3df7c9f337","77310f43383d4e37ac804e262c46915e","f9aacf42ee384d23a3758db479e62b58","4925bdece9b44221a0f523253d9b8194","2f54b05f221743a087c7e9f5cf7806ee","5e569a348e6e4b2e8bc929887c274e2a","e0eee41cd5f84ffabe0cb17fe4feb4cd","05de8fbc14ea4aeb80091c774187abbb","ef5523cff1fe4b5dbb992871a193a7fd","bd4167cfbf32433c91e9ae0932750ee1","b559bbb878c84ed2992c5da3d98d8f2a","b11d61fb7c614a43b8613aeceef34dad","e60d0bcbda144ec8ad81089d90b3635a","751b65f6244f4c1180f9d9d0a491a9ad","2e38560ff08945cd9820e3781ad89690","8d38e58461f746a6a4963b5dd9802255","123f81146d534da9bf049d1a25be496b","55d71579522844febaeedd38b2a20613","95bf480fbe6e4ea7973e12c9e1bb92cf","7f0b15ed6d9c4bd4b09ecb396093212b","35ba3fa3c6d44e249ebfb2ed28c91aa5","afb5e14bb65945dab4e9566961a1cd80","db93a9908f19458fb1ab24e487da9dc6","1ccb229087154c5f982ed9a457600b2f","33516225315d45ea8089623599fa6377","6e6acdfae88046bba786f9b2396bbee1","33c30445c8fd40e19277885e6f579318","1682d7828b19482d90f30407287d0e05","4167194f0227405b86dd262983387c25","56121314be03475bbe1cb330cc2e4864","b801d0aec0ee47828c248dd17f5c7bfd","98bd9af33c274c6ebc209875044d8fda","7a34b79c32544236b138692e249f726c","734784dafb7d476a83e0ed2555400e3f","8774a89dc65e41e9b7bc6ff3bf9a12d6","718830fe1be742e99935df459e2d40d8","39afb548c9874c16b59c1b1679f14cd4","2f25724062e04ca3b7e753c28c8060aa","a060f86e48d446a0a8bc377bf6034346","345649f9350f408aab65984e6f39d520","dcb423bc1fd04ead9268adf842a0d0d9","f9c7b911084f4acf8766e94cffd2eb50","9f90d35918cd4d14a183ecf9cec1374b","ef3edba10d184d3c908dfaca0604f1cb","ae2c33f69d4a464fa8f5db68a6a124ad","d344e41d0d5b465c8d0b6e86c16df806","afdcf27a088b4468a21192fd34c26c6a","449228a23b144f02b4904bde190c7785","cb4f99383b9f474da6b5b8943f67c624","5081d8c8b3ce4b97895ce2d8ecaa54ba","37f9517b6fc647ebaec61801e6dd5f65","627a4e3d16764366bcfd7887be568299","f089156c09e84318a33bac72ab26d48d","0318611f960a40229f34e2c5cd4160cb","f8e8bc20f7d64b3e916a5ff613c41aa2","86d549447568497d8cc89c752eccda36","ef85e993d2ee4272af0d8d8b461e41d0","60cc796801424aeda0850346e39b13e7","eee5df4b34b64d629359c49bd9bb67cc","0b5aae12e3854022a0a8169acc79300e","0aaed17b974a422c8794e237612abc92","2e072fd4d2ad4f1aab1d57a5c572f334","d53125acfba04541aa8d051c7a2bb364","18d4d427fac649a3916e15464669b4f5","feeeba152e364f9bbc46c325291735d0","67fc24cc5d26498cb7a01846bbdc907e"]},"id":"Zop9K_NMdXqw","executionInfo":{"status":"ok","timestamp":1707040141535,"user_tz":-60,"elapsed":23674,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"9b5d21e4-b2cc-4a18-a509-31871e3b4fd0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO]: Downloading labels_info.json from hub\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["labels_info.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd53094783d4cb4a2b289ec550a1544"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/77.7k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a54fc691e85248559ada5b03bb8c0ffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/879M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e11b9f773347b2a26ae92a8ea6c28f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized: ['model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_projector.layers.1.0.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa3d0422209341dbb1c2cb3df7c9f337"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["cityscapes_panoptic.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b11d61fb7c614a43b8613aeceef34dad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/812 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db93a9908f19458fb1ab24e487da9dc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734784dafb7d476a83e0ed2555400e3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae2c33f69d4a464fa8f5db68a6a124ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d549447568497d8cc89c752eccda36"}},"metadata":{}}]},{"cell_type":"code","source":["railsem_ds = railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-expanded\")\n","splits = railsem_ds[\"data\"].train_test_split(test_size=0.05, shuffle=True)\n","\n","# train_split = splits[\"train\"]\n","# val_split = splits[\"test\"]\n","train_split = splits[\"train\"].select(indices=(range(8)))\n","val_split = splits[\"test\"].select(indices=(range(4)))\n","print(f\"[INFO]: Total Training images: {len(train_split)}\")\n","print(f\"[INFO]: Total Validation images: {len(val_split)}\")\n","\n","train_transforms, val_transforms = get_transforms()\n","# Custom Dataset\n","train_dataset = CustomDataset(train_split, train_transforms)\n","val_dataset = CustomDataset(val_split, val_transforms)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500,"referenced_widgets":["acb66233eb614a27b9fa4addbe29f8e7","16bdae5d3fe74799a23b37806458e7e8","4d07a24fc36b4699b85a041044d27844","c7ce2c12224c4bceaf48fda55b78cb6e","2a9c0aa1cc2b4889bbd2d802e98cc4d8","0bc543c48d5348fd9ae5d2ae6c623a5c","481df29be11e43a89712cd1ec50581e8","afaef9b1c8fa4a2e89429dd167309b31","5d5d0a0a792c49feb2ac3e446ccf278f","cd74cda8113643a2b3f19e6a815f2dcd","7a6c46452cf648658f00ec383bd0deb1","a384f37852be4840a8290df6465969c0","7e61d3bfca35489c8ec3081277da78fe","8f3bec7ac075422f998cdbaa1acf9a92","403f209f59b44f77abf27b10542c2e94","51a8b19999b74a6384b56d7b211e144a","b771d0d4c9014bedad0767f8402399c8","583bc594411d4e29b719769fc915b846","41512a00532046c48a79d3f35910dae9","89edddd0912945eb9da82fc2c46f29f0","f52c7682c9344defb3cb39b57df5cdd3","cb240772432c425bb2ff7169b4dc554d","26fa44a0f34f4599a91e8410ffaad99f","77c9f50aab1147e48f1a475a12bf218f","2544f0d786874be08a80365cb40822d8","e0d5f25fa5184d78aa3949329e7bb367","54c0fcb3187143a7b43844db86b21c9a","9db137601afc4dfcb4e7b466e6f4d8bb","109dc7ce8ffd4d01983bf79a2d2cf51c","ae1980a7858c4d1bb963d874f9699f59","8631342c0d784a3bac1e82a4665e67d1","f7154a2699bb4a20840973ea826c8dd7","44b1b65a1c4b4b66bea3e35ea0ef0281","4968045158dc4503ad1b821cf2ae075f","f9c845b9d82b493e908bcd33e21bb970","889e7eb757084b20ba649dbecd8322ba","5760c6fa5ddd483797bbb7f3b5bd80f0","8e51477d7eae49e482577e67517dbfee","0e7774c29f91419b862bebcabd34b742","a305c0c8f75e470ead60f2d6abb4d3e1","acca7312f9534f97a50fd6e71d5c7f25","ad233f87fb4d41e5a0c6d0fc89a888a0","e4e5eb813d0d4d90b575fe4e0ed0c9fc","c8c14b675908421fb5c95a3762164299","dc81faede80e4d229da12e02fa2ee660","31f32437b88f4a4ba14770b9fdf968dd","6eb0cacc1775427bbe75b54560d1a3c0","cb15d2739e9748a1a487479c7b576572","e335005cc7614af9816708b73c947fad","233ab8c63dbe4306b47540d5aab6f1fd","276c01d048ea46ed920daf3edd510fce","17d45bace43a4c03aca7aa90302f7e57","ced578f7fa4a47789bf0f3060efa54c8","f550d2242a424577a60ccf0b4cf88617","a5389071c1b64609a497a7e3ff6adcaf","144e93305074473cbcb536f773dc27cf","62e6f3e6308b4a5ba0a8a00ed25919e0","226b45aba8e24e4abc646e408bf89b81","8920453de84740bcb6eee9e27a438d61","44acb765a6974f3389eb47a493aaba55","7867cf749a0f493994ba9e077366721e","659d39962958421a8f97f0a4af40ee2d","240683e35f4c431f9eb96db4d1f731bd","84a0dbae4f76441096e9b751e9741c05","5b3c640c23024097ae04cdf9747454e9","3db8d97892d64462a72966eba93a852f","c782862f10084c128f3ec8422a4e1923","b029a52481a942f28e707c583986546c","e1eedc54557b45429ac04335692a5901","3bf333b2c59f42d2be89cc37490a5cca","2a6348aec7c64832bda11df2c971afac","fc26b66b3f70458b93d0cd261728c174","a9628fc293724e97837f9d40f31f0af5","a1e66baa31e04a52922085268c2e8970","ac0410b0400a4ff0bac8f88521746837","b77948265ed54e2cba32db23849905d4","13dda8430f7842ba95cace5f6124da24","5bd33a202bb84dca84e570ff8db3d30f","bfd7a3fcedfd4fe1adfd93184a0339dd","347a210e5ced4a5494959e4417fe1635","9f30b68f0469406a9b944bd837d9ad0e","a810f9b79e1b4583a9147dd658169824","94c33bb538284a62abca56fc6314ccfb","f0572f3825664d4d9604e97bcc15061a","fdba53703ff2454bb55ee74114c1e1f0","e31692625e3841d0a9bad27094f2963d","5b799bdbe08b4bb0bf1f70d8a4ba1920","6a3f54deb23f43b2b06e3612301fe1cc","68f69048a7b341b186e7a4d1da51dd30","2c2213d0b7d24915bcb9b8e282237643","41360c6b2a234e7582fd234fb5ea5d04","6381193a1bb54a52a5ed7ebab45cb2ef","76065a5791d5483889c0542726fca2bc","8e79792f16d44d0890875850bb3e32a8","0b97085f2614428aa2c56c079fde881e","1ccd2a4be4f24ea384d0e932b0a692c6","d58d1fb55da645eca4762f2c20bd077b","77314328fc8743139e23ca9e9460c2e5","2905cd8126b24191820dd2ab9905e43e","346ac103e3a04effbfe21c6f7b2b05a2","f6a2516064f843dea9d451f430d7cccf","d34923bb23a64511960c6c5b76826797","8edc94df7fc4404e82855ae4d2f57559","390677d470a04892887a9c16afe9a47c","2966d4dd4f454136ab8075389088898b","2b53fc272bf3454b8b12bc21ad366729","6da16a4433004120a296bb870cff4e2b","ec2e767b7ad4403ab24da27aa90a85ea","6b42a24aed5e42a18c460402f8e392d5","21b0ee607fd747d8aee76d40f2a4940e","329426deb8bc477b9df02d077a5801f9","6c9e184c18c7424d891feb629fa74cc9","2d8fd5b5ad0c46f4bea3f662b5275292","bd498ae6b7fb4849a13d1dfd9a212a71","57be65c4b2384a05a40aad798673979f","bb2fdfb450cd435e84336468907cd5ce","f6cf67e62fac45c7a30a68c9794e6b79","f8bb33317b814cb5844b2e16916c235f","ec8d5ecd87ee4defbe8a517a9cab90bd","22d256b7c18943ad8abf611ec5acd793","32bcb4aaac844ce4a35420c090207773","d1ef404acc5d4d34979a7a134a5f5c97","3379eecdec464be0a751ae6f029a49fc","9893e5d77a6b4f30be4fc31d57eeb233","a6196503429744de962613c351113306","2e36d3cbf3c0402aa2986209dbce5543","9e12fd43256e425d8dd8486d800e994b","8af611ef15f54ca0b9d4cc6c46684b4a","35ec797e155d4140b7bd45b13da3d509","3400be871c254be3bb5dfbd630b9aeb9","e40e6479514d49d5a4176002144b9171","7c340702d44742d9aa9a915d92f4e22d","bc83795d415e4e45ad249aaeb7ab33e0","ffce499bd7774baeadd47af2d9a5521a","fb81855dd3084aaca424ec6866d34ad3","a746e283a3344460b7b5b52c75c1e1b5","7dabf0c0b1be40fab6098c808bc87e86","8949672bd6224236b1a25ab91f1cd3d7","55774be35b824db08cabdff6f0fcc6ad","0a8b5df591034968a746bae636bc48f6","9232f897dd264d1a81773cc5819a5ae2","d3ac42e708ad432191dfb01936f41fb1","5f216a9b1c8c4cb5a401b67a11b69e9d","e3dad10e24c24d859777da8eaf2eb2f0","6c570753b03e4cad8c39c0adbf03167b","839d1bb6270f46c78e166988293484ff","fea52f5ac70c4c18b03ce2b91eb76807","446b4c048dc54b2589cd0a3e7c19c213","15eb373876744eaa8ed09c4db61f0ee9","f4b61f3942ec434fac60bd3f9088dd55","6432cbf5d16344d8b77f824954e7e7e7","50442b688b63428ca32cb0342c182195","aed77354ec624543972846033b3455de","c11be314812b4abf9d9505c54a5e9bed"]},"id":"LGyBKidlcjsL","executionInfo":{"status":"ok","timestamp":1706957276659,"user_tz":-60,"elapsed":322369,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"2321a6de-68c8-4d3b-e1f3-0054ef7ab410"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/804 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb66233eb614a27b9fa4addbe29f8e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/454M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a384f37852be4840a8290df6465969c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/458M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fa44a0f34f4599a91e8410ffaad99f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/452M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4968045158dc4503ad1b821cf2ae075f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/448M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc81faede80e4d229da12e02fa2ee660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/455M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144e93305074473cbcb536f773dc27cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/456M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c782862f10084c128f3ec8422a4e1923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/453M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd33a202bb84dca84e570ff8db3d30f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/456M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f69048a7b341b186e7a4d1da51dd30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/455M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"346ac103e3a04effbfe21c6f7b2b05a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/451M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329426deb8bc477b9df02d077a5801f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/459M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1ef404acc5d4d34979a7a134a5f5c97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/447M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc83795d415e4e45ad249aaeb7ab33e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating data split:   0%|          | 0/8500 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3dad10e24c24d859777da8eaf2eb2f0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[INFO]: Total Training images: 8\n","[INFO]: Total Validation images: 4\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","train_dataloader = _prepare_dataloader(train_dataset, batch_size=2, is_train=True)\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","for step, batch in tqdm(enumerate(train_dataloader)):\n","    print(f\"-------Step: {step}-----\")\n","    print(type(batch))\n","    print(batch.keys())\n","    print(f\"pixel_values : {batch['pixel_values'].shape}\")\n","    print(f\"pixel_mask : {batch['pixel_mask'].shape}\")\n","    print(f\"text_inputs :{batch['text_inputs'].shape}\")\n","    print(f\"task_inputs :{batch['task_inputs'].shape}\")\n","    print(f\"mask_labels type tuple, length: {len(batch['mask_labels'])}\")\n","    for label in batch[\"mask_labels\"]:\n","        print(f\"\\t{label.shape}\")\n","    print(f\"class_labels type tuple, length: {len(batch['class_labels'])}\")\n","    for label in batch[\"class_labels\"]:\n","        print(f\"\\t{label.shape}\")\n","    print(f\"original_images type tuple, length: {len(batch['original_images'])}\")\n","    for img in batch[\"original_images\"]:\n","        print(f\"\\t{img.shape}\")\n","    print(f\"original_segmentation_maps type tuple, length: {len(batch['original_segmentation_maps'])}\")\n","    for img in batch[\"original_segmentation_maps\"]:\n","        print(f\"\\t{img.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awLHWf2QgthK","executionInfo":{"status":"ok","timestamp":1706871226596,"user_tz":-60,"elapsed":1617,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"c76159d9-8bf3-429f-9989-6c99b112bd68"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["1it [00:00,  3.61it/s]"]},{"output_type":"stream","name":"stdout","text":["-------Step: 0-----\n","<class 'transformers.image_processing_utils.BatchFeature'>\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","pixel_values : torch.Size([1, 3, 512, 512])\n","pixel_mask : torch.Size([1, 512, 512])\n","text_inputs :torch.Size([1, 234, 77])\n","task_inputs :torch.Size([1, 77])\n","mask_labels type tuple, length: 1\n","\ttorch.Size([10, 512, 512])\n","class_labels type tuple, length: 1\n","\ttorch.Size([10])\n","original_images type tuple, length: 1\n","\t(1080, 1920, 3)\n","original_segmentation_maps type tuple, length: 1\n","\t(1080, 1920)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","train_dataloader = _prepare_dataloader(train_dataset, batch_size=2, is_train=True)\n","device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","for step, batch in tqdm(enumerate(train_dataloader)):\n","    print(f\"-------Step: {step}-----\")\n","    batch_dict = {\n","        \"pixel_values\": batch[\"pixel_values\"].to(device),\n","        \"mask_labels\": [labels.to(device) for labels in batch[\"mask_labels\"]],\n","        \"class_labels\": [labels.to(device) for labels in batch[\"class_labels\"]],\n","        \"pixel_mask\": batch[\"pixel_mask\"].to(device),\n","        \"text_inputs\": batch[\"text_inputs\"].to(device),\n","        \"task_inputs\": batch[\"task_inputs\"].to(device),\n","    }\n","    with autocast():\n","        outputs = model(**batch_dict)\n","    #del batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuCTZIymOqOY","executionInfo":{"status":"ok","timestamp":1706912025673,"user_tz":-60,"elapsed":9182,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"ebce4725-6fdc-49ee-e1bd-1933d5186f3b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0it [00:00, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["-------Step: 0-----\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","1it [00:08,  8.36s/it]\n"]}]},{"cell_type":"code","source":["print(outputs.keys())\n","print(f'loss shape: {outputs[\"loss\"].shape}, loss: {outputs[\"loss\"].item()}')\n","print(f'class_queries_logits: {outputs[\"class_queries_logits\"].shape}')\n","print(f'masks_queries_logits: {outputs[\"masks_queries_logits\"].shape}')\n","\n","print(f\"auxiliary_predictions type tuple[dicts], length: {len(outputs['auxiliary_predictions'])}\")\n","for auxpred in outputs[\"auxiliary_predictions\"]:\n","        print(f\"\\t{auxpred.keys()}\")\n","        print(f\"\\t\\t{auxpred['class_queries_logits'].shape}\")\n","        print(f\"\\t\\t{auxpred['masks_queries_logits'].shape}\")\n","\n","print(f\"encoder_hidden_states type tuple, length: {len(outputs['encoder_hidden_states'])}\")\n","for enchid in outputs[\"encoder_hidden_states\"]:\n","        print(f\"\\t{enchid.shape}\")\n","\n","print(f\"pixel_decoder_hidden_states type tuple, length: {len(outputs['pixel_decoder_hidden_states'])}\")\n","for enchid in outputs[\"pixel_decoder_hidden_states\"]:\n","        print(f\"\\t{enchid.shape}\")\n","\n","print(f\"transformer_decoder_hidden_states type tuple[dicts], length: {len(outputs['transformer_decoder_hidden_states'])}\")\n","for enchid in outputs[\"transformer_decoder_hidden_states\"]:\n","        print(f\"\\t{enchid.keys()}\")\n","        print(f\"\\t\\t{enchid['class_queries_logits'].shape}\")\n","        print(f\"\\t\\t{enchid['masks_queries_logits'].shape}\")\n","\n","print(f\"transformer_decoder_object_queries type tuple, length: {len(outputs['transformer_decoder_object_queries'])}\")\n","for enchid in outputs[\"transformer_decoder_object_queries\"]:\n","        print(f\"\\t{enchid.shape}\")\n","\n","print(f\"transformer_decoder_contrastive_queries type tuple, length: {len(outputs['transformer_decoder_contrastive_queries'])}\")\n","for enchid in outputs[\"transformer_decoder_contrastive_queries\"]:\n","        print(f\"\\t{enchid.shape}\")\n","\n","print(f\"transformer_decoder_mask_predictions type tuple, length: {len(outputs['transformer_decoder_mask_predictions'])}\")\n","for enchid in outputs[\"transformer_decoder_mask_predictions\"]:\n","        print(f\"\\t{enchid.shape}\")\n","\n","print(f\"transformer_decoder_class_predictions type tuple, length: {len(outputs['transformer_decoder_class_predictions'])}\")\n","for enchid in outputs[\"transformer_decoder_class_predictions\"]:\n","        print(f\"\\t{enchid.shape}\")\n","\n","print(f\"transformer_decoder_auxiliary_predictions type tuple[dicts], length: {len(outputs['transformer_decoder_auxiliary_predictions'])}\")\n","for enchid in outputs[\"transformer_decoder_auxiliary_predictions\"]:\n","        print(f\"\\t{enchid.keys()}\")\n","        print(f\"\\t\\t{enchid['class_queries_logits'].shape}\")\n","        print(f\"\\t\\t{enchid['masks_queries_logits'].shape}\")\n","\n","print(f\"text_queries: {outputs['text_queries'].shape}\")\n","print(f\"task_token: {outputs['task_token'].shape}\")\n","\n","print(f\"attentions type tuple[tuples], length: {len(outputs['attentions'])}\")\n","for enchid in outputs[\"attentions\"]:\n","        print(f\"\\t{len(enchid)}\")\n","        for i in enchid:\n","            print(f\"\\t\\t{i.shape}\")\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpr2pPx5xFoe","executionInfo":{"status":"ok","timestamp":1706871230327,"user_tz":-60,"elapsed":48,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"cc6f54b1-1a46-48f0-8705-c38bc749be7f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["odict_keys(['loss', 'class_queries_logits', 'masks_queries_logits', 'auxiliary_predictions', 'encoder_hidden_states', 'pixel_decoder_hidden_states', 'transformer_decoder_hidden_states', 'transformer_decoder_object_queries', 'transformer_decoder_contrastive_queries', 'transformer_decoder_mask_predictions', 'transformer_decoder_class_predictions', 'transformer_decoder_auxiliary_predictions', 'text_queries', 'task_token', 'attentions'])\n","loss shape: torch.Size([1]), loss: 72.18132019042969\n","class_queries_logits: torch.Size([1, 250, 20])\n","masks_queries_logits: torch.Size([1, 250, 128, 128])\n","auxiliary_predictions type tuple[dicts], length: 9\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","encoder_hidden_states type tuple, length: 4\n","\ttorch.Size([1, 192, 128, 128])\n","\ttorch.Size([1, 384, 64, 64])\n","\ttorch.Size([1, 768, 32, 32])\n","\ttorch.Size([1, 1536, 16, 16])\n","pixel_decoder_hidden_states type tuple, length: 4\n","\ttorch.Size([1, 256, 128, 128])\n","\ttorch.Size([1, 256, 16, 16])\n","\ttorch.Size([1, 256, 32, 32])\n","\ttorch.Size([1, 256, 64, 64])\n","transformer_decoder_hidden_states type tuple[dicts], length: 9\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","transformer_decoder_object_queries type tuple, length: 1\n","\ttorch.Size([250, 256])\n","transformer_decoder_contrastive_queries type tuple, length: 1\n","\ttorch.Size([250, 256])\n","transformer_decoder_mask_predictions type tuple, length: 1\n","\ttorch.Size([250, 128, 128])\n","transformer_decoder_class_predictions type tuple, length: 1\n","\ttorch.Size([250, 20])\n","transformer_decoder_auxiliary_predictions type tuple[dicts], length: 9\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","\tdict_keys(['class_queries_logits', 'masks_queries_logits'])\n","\t\ttorch.Size([1, 250, 20])\n","\t\ttorch.Size([1, 250, 128, 128])\n","text_queries: torch.Size([1, 250, 256])\n","task_token: torch.Size([1, 256])\n","attentions type tuple[tuples], length: 9\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 256])\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 1024])\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 4096])\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 256])\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 1024])\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 4096])\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 256])\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 1024])\n","\t2\n","\t\ttorch.Size([1, 8, 250, 250])\n","\t\ttorch.Size([1, 250, 4096])\n"]}]},{"cell_type":"code","source":["original_images = batch[\"original_images\"] #tuple of tensors\n","target_sizes = [(img.shape[0], img.shape[1]) for img in original_images] #post_process_semantic_segmentation expects list[(int, int)] where list len is batch size\n","predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes) #returns List[torch.Tensor] , list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)"],"metadata":{"id":"7u0wOkNP2xAf","executionInfo":{"status":"ok","timestamp":1706912040313,"user_tz":-60,"elapsed":491,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["```\n","def post_process_semantic_segmentation(\n","        self, outputs, target_sizes: Optional[List[Tuple[int, int]]] = None\n","    ) -> \"torch.Tensor\":\n","        \"\"\"\n","        Converts the output of [`MaskFormerForInstanceSegmentation`] into semantic segmentation maps. Only supports\n","        PyTorch.\n","\n","        Args:\n","            outputs ([`MaskFormerForInstanceSegmentation`]):\n","                Raw outputs of the model.\n","            target_sizes (`List[Tuple[int, int]]`, *optional*):\n","                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n","                final size (height, width) of each prediction. If left to None, predictions will not be resized.\n","        Returns:\n","            `List[torch.Tensor]`:\n","                A list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)\n","                corresponding to the target_sizes entry (if `target_sizes` is specified). Each entry of each\n","                `torch.Tensor` correspond to a semantic class id.\n","        \"\"\"\n","        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n","        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n","\n","        # Remove the null class `[..., :-1]`\n","        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n","        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n","\n","        # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n","        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n","        batch_size = class_queries_logits.shape[0]\n","\n","        # Resize logits and compute semantic segmentation maps\n","        if target_sizes is not None:\n","            if batch_size != len(target_sizes):\n","                raise ValueError(\n","                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n","                )\n","\n","            semantic_segmentation = []\n","            for idx in range(batch_size):\n","                resized_logits = torch.nn.functional.interpolate(\n","                    segmentation[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n","                )\n","                semantic_map = resized_logits[0].argmax(dim=0)\n","                semantic_segmentation.append(semantic_map)\n","        else:\n","            semantic_segmentation = segmentation.argmax(dim=1)\n","            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n","\n","        return semantic_segmentation\n","```"],"metadata":{"id":"IbPpC9rAQzef"}},{"cell_type":"code","source":["print(len(predicted_segmentation_maps)) #list(tensors)\n","print(predicted_segmentation_maps[0].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65sQoyGc3H-A","executionInfo":{"status":"ok","timestamp":1706871230327,"user_tz":-60,"elapsed":41,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"bea3700b-21b1-4234-9fad-b41a7ab6630a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","torch.Size([1080, 1920])\n"]}]},{"cell_type":"code","source":["ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"] ##tuple of tensors\n","predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps] ###list(tensors)\n","jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=len(id2label), average='weighted', ignore_index=255)\n","running_iou = []\n","running_iou.append(jaccard_index(torch.tensor(predicted_segmentation_maps_arr), torch.tensor(ground_truth_segmentation_maps)).item())\n","jacktrain_iou = statistics.mean(running_iou)\n","metric = evaluate.load(\"mean_iou\")\n","metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps_arr)\n","train_miou_per_epoch = metric.compute(num_labels=len(id2label), ignore_index=255)\n","jacktrain_iou, train_miou_per_epoch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381,"referenced_widgets":["c70b17f4799941daa8ae118ba91d75d2","2b38d3012d7545999b636d665f6e9d28","8a219a6c88f24a1ea67121b6c29ff44b","51564aaaa5994b35839edcbfce5148b5","122832b37b394569848a3e62b1dc3447","f444f0ca8bcc45c18ed170de8c631a2e","2b589b4f5d9f4e159e80ea8b1b9816ba","53683e9085684faf87e5d2f6f5b0fb5b","88f0e134a1214b91b8dafae923d465e5","7ad7236737714bee9a357fe77cd94870","062a4ef225ce4f4ba76f9955ecff5879"]},"id":"6PfCVaraR0DR","executionInfo":{"status":"ok","timestamp":1706871259825,"user_tz":-60,"elapsed":10859,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"4da795ab-c795-4e8f-985a-b7f6bfac7ff9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-9aab21b76c27>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  running_iou.append(jaccard_index(torch.tensor(predicted_segmentation_maps_arr), torch.tensor(ground_truth_segmentation_maps)).item())\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c70b17f4799941daa8ae118ba91d75d2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n","  iou = total_area_intersect / total_area_union\n","/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n","  acc = total_area_intersect / total_area_label\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.07859280705451965,\n"," {'mean_iou': 0.01776598931979131,\n","  'mean_accuracy': 0.0348617343176793,\n","  'overall_accuracy': 0.15125048225308643,\n","  'per_category_iou': array([0.        , 0.        , 0.        ,        nan, 0.        ,\n","         0.        ,        nan,        nan, 0.        , 0.        ,\n","         0.16860127, 0.        , 0.        , 0.        , 0.        ,\n","         0.        , 0.09788857, 0.        ,        nan]),\n","  'per_category_accuracy': array([0.        , 0.        , 0.        ,        nan,        nan,\n","         0.        ,        nan,        nan, 0.        , 0.        ,\n","         0.23189   , 0.        , 0.        , 0.        , 0.        ,\n","         0.        , 0.25617428, 0.        ,        nan])})"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"] ##tuple of tensors\n","predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps] ###list(tensors)\n","jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=len(id2label) + 1, average='weighted')\n","running_iou = []\n","running_iou.append(jaccard_index(torch.tensor(predicted_segmentation_maps_arr), torch.tensor(ground_truth_segmentation_maps)).item())\n","jacktrain_iou = statistics.mean(running_iou)\n","metric = evaluate.load(\"mean_iou\")\n","metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps_arr)\n","train_miou_per_epoch = metric.compute(num_labels=len(id2label) + 1, ignore_index=None)\n","running_iou, jacktrain_iou, train_miou_per_epoch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wn_Pfy1o3vhr","executionInfo":{"status":"ok","timestamp":1706872058951,"user_tz":-60,"elapsed":18120,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"b8fdeb7c-2273-44f0-c7fd-1f96b7194a1d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n","  iou = total_area_intersect / total_area_union\n","/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n","  acc = total_area_intersect / total_area_label\n"]},{"output_type":"execute_result","data":{"text/plain":["([0.07859280705451965],\n"," 0.07859280705451965,\n"," {'mean_iou': 0.01776598931979131,\n","  'mean_accuracy': 0.0348617343176793,\n","  'overall_accuracy': 0.15125048225308643,\n","  'per_category_iou': array([0.        , 0.        , 0.        ,        nan, 0.        ,\n","         0.        ,        nan,        nan, 0.        , 0.        ,\n","         0.16860127, 0.        , 0.        , 0.        , 0.        ,\n","         0.        , 0.09788857, 0.        ,        nan,        nan]),\n","  'per_category_accuracy': array([0.        , 0.        , 0.        ,        nan,        nan,\n","         0.        ,        nan,        nan, 0.        , 0.        ,\n","         0.23189   , 0.        , 0.        , 0.        , 0.        ,\n","         0.        , 0.25617428, 0.        ,        nan,        nan])})"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"] ##tuple of tensors\n","predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps] ###list(tensors)\n","jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=len(id2label) + 1, average='weighted') #calculates statistics for each label and computes weighted average using their support\n","running_iou = []\n","iou_score = jaccard_index(torch.tensor(predicted_segmentation_maps_arr), torch.tensor(ground_truth_segmentation_maps))\n","running_iou.append(iou_score.item())\n","jacktrain_iou = statistics.mean(running_iou)\n","iou_score, iou_score.shape, jacktrain_iou"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3DuRR_Pe6GY","executionInfo":{"status":"ok","timestamp":1706872357365,"user_tz":-60,"elapsed":2334,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"768b61bb-a46e-4f12-b2b6-6ac98d295989"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0786), torch.Size([]), 0.07859280705451965)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"] ##tuple of tensors\n","predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps] ###list(tensors)\n","jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=len(id2label) + 1, average='none') #calculates statistic for each label and applies no reduction\n","iou_scores = jaccard_index(torch.tensor(predicted_segmentation_maps_arr), torch.tensor(ground_truth_segmentation_maps))\n","running_iou = []\n","running_iou.append(torch.mean(iou_scores).item())\n","jacktrain_iou = statistics.mean(running_iou)\n","iou_scores, running_iou, jacktrain_iou"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBBPZneOf8xD","executionInfo":{"status":"ok","timestamp":1706872826036,"user_tz":-60,"elapsed":1454,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"7d75ace8-9eef-4c91-ebdc-a614e49e57c1"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.1686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0979, 0.0000,\n","         0.0000, 0.0000]),\n"," [0.013324491679668427],\n"," 0.013324491679668427)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"] ##tuple of tensors\n","predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps] ###list(tensors)\n","jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=len(id2label) + 1, average='micro') #Calculate statistics for each label and average them\n","iou_score = jaccard_index(torch.tensor(predicted_segmentation_maps_arr), torch.tensor(ground_truth_segmentation_maps))\n","running_iou = []\n","running_iou.append(iou_score.item())\n","jacktrain_iou = statistics.mean(running_iou)\n","iou_score, running_iou"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nit-cahlfq8C","executionInfo":{"status":"ok","timestamp":1706872881381,"user_tz":-60,"elapsed":5596,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"fc93fbe3-c78f-42dd-b4e7-4b3ea568aaf8"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0818), [0.0818123146891594])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"] ##tuple of tensors\n","predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps] ###list(tensors)\n","jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=len(id2label) + 1, average='macro') #Sum statistics over all labels\n","iou_score = jaccard_index(torch.tensor(predicted_segmentation_maps_arr), torch.tensor(ground_truth_segmentation_maps))\n","running_iou = []\n","running_iou.append(iou_score.item())\n","jacktrain_iou = statistics.mean(running_iou)\n","iou_score, running_iou"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WNKsq1DIic7C","executionInfo":{"status":"ok","timestamp":1706872809542,"user_tz":-60,"elapsed":2346,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"ebfe74a2-c0df-445a-da95-79fb8cda2d5c"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0178), [0.017765989527106285])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from torch import tensor\n","from torchmetrics.classification import MulticlassJaccardIndex\n","target = tensor([2, 1, 0, 0])\n","preds = tensor([2, 1, 0, 1])\n","# preds = tensor([1, 2, 19, 17, 17])\n","# target = tensor([1, 2, 3, 17, 4])\n","metric = MulticlassJaccardIndex(num_classes=3, average='none')\n","metric_samp = JaccardIndex(task=\"multiclass\", num_classes=3, average='none') ##calculates statistic for each label and applies no reduction\n","#  Calculate the metric for each class separately, and return the metric for every class. Note that if a given class doesn’t occur in the preds or target, the value for the class will be nan.\n","metric(preds, target), metric_samp(preds, target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53ySvLQFl8oe","executionInfo":{"status":"ok","timestamp":1706877454543,"user_tz":-60,"elapsed":316,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"9baae495-c8d9-4848-afc1-9dcde6875c49"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000,\n","         0.0000, 0.0000]),\n"," tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000,\n","         0.0000, 0.0000]))"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["from torch import tensor\n","from torchmetrics.classification import MulticlassJaccardIndex\n","target = tensor([2, 1, 0, 0])\n","preds = tensor([2, 1, 0, 1])\n","metric = MulticlassJaccardIndex(num_classes=3)\n","metric_samp = JaccardIndex(task=\"multiclass\", num_classes=3, average='macro') #Sum statistics over all labels (sum / numlabels).\n","# Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class).\n","metric(preds, target), metric_samp(preds, target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pn7KOJbckVr7","executionInfo":{"status":"ok","timestamp":1706873353688,"user_tz":-60,"elapsed":509,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"83f32270-8494-4320-9965-09cb2655e5ac"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.6667), tensor(0.6667))"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["from torch import tensor\n","from torchmetrics.classification import MulticlassJaccardIndex\n","target = tensor([2, 1, 0, 0])\n","preds = tensor([2, 1, 0, 1])\n","metric = MulticlassJaccardIndex(num_classes=3, average=\"micro\")\n","metric_samp = JaccardIndex(task=\"multiclass\", num_classes=3, average='micro') #Calculate statistics for each label and average them ()\n","# Calculate the metric globally, across all samples and classes.\n","metric(preds, target), metric_samp(preds, target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7W3TS1mflJ4w","executionInfo":{"status":"ok","timestamp":1706873525131,"user_tz":-60,"elapsed":256,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"4255475c-ab69-429c-9658-62184503124a"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.6000), tensor(0.6000))"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from torch import tensor\n","from torchmetrics.classification import MulticlassJaccardIndex\n","target = tensor([2, 1, 0, 0])\n","preds = tensor([2, 1, 0, 1])\n","metric = MulticlassJaccardIndex(num_classes=3, average=\"weighted\")\n","metric_samp = JaccardIndex(task=\"multiclass\", num_classes=3, average='weighted') ##calculates statistics for each label and computes weighted average using their support\n","# Calculate the metric for each class separately, and average the metrics across classes, weighting each class by its support (tp + fn).\n","metric(preds, target), metric_samp(preds, target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixTkSbYllzFu","executionInfo":{"status":"ok","timestamp":1706873548958,"user_tz":-60,"elapsed":19,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"249f1356-cbf4-4819-db47-78d4f943663f"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.6250), tensor(0.6250))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#Deeplab v3\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","\n","class _StreamMetrics(object):\n","    def __init__(self):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","    def update(self, gt, pred):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","    def get_results(self):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","    def to_str(self, metrics):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","    def reset(self):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","class StreamSegMetrics(_StreamMetrics):\n","    \"\"\"\n","    Stream Metrics for Semantic Segmentation Task\n","    \"\"\"\n","    def __init__(self, n_classes):\n","        self.n_classes = n_classes\n","        self.confusion_matrix = np.zeros((n_classes, n_classes))\n","        self.class_counts = np.zeros(n_classes)\n","\n","    def update(self, label_trues, label_preds):\n","        for lt, lp in zip(label_trues, label_preds):\n","            #hist = self._fast_hist(lt.flatten(), lp.flatten())\n","            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten())\n","            #self.class_counts += np.sum(hist, axis=1)\n","\n","    @staticmethod\n","    def to_str(results):\n","        string = \"\\n\"\n","        for k, v in results.items():\n","            if k!=\"Class IoU\":\n","                string += \"%s: %f\\n\"%(k, v)\n","        return string\n","\n","    def _fast_hist(self, label_true, label_pred):\n","        mask = (label_true >= 0) & (label_true < self.n_classes)\n","        hist = np.bincount(\n","            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n","            minlength=self.n_classes ** 2,\n","        ).reshape(self.n_classes, self.n_classes)\n","        return hist\n","\n","    def get_results(self):\n","        \"\"\"Returns accuracy score evaluation result.\n","            - overall accuracy\n","            - mean accuracy\n","            - mean IU\n","            - fwavacc\n","        \"\"\"\n","        hist = self.confusion_matrix\n","        acc = np.diag(hist).sum() / hist.sum()\n","        acc_cls = np.diag(hist) / hist.sum(axis=1)\n","        print(acc_cls)\n","        #acc_cls = np.diag(hist) / self.class_counts\n","        mean_acc_cls = np.nanmean(acc_cls)\n","        cls_acc = dict(zip(range(self.n_classes), acc_cls))\n","        iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n","        mean_iou = np.nanmean(iou)\n","        freq = hist.sum(axis=1) / hist.sum()\n","        fwavacc = (freq[freq > 0] * iou[freq > 0]).sum()\n","        cls_iou = dict(zip(range(self.n_classes), iou))\n","\n","\n","        return {\n","                \"Overall Acc\": acc,\n","                \"Mean Acc\": mean_acc_cls,\n","                \"Class Acc\": cls_acc,\n","                \"FreqW Acc\": fwavacc,\n","                \"Mean IoU\": mean_iou,\n","                \"Class IoU\": cls_iou,\n","            }\n","\n","    def reset(self):\n","        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n","\n","class AverageMeter(object):\n","    \"\"\"Computes average values\"\"\"\n","    def __init__(self):\n","        self.book = dict()\n","\n","    def reset_all(self):\n","        self.book.clear()\n","\n","    def reset(self, id):\n","        item = self.book.get(id, None)\n","        if item is not None:\n","            item[0] = 0\n","            item[1] = 0\n","\n","    def update(self, id, val):\n","        record = self.book.get(id, None)\n","        if record is None:\n","            self.book[id] = [val, 1]\n","        else:\n","            record[0]+=val\n","            record[1]+=1\n","\n","    def get_results(self, id):\n","        record = self.book.get(id, None)\n","        assert record is not None\n","        return record[0] / record[1]"],"metadata":{"id":"cvVOw8T_xXjF","executionInfo":{"status":"ok","timestamp":1706912429337,"user_tz":-60,"elapsed":3,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# target = np.array([2, 1, 0, 0])\n","# preds = np.array([2, 1, 0, 1])\n","preds = np.array([1, 2, 19, 17, 17])\n","target = np.array([1, 2, 3, 17, 4])\n","metric = StreamSegMetrics(n_classes=21)\n","# for batch_idx, _ in enumerate(dataLoader):\n","#     metric.update(label_trues, label_preds) # update every iterations\n","#     metrics_out = metric.get_results()\n","metric.update(target, preds)\n","metric.get_results()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FeKjxmzUxlN3","executionInfo":{"status":"ok","timestamp":1706912184925,"user_tz":-60,"elapsed":1112,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"4cfa93d1-e2b4-4cf8-ffb1-631d87c9b114"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-215ed41672b6>:67: RuntimeWarning: invalid value encountered in divide\n","  acc_cls = np.diag(hist) / self.class_counts\n","<ipython-input-11-215ed41672b6>:70: RuntimeWarning: invalid value encountered in divide\n","  iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Overall Acc': 0.6,\n"," 'Mean Acc': 0.6,\n"," 'Class Acc': {0: nan,\n","  1: 1.0,\n","  2: 1.0,\n","  3: 0.0,\n","  4: 0.0,\n","  5: nan,\n","  6: nan,\n","  7: nan,\n","  8: nan,\n","  9: nan,\n","  10: nan,\n","  11: nan,\n","  12: nan,\n","  13: nan,\n","  14: nan,\n","  15: nan,\n","  16: nan,\n","  17: 1.0,\n","  18: nan,\n","  19: nan,\n","  20: nan},\n"," 'FreqW Acc': 0.5,\n"," 'Mean IoU': 0.4166666666666667,\n"," 'Class IoU': {0: nan,\n","  1: 1.0,\n","  2: 1.0,\n","  3: 0.0,\n","  4: 0.0,\n","  5: nan,\n","  6: nan,\n","  7: nan,\n","  8: nan,\n","  9: nan,\n","  10: nan,\n","  11: nan,\n","  12: nan,\n","  13: nan,\n","  14: nan,\n","  15: nan,\n","  16: nan,\n","  17: 0.5,\n","  18: nan,\n","  19: 0.0,\n","  20: nan}}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from torch import tensor\n","from torchmetrics.classification import MulticlassJaccardIndex\n","\n","preds = tensor([1, 2, 19, 17, 17])\n","target = tensor([1, 2, 3, 17, 4])\n","\n","metric_all = JaccardIndex(task=\"multiclass\", num_classes=21, average='none') ##calculates statistic for each label and applies no reduction\n","metric_weighted = JaccardIndex(task=\"multiclass\", num_classes=21, average='weighted') #cls weight\n","metric_micro = JaccardIndex(task=\"multiclass\", num_classes=21, average='micro') #no cls weight\n","metric_macro = JaccardIndex(task=\"multiclass\", num_classes=21, average='macro') #emptyignred\n","#  Calculate the metric for each class separately, and return the metric for every class. Note that if a given class doesn’t occur in the preds or target, the value for the class will be nan.\n","metric_all(preds, target), metric_weighted(preds, target), metric_micro(preds, target), metric_macro(preds, target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebWT-_v01rCu","executionInfo":{"status":"ok","timestamp":1706878022775,"user_tz":-60,"elapsed":312,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"8f86c957-ef06-4676-8b26-66b80e92f2d2"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000,\n","         0.0000, 0.0000, 0.0000]),\n"," tensor(0.5000),\n"," tensor(0.4286),\n"," tensor(0.4167))"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["import evaluate\n","metric = evaluate.load(\"mean_iou\")\n","preds = [np.array([[1, 2, 19, 17, 17]], dtype=np.uint8)]\n","target = [np.array([[1, 2, 3, 17, 4]], dtype=np.uint8)]\n","metric.add_batch(references=target, predictions=preds)\n","metrics = metric.compute(num_labels=21, ignore_index=None)\n","metrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52l_N7PG9Ujm","executionInfo":{"status":"ok","timestamp":1706880175121,"user_tz":-60,"elapsed":1633,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"1981fa3a-56a6-4488-8915-2441ad715661"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'mean_iou': 0.4166666666666667,\n"," 'mean_accuracy': 0.6,\n"," 'overall_accuracy': 0.6,\n"," 'per_category_iou': array([nan, 1. , 1. , 0. , 0. , nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan, 0.5, nan, 0. , nan]),\n"," 'per_category_accuracy': array([nan,  1.,  1.,  0.,  0., nan, nan, nan, nan, nan, nan, nan, nan,\n","        nan, nan, nan, nan,  1., nan, nan, nan])}"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["bin_p = torch.bincount(preds, minlength = 21) #[0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,1,0]\n","bin_t = torch.bincount(target, minlength = 21) #[0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]\n","\n","# mask is 1 if pred == target, 0 otherwise\n","hit_mask = 1 - torch.clamp((preds - target), min = 0, max = 1) #[1,1,0,1,0]\n","\n","hits_class = preds[hit_mask.nonzero()].squeeze() #[1,2,17]\n","bin_hits   = torch.bincount(hits_class, minlength = 21) #[0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]\n","union = bin_p + bin_t - bin_hits # [0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,2,0,1,0]\n","\n","# Per-class Jaccard Index (no reduction to a single value):\n","jaccard_percls = bin_hits / union # [0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.5,0,0,0]\n","\n","# Basic Jaccard Index (no weighting by class). Note that classes with no predictions and no true labels do not affect the score as they affect neither sum:\n","\n","jaccard_noweight = bin_hits.sum() / union.sum()  # 3/7\n","\n","# Mean Jaccard Index (each class counts for the same proportion of the total score). Note that classes with no predictions and no true labels do affect the score:\n","jaccard_mean = (bin_hits / union).mean()\n","\n","# Mean Jaccard Index with empty classes ignored:\n","jaccard_per_class = (bin_hits / union)\n","# get class indices with at least one prediction or example\n","mask = (bin_p + bin_t).nonzero()\n","jaccard_emptyignred = jaccard_per_class[mask] # takes mean of non-zero bins only\n","\n","jaccard_percls, jaccard_noweight, jaccard_mean, jaccard_emptyignred, jaccard_emptyignred.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_XdTzq9z0X-9","executionInfo":{"status":"ok","timestamp":1706879282010,"user_tz":-60,"elapsed":255,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"3fcf003d-386b-47b3-a5f7-623b025f074d"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([   nan, 1.0000, 1.0000, 0.0000, 0.0000,    nan,    nan,    nan,    nan,\n","            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.5000,\n","            nan, 0.0000,    nan]),\n"," tensor(0.4286),\n"," tensor(nan),\n"," tensor([[1.0000],\n","         [1.0000],\n","         [0.0000],\n","         [0.0000],\n","         [0.5000],\n","         [0.0000]]),\n"," tensor(0.4167))"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["#Deeplab v3 with per class accuracy too\n","class _StreamMetrics(object):\n","    def __init__(self):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","    def update(self, gt, pred):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","    def get_results(self):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","    def to_str(self, metrics):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","    def reset(self):\n","        \"\"\" Overridden by subclasses \"\"\"\n","        raise NotImplementedError()\n","\n","class StreamSegMetrics(_StreamMetrics):\n","    \"\"\"\n","    Stream Metrics for Semantic Segmentation Task\n","    \"\"\"\n","    def __init__(self, n_classes):\n","        self.n_classes = n_classes\n","        self.confusion_matrix = np.zeros((n_classes, n_classes))\n","        self.class_counts = np.zeros(n_classes)\n","\n","    def update(self, label_trues, label_preds):\n","        for lt, lp in zip(label_trues, label_preds):\n","            hist = self._fast_hist( lt.flatten(), lp.flatten() )\n","            self.confusion_matrix += hist\n","            self.class_counts += np.sum(hist, axis=1)\n","\n","    @staticmethod\n","    def to_str(results):\n","        string = \"\\n\"\n","        for k, v in results.items():\n","            if k!=\"Class IoU\":\n","                string += \"%s: %f\\n\"%(k, v)\n","\n","        return string\n","\n","    def _fast_hist(self, label_true, label_pred):\n","        mask = (label_true >= 0) & (label_true < self.n_classes)\n","        hist = np.bincount(\n","            self.n_classes * label_true[mask].astype(int) + label_pred[mask],\n","            minlength=self.n_classes ** 2,\n","        ).reshape(self.n_classes, self.n_classes)\n","        return hist\n","\n","    def get_results(self):\n","        \"\"\"Returns accuracy score evaluation result.\n","            - overall accuracy\n","            - mean accuracy\n","            - mean IU\n","            - fwavacc\n","        \"\"\"\n","        hist = self.confusion_matrix\n","        acc = np.diag(hist).sum() / hist.sum()\n","        acc_cls = np.diag(hist) / hist.sum(axis=1)\n","        mean_acc_cls = np.nanmean(acc_cls)\n","        cls_acc = dict(zip(range(self.n_classes), acc_cls))\n","        iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n","        mean_iou = np.nanmean(iou)\n","        freq = hist.sum(axis=1) / hist.sum()\n","        fwavacc = (freq[freq > 0] * iou[freq > 0]).sum()\n","        cls_iou = dict(zip(range(self.n_classes), iou))\n","\n","\n","        return {\n","                \"Overall Acc\": acc,\n","                \"Mean Acc\": mean_acc_cls,\n","                \"Class Acc\": cls_acc,\n","                \"FreqW Acc\": fwavacc,\n","                \"Mean IoU\": mean_iou,\n","                \"Class IoU\": cls_iou,\n","            }\n","\n","    def reset(self):\n","        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n","\n"],"metadata":{"id":"awEG8VU-6uAc","executionInfo":{"status":"ok","timestamp":1706914163745,"user_tz":-60,"elapsed":944,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"] ##tuple of tensors\n","predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps] ###list(tensors)\n","target = np.array(ground_truth_segmentation_maps)\n","preds = np.array(predicted_segmentation_maps_arr)\n","print(target.shape, preds.shape)\n","mask = (target == 255)\n","target[mask] = 19\n","mask = (preds == 255)\n","target[mask] = 19\n","metric = StreamSegMetrics(n_classes=20)\n","metric.update(target, preds)\n","metric.get_results() #macro miou\n","\n","\n","##from evaluate metrics - 2 examples in a batch\n","#  {'mean_iou': 0.03787085134002275,\n","#  'mean_accuracy': 0.06991241265792929,\n","#  'overall_accuracy': 0.2521464641095291,\n","#  'per_category_iou': array([0., 0., 0.03660676, 0., 0., 0.01204219, 0., 0., 0.19161021, 0.15466964, 0.32461737, nan, 0., 0., 0., 0., 0., 0., 0., 0. ]),\n","#  'per_category_accuracy': array([nan, 0., 0.06782437, 0., 0., 0.02028438, nan, 0., 0.30300096, 0.24739035, 0.55001096, nan, 0., 0., 0., 0., 0., 0., 0., 0. ])}\n","#from evaluate metrics - 1 example in a batch\n","#'mean_accuracy': 0.10544952947346956,\n","#'overall_accuracy': 0.5138020833333333,\n","# mean_iou': 0.04959345701968404,(macro)\n","# 'overall_accuracy': 0.5138020833333333,\n","#'per_category_iou': array([0., 0., 0., nan, 0., 0.01505165, nan, nan, 0.57328831, 0.00450974, 0.00227178, nan, 0. , nan, nan, 0., nan, 0., nan, 0.]),\n","#'per_category_accuracy': array([nan, nan, 0. , nan, 0. , 0.01680321, nan, nan, 0.75683603, 0.27858427, 0.00227178, nan, 0. , nan, nan, 0.,  nan, 0. , nan, 0.])}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSXtwWEvAgfm","executionInfo":{"status":"ok","timestamp":1706914167571,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"6125ad23-eaae-469c-8b01-cc220938add3"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 1080, 1920) (2, 1080, 1920)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-50-86c303fa1eb0>:64: RuntimeWarning: invalid value encountered in divide\n","  acc_cls = np.diag(hist) / hist.sum(axis=1)\n","<ipython-input-50-86c303fa1eb0>:67: RuntimeWarning: invalid value encountered in divide\n","  iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Overall Acc': 0.24305266203703704,\n"," 'Mean Acc': 0.06602913255809337,\n"," 'Class Acc': {0: 1.3370860882877945e-05,\n","  1: 0.0,\n","  2: 0.0678243652167416,\n","  3: 0.0,\n","  4: 0.0,\n","  5: 0.020284380082900465,\n","  6: nan,\n","  7: 0.0,\n","  8: 0.3030009622905641,\n","  9: 0.24739034960621809,\n","  10: 0.5500109579883735,\n","  11: nan,\n","  12: 0.0,\n","  13: 0.0,\n","  14: 0.0,\n","  15: 0.0,\n","  16: 0.0,\n","  17: 0.0,\n","  18: 0.0,\n","  19: 0.0},\n"," 'FreqW Acc': 0.14422928046420855,\n"," 'Mean IoU': 0.03635265536926692,\n"," 'Class IoU': {0: 1.8916053392452305e-06,\n","  1: 0.0,\n","  2: 0.03637584027089031,\n","  3: 0.0,\n","  4: 0.0,\n","  5: 0.011748474460895145,\n","  6: 0.0,\n","  7: 0.0,\n","  8: 0.1834153852807456,\n","  9: 0.1351783106669716,\n","  10: 0.32398054973122953,\n","  11: nan,\n","  12: 0.0,\n","  13: 0.0,\n","  14: 0.0,\n","  15: 0.0,\n","  16: 0.0,\n","  17: 0.0,\n","  18: 0.0,\n","  19: 0.0}}"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["from torch import tensor\n","from torchmetrics.classification import MulticlassJaccardIndex, JaccardIndex\n","\n","target = torch.tensor(batch[\"original_segmentation_maps\"])\n","preds = torch.tensor([t.detach().cpu().numpy() for t in predicted_segmentation_maps])\n","print(target.shape, preds.shape)\n","\n","#we need to remap label 255 to 19 cuz the class labels will be 0 to num_classes-1\n","mask = (target == 255)\n","target[mask] = 19\n","mask = (preds == 255)\n","target[mask] = 19\n","\n","metric_all = JaccardIndex(task=\"multiclass\", num_classes=20, average='none') ##calculates statistic for each label and applies no reduction\n","metric_weighted = JaccardIndex(task=\"multiclass\", num_classes=20, average='weighted') #cls weight\n","metric_micro = JaccardIndex(task=\"multiclass\", num_classes=20, average='micro') #no cls weight\n","metric_macro = MulticlassJaccardIndex(num_classes=20, average='macro') #emptyignred\n","# Calculate the metric for each class separately, and return the metric for every class. Note that if a given class doesn’t occur in the preds or target, the value for the class will be nan.\n","metric_all(preds, target), metric_weighted(preds, target), metric_micro(preds, target), metric_macro(preds, target)\n","\n","##from evaluate metrics - 2 examples in a batch\n","#  {'mean_iou': 0.03787085134002275, #macro, but we ll look into weighted hereafter\n","#  'mean_accuracy': 0.06991241265792929,\n","#  'overall_accuracy': 0.2521464641095291,\n","#  'per_category_iou': array([0., 0., 0.03660676, 0., 0., 0.01204219, 0., 0., 0.19161021, 0.15466964, 0.32461737, nan, 0., 0., 0., 0., 0., 0., 0., 0. ]),\n","#  'per_category_accuracy': array([nan, 0., 0.06782437, 0., 0., 0.02028438, nan, 0., 0.30300096, 0.24739035, 0.55001096, nan, 0., 0., 0., 0., 0., 0., 0., 0. ])}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ctl_zy2XFwFS","executionInfo":{"status":"ok","timestamp":1706914200788,"user_tz":-60,"elapsed":1739,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"90036935-b0de-4bf3-94f0-beae2ddd7adf"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1080, 1920]) torch.Size([2, 1080, 1920])\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([1.8916e-06, 0.0000e+00, 3.6376e-02, 0.0000e+00, 0.0000e+00, 1.1748e-02,\n","         0.0000e+00, 0.0000e+00, 1.8342e-01, 1.3518e-01, 3.2398e-01, 0.0000e+00,\n","         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","         0.0000e+00, 0.0000e+00]),\n"," tensor(0.1442),\n"," tensor(0.1383),\n"," tensor(0.0364))"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["#accuracy\n","from torch import tensor\n","from torchmetrics.classification import Accuracy, MulticlassAccuracy\n","\n","target = torch.tensor(batch[\"original_segmentation_maps\"])\n","preds = torch.tensor([t.detach().cpu().numpy() for t in predicted_segmentation_maps])\n","print(target.shape, preds.shape)\n","\n","#we need to remap label 255 to 19 cuz the class labels will be 0 to num_classes-1\n","mask = (target == 255)\n","target[mask] = 19\n","mask = (preds == 255)\n","target[mask] = 19\n","\n","metric_all = Accuracy(task=\"multiclass\", num_classes=20, average='none') ##calculates statistic for each label and applies no reduction\n","metric_weighted = Accuracy(task=\"multiclass\", num_classes=20, average='weighted') #cls weight\n","metric_micro = Accuracy(task=\"multiclass\", num_classes=20, average='micro') #no cls weight\n","metric_macro = MulticlassAccuracy(num_classes=20, average='macro') #emptyignred\n","# Calculate the metric for each class separately, and return the metric for every class. Note that if a given class doesn’t occur in the preds or target, the value for the class will be nan.\n","metric_all(preds, target), metric_weighted(preds, target), metric_micro(preds, target), metric_macro(preds, target)\n","\n","###from evaluate metrics - 2 examples in a batch\n","#  {'mean_iou': 0.03787085134002275, #macro, but we ll look into weighted hereafter\n","#  'mean_accuracy': 0.06991241265792929,\n","#  'overall_accuracy': 0.2521464641095291,\n","#  'per_category_iou': array([0., 0., 0.03660676, 0., 0., 0.01204219, 0., 0., 0.19161021, 0.15466964, 0.32461737, nan, 0., 0., 0., 0., 0., 0., 0., 0. ]),\n","#  'per_category_accuracy': array([nan, 0., 0.06782437, 0., 0., 0.02028438, nan, 0., 0.30300096, 0.24739035, 0.55001096, nan, 0., 0., 0., 0., 0., 0., 0., 0. ])}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcmOpl-cDWB8","executionInfo":{"status":"ok","timestamp":1706914893848,"user_tz":-60,"elapsed":2949,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"a68a818c-8c86-4739-fa8c-6e4498c7956d"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1080, 1920]) torch.Size([2, 1080, 1920])\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([1.3371e-05, 0.0000e+00, 6.7824e-02, 0.0000e+00, 0.0000e+00, 2.0284e-02,\n","         0.0000e+00, 0.0000e+00, 3.0300e-01, 2.4739e-01, 5.5001e-01, 0.0000e+00,\n","         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","         0.0000e+00, 0.0000e+00]),\n"," tensor(0.2431),\n"," tensor(0.2431),\n"," tensor(0.0626))"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["import torch\n","torchmetc_mean_acc = torch.nanmean(torch.tensor([1.3371e-05, 0.0000e+00, 6.7824e-02, 0.0000e+00, 0.0000e+00, 2.0284e-02,\n","         0.0000e+00, 0.0000e+00, 3.0300e-01, 2.4739e-01, 5.5001e-01, 0.0000e+00,\n","         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","         0.0000e+00, 0.0000e+00]))\n","evaluatemetric_meanacc = torch.nanmean(torch.tensor([np.nan, 0., 0.06782437, 0., 0., 0.02028438, np.nan, 0., 0.30300096, 0.24739035, 0.55001096, np.nan, 0., 0., 0., 0., 0., 0., 0., 0. ]))\n","torchmetc_mean_acc, evaluatemetric_meanacc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QgHHBla8FHkz","executionInfo":{"status":"ok","timestamp":1706916325204,"user_tz":-60,"elapsed":7406,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"add2bcb9-3edd-403c-c474-6c858bb4337d"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.0594), tensor(0.0699))"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import evaluate\n","metric = evaluate.load(\"mean_iou\")\n","target = torch.tensor(batch[\"original_segmentation_maps\"]) ##tuple of tensors\n","preds = torch.tensor([t.detach().cpu().numpy() for t in predicted_segmentation_maps]) ###list(tensors)#\n","mask = (target == 255)\n","target[mask] = 19\n","mask = (preds == 255)\n","target[mask] = 19\n","metric.add_batch(references=target, predictions=preds)\n","metrics = metric.compute(num_labels=20, ignore_index=False)\n","metrics #macro"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_SfCxTZVk_X","executionInfo":{"status":"ok","timestamp":1706914375603,"user_tz":-60,"elapsed":24606,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"c2182abd-a267-4500-d409-3ae2e0ec8032"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stderr","text":["/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n","  iou = total_area_intersect / total_area_union\n","/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n","  acc = total_area_intersect / total_area_label\n"]},{"output_type":"execute_result","data":{"text/plain":["{'mean_iou': 0.03787085134002275,\n"," 'mean_accuracy': 0.06991241265792929,\n"," 'overall_accuracy': 0.2521464641095291,\n"," 'per_category_iou': array([0.        , 0.        , 0.03660676, 0.        , 0.        ,\n","        0.01204219, 0.        , 0.        , 0.19161021, 0.15466964,\n","        0.32461737,        nan, 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ]),\n"," 'per_category_accuracy': array([       nan, 0.        , 0.06782437, 0.        , 0.        ,\n","        0.02028438,        nan, 0.        , 0.30300096, 0.24739035,\n","        0.55001096,        nan, 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ])}"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["batch.keys(), batch[\"pixel_values\"].shape, batch[\"pixel_mask\"].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-ZpdYtA7RMN","executionInfo":{"status":"ok","timestamp":1706913098029,"user_tz":-60,"elapsed":16,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"b6cb88ec-03bd-49c7-ebfc-9b498c5c3407"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps']),\n"," torch.Size([2, 3, 512, 512]),\n"," torch.Size([2, 512, 512]))"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["import evaluate\n","metric = evaluate.load(\"mean_iou\")\n","target = torch.tensor(batch[\"original_segmentation_maps\"]) ##tuple of tensors\n","preds = torch.tensor([t.detach().cpu().numpy() for t in predicted_segmentation_maps]) ###list(tensors)#\n","metric.add_batch(references=target, predictions=preds)\n","metrics_ = metric.compute(num_labels=20, ignore_index=255, reduce_labels=False)\n","metrics = metric._compute(references=target, predictions=preds, num_labels=20, ignore_index=255, reduce_labels=False) #_compute just for test\n","metrics, metrics_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGd7mH-MLU9k","executionInfo":{"status":"ok","timestamp":1706914516502,"user_tz":-60,"elapsed":21116,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"268de525-85db-40f0-9cce-af76ba3fa54c"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'mean_iou': 0.03837246366153994,\n","  'mean_accuracy': 0.06991319917915768,\n","  'overall_accuracy': 0.24426136471030693,\n","  'per_category_iou': array([1.92833906e-06, 0.00000000e+00, 3.63758403e-02, 0.00000000e+00,\n","         0.00000000e+00, 1.17523316e-02, 0.00000000e+00, 0.00000000e+00,\n","         1.83415385e-01, 1.35178311e-01, 3.23980550e-01,            nan,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00,            nan]),\n","  'per_category_accuracy': array([1.33708609e-05, 0.00000000e+00, 6.78243652e-02, 0.00000000e+00,\n","         0.00000000e+00, 2.02843801e-02,            nan, 0.00000000e+00,\n","         3.03000962e-01, 2.47390350e-01, 5.50010958e-01,            nan,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00,            nan])},\n"," {'mean_iou': 0.03837246366153994,\n","  'mean_accuracy': 0.06991319917915768,\n","  'overall_accuracy': 0.24426136471030693,\n","  'per_category_iou': array([1.92833906e-06, 0.00000000e+00, 3.63758403e-02, 0.00000000e+00,\n","         0.00000000e+00, 1.17523316e-02, 0.00000000e+00, 0.00000000e+00,\n","         1.83415385e-01, 1.35178311e-01, 3.23980550e-01,            nan,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00,            nan]),\n","  'per_category_accuracy': array([1.33708609e-05, 0.00000000e+00, 6.78243652e-02, 0.00000000e+00,\n","         0.00000000e+00, 2.02843801e-02,            nan, 0.00000000e+00,\n","         3.03000962e-01, 2.47390350e-01, 5.50010958e-01,            nan,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00,            nan])})"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","source":["- 'micro':\n","\n","    - Calculate the metric globally, across all samples and classes.\n","It treats all predictions and targets as a single set, and computes the metric globally.\n","\n","- 'macro':\n","\n","    - Calculate the metric for each class separately and average the metrics across classes. It gives equal weight to each class, irrespective of class imbalance.\n","\n","- 'weighted':\n","\n","    - Calculate the metric for each class separately and average the metrics across classes. Weight each class by its support, where support is the number of true positives plus false negatives.\n","\n","- 'none' or None:\n","\n","    - Calculate the metric for each class separately and return the metric for every class.It does not perform any averaging or reduction.\n","- 'samples':\n","\n","    - Calculate the metric for each sample, and average the metrics across samples.It treats each sample equally, regardless of class or sample imbalance."],"metadata":{"id":"aCpWHy_JgEdC"}},{"cell_type":"code","source":["from torchmetrics.classification import Accuracy, MulticlassAccuracy, MulticlassJaccardIndex, JaccardIndex\n","\n","#calculates metrics(iou, acc) for each label and applies no reduction, we ignore background class\n","iou_per_cls = JaccardIndex(task=\"multiclass\", num_classes=len(id2label), average=\"none\", ignore_index=255)\n","acc_per_cls = Accuracy(task=\"multiclass\", num_classes=len(id2label), average=\"none\", ignore_index=255)\n","# Calculate metrics(miou, Overallacc) for each class separately and average the metrics across classes after weighing each class by its support = TP + FN\n","mIoU_weighted = JaccardIndex(task=\"multiclass\", num_classes=len(id2label), average='weighted', ignore_index=255)\n","acc_overall_weighted = Accuracy(task=\"multiclass\", num_classes=20, average='weighted', ignore_index=255)\n","\n","#in dataloader batch\n","target = torch.stack([torch.from_numpy(arr) for arr in batch[\"original_segmentation_maps\"]]).to(\"cpu\")\n","preds = torch.stack([t.detach().cpu() for t in predicted_segmentation_maps])\n","\n","iou_per_cls_batch = iou_per_cls(preds, target)\n","acc_per_cls_batch = acc_per_cls(preds, target)\n","mIoU_batch = mIoU_weighted(preds, target)\n","acc_overall_batch = acc_overall_weighted(preds, target)\n","\n","#after dataloader\n","iou_per_cls_epoch = iou_per_cls.compute()\n","mIoU_epoch = mIoU_weighted.compute()\n","acc_per_cls_epoch = acc_per_cls.compute()\n","acc_overall_epoch = acc_overall_weighted.compute()\n","mean_acc_epoch = torch.nanmean(acc_per_cls_epoch)\n","\n","train_metrics_epoch = {'mean_iou': mIoU_epoch.item(),\n","                       'mean_accuracy': mean_acc_epoch.item(),\n","                       'overall_accuracy': acc_overall_epoch.item(),\n","                       'per_category_iou': iou_per_cls_epoch,\n","                       'per_category_accuracy': acc_per_cls_epoch}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"_eP320yvGakt","executionInfo":{"status":"error","timestamp":1706916993622,"user_tz":-60,"elapsed":277,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"7f94bb67-ea54-4ffd-a053-bc57877c1a44"},"execution_count":9,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'torchmetrics'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-0ea9877fa14b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMulticlassAccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMulticlassJaccardIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJaccardIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#calculates metrics(iou, acc) for each label and applies no reduction, we ignore background class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0miou_per_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJaccardIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"none\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0macc_per_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"none\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["from torchmetrics.classification import Accuracy, MulticlassAccuracy, MulticlassJaccardIndex, JaccardIndex\n","\n","class OneformerTrainer:\n","    def __init__(self, args, run, gpu_id):\n","        self.seed = args.seed\n","        self.railsem_ds = load_railsem_dataset(gpu_id)\n","        labels_info = get_labels()\n","        self.id2label = labels_info[\"id2label\"]\n","        #self.id2label[\"255\"] = \"background\"\n","        self.label2id = labels_info[\"label2id\"]\n","        self.labels = labels_info[\"labels\"]\n","        self.color_palette = labels_info[\"color_palette\"]\n","        self.color_palette.append([0, 0, 0]) # Appending background color\n","        print(f\"[INFO]: id2label: {self.id2label}, color_palette for background:{self.color_palette[-1]}\")\n","        print(f\"[INFO]: Setting the seed {self.seed}\")\n","        set_seeds(self.seed)\n","        # ckpt_dataset_name, ckpt_name,\n","        #                     id2label=None, label2id=None, labels=None,\n","        #                     is_train=False\n","        self.model, self.processor = get_model_and_processor(\n","            args.ckpt_dataset_name, args.ckpt_model_name, id2label=self.id2label, label2id=self.label2id,\n","            labels=self.labels, is_train=args.is_train\n","        )\n","        self.epochs = args.num_epochs\n","        self.batch_size = args.batch_size\n","        self.learning_rate = args.lr\n","        self.device = gpu_id\n","        self.model.to(self.device)\n","        self.is_train = args.is_train\n","        self.optimizer = AdamW(params=self.model.parameters(), lr=self.learning_rate)\n","        self.scheduler = ReduceLROnPlateau(self.optimizer, \"min\", factor=0.5, patience=5, threshold=0.0001, min_lr=1e-6)\n","        self.scaler = GradScaler()\n","        self.best_loss = 1e+5\n","        self.start_epoch = 0\n","        #calculates metrics(iou, acc) for each label and applies no reduction, we ignore background class\n","        self.iou_per_cls = JaccardIndex(task=\"multiclass\", num_classes=len(id2label), average=\"none\", ignore_index=255).to(\"cpu\")\n","        self.acc_per_cls = Accuracy(task=\"multiclass\", num_classes=len(id2label), average=\"none\", ignore_index=255).to(\"cpu\")\n","        # Calculate metrics(miou, Overallacc) for each class separately and average the metrics across classes after weighing each class by its support = TP + FN\n","        self.mIoU_weighted = JaccardIndex(task=\"multiclass\", num_classes=len(id2label), average='weighted', ignore_index=255).to(\"cpu\")\n","        self.acc_overall_weighted = Accuracy(task=\"multiclass\", num_classes=len(id2label), average='weighted', ignore_index=255).to(\"cpu\")\n","        self.jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=len(id2label), average='weighted', ignore_index=255).to(\"cpu\")\n","        if run is not None:\n","            print(f\"[INFO]: Run object: {run}\")\n","            run.config[\"classes\"] = len(self.id2label)\n","            self.wandb_config = run.config\n","            self.run = run\n","        self.log = self.run is not None\n","\n","    def _get_datasets(self):\n","        print(\"[INFO]: Preparing the Datasets...\")\n","        try:\n","            if \"train\" in self.railsem_ds and \"validation\" in self.railsem_ds:\n","                train_split = self.railsem_ds[\"train\"].select(\n","                    indices=range(6)\n","                )\n","                val_split = self.railsem_ds[\"validation\"].select(\n","                    indices=range(2)\n","                )\n","                # train_split = self.railsem_ds[\"train\"]\n","                # val_split = self.railsem_ds[\"validation\"]\n","                print(f\"[INFO]: Total training images: {len(train_split)}\")\n","                print(f\"[INFO]: Total Validation images: {len(val_split)}\")\n","            elif \"data\" in self.railsem_ds:\n","                print(\"[INFO]: No splits found.. Generating splits\")\n","                print(f\"[INFO]: Total images: {len(self.railsem_ds['data'])}\")\n","                splits = self.railsem_ds[\"data\"].train_test_split(\n","                    test_size=0.05, shuffle=True\n","                )\n","                train_split = splits[\"train\"].select(indices=(range(8)))\n","                val_split = splits[\"test\"].select(indices=(range(4)))\n","                # train_split = splits[\"train\"]\n","                # val_split = splits[\"test\"]\n","                print(f\"[INFO]: Total Training images: {len(train_split)}\")\n","                print(f\"[INFO]: Total Validation images: {len(val_split)}\")\n","            else:\n","                raise ValueError(\"Data splits not in required format\")\n","        except ValueError as e:\n","            print(e)\n","        # Get the transforms\n","        train_transforms, val_transforms = get_transforms()\n","        # Custom Dataset\n","        train_dataset = CustomDataset(train_split, train_transforms)\n","        val_dataset = CustomDataset(val_split, val_transforms)\n","        return train_dataset, val_dataset\n","\n","    def _prepare_dataloader(self, dataset: torch.utils.data.Dataset,\n","                           batch_size: int, is_train):\n","        sampler = None\n","        dataloader = DataLoader(\n","            dataset,\n","            batch_size=batch_size,\n","            pin_memory=True,\n","            shuffle=(is_train and sampler is None),\n","            sampler=sampler,\n","            collate_fn=self._collate_fn,\n","        )\n","        return dataloader\n","\n","    def _collate_fn(self, batch):\n","        inputs = list(zip(*batch))\n","        images = inputs[0]\n","        segmentation_maps = inputs[1]\n","        batch = self.processor(images,\n","                               segmentation_maps=segmentation_maps,\n","                               task_inputs=[\"semantic\"] * len(images),\n","                               return_tensors=\"pt\")\n","        batch[\"original_images\"] = inputs[2]\n","        batch[\"original_segmentation_maps\"] = inputs[3]\n","        return batch\n","\n","    def train_step(self, train_dataloader: torch.utils.data.DataLoader,\n","                   global_step):\n","        running_train_loss = 0.0\n","        num_samples = 0\n","        total_steps = len(train_dataloader) * self.epochs\n","        metric = evaluate.load(\"mean_iou\")\n","        running_iou =  []\n","        self.model.train()\n","        for step, batch in enumerate(tqdm(train_dataloader)):\n","            self.optimizer.zero_grad()\n","            batch_dict = {\n","                \"pixel_values\": batch[\"pixel_values\"].to(self.device),\n","                \"mask_labels\": [labels.to(self.device)\n","                                for labels in batch[\"mask_labels\"]],\n","                \"class_labels\": [labels.to(self.device)\n","                                 for labels in batch[\"class_labels\"]],\n","                \"pixel_mask\": batch[\"pixel_mask\"].to(self.device),\n","                \"text_inputs\": batch[\"text_inputs\"].to(self.device),\n","                \"task_inputs\": batch[\"task_inputs\"].to(self.device),\n","            }\n","            # Forward Pass\n","            with autocast():\n","                outputs = self.model(**batch_dict)\n","                train_loss = outputs.loss\n","\n","            running_train_loss += train_loss.item()\n","            train_batch_size = batch[\"pixel_values\"].size(0)\n","            num_samples += train_batch_size\n","            print(f\"Train Loss after {str(num_samples).zfill(6)} examples:\"\n","                  f\"{train_loss.item():.3f}\\n\")\n","\n","            # Post process segmentation\n","            original_images = batch[\"original_images\"]\n","            target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","            predicted_segmentation_maps = self.processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n","\n","            # Get ground truth segmentation maps\n","            ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","            predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps]\n","            metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps_arr)\n","            #torchmetrics\n","            target = torch.stack([torch.from_numpy(arr) for arr in batch[\"original_segmentation_maps\"]]).to(\"cpu\")\n","            preds = torch.stack([t.detach().cpu() for t in predicted_segmentation_maps])\n","\n","            iou_per_cls_batch = self.iou_per_cls(preds, target)\n","            acc_per_cls_batch = self.acc_per_cls(preds, target)\n","            mIoU_batch = self.mIoU_weighted(preds, target)\n","            acc_overall_batch = self.acc_overall_weighted(preds, target)\n","            print(\"train stats per batch\")\n","            print(iou_per_cls_batch, acc_per_cls_batch, mIoU_batch, acc_overall_batch)\n","            #Jackard\n","            running_iou.append(self.jaccard_index(preds, target).item())\n","\n","\n","            # Log loss to wandb every 2 batches i.e after every 2 steps\n","            global_step += 1\n","            if ((step + 1) % 2) == 0 or step == len(train_dataloader) - 1:\n","                if self.log:\n","                    self.run.log({\n","                        \"custom_step\": int(global_step),\n","                        \"Train_loss\": train_loss.item()\n","                    })\n","            # Backward pass\n","            self.scaler.scale(train_loss).backward()\n","            self.scaler.step(self.optimizer)\n","            self.scaler.update()\n","            del batch, outputs, predicted_segmentation_maps_arr, ground_truth_segmentation_maps\n","\n","        train_loss_per_epoch = running_train_loss / len(train_dataloader)\n","        train_miou_per_epoch = metric.compute(num_labels=len(self.id2label),\n","                                              ignore_index=255)\n","        #after dataloader\n","        iou_per_cls_epoch = self.iou_per_cls.compute()\n","        mIoU_epoch = self.mIoU_weighted.compute()\n","        acc_per_cls_epoch = self.acc_per_cls.compute()\n","        acc_overall_epoch = self.acc_overall_weighted.compute()\n","        mean_acc_epoch = torch.nanmean(acc_per_cls_epoch)\n","        train_metrics_epoch = {'mean_iou': mIoU_epoch.item(),\n","                       'mean_accuracy': mean_acc_epoch.item(),\n","                       'overall_accuracy': acc_overall_epoch.item(),\n","                       'per_category_iou': iou_per_cls_epoch,\n","                       'per_category_accuracy': acc_per_cls_epoch}\n","        self.iou_per_cls.reset()\n","        self.mIoU_weighted.reset()\n","        self.acc_per_cls.reset()\n","        self.acc_overall_weighted.reset()\n","        #Jaccard\n","        print(f\"train running iou: {running_iou}\")\n","        jacctrain_iou = statistics.mean(running_iou)\n","        return train_loss_per_epoch, train_miou_per_epoch, global_step, jacctrain_iou, train_metrics_epoch\n","\n","    def val_step(self, val_dataloader, epoch, global_step):\n","        running_eval_loss = 0.0\n","        num_samples_eval = 0\n","        metric = evaluate.load(\"mean_iou\")\n","        running_iou = []\n","        # Eval Loop\n","        self.model.eval()\n","        for step, batch in enumerate(tqdm(val_dataloader)):\n","            batch_dict = {\n","                \"pixel_values\": batch[\"pixel_values\"].to(self.device),\n","                \"mask_labels\": [labels.to(self.device)\n","                                for labels in batch[\"mask_labels\"]],\n","                \"class_labels\": [labels.to(self.device)\n","                                 for labels in batch[\"class_labels\"]],\n","                \"pixel_mask\": batch[\"pixel_mask\"].to(self.device),\n","                \"text_inputs\": batch[\"text_inputs\"].to(self.device),\n","                \"task_inputs\": batch[\"task_inputs\"].to(self.device),\n","            }\n","\n","            # Eval forward pass\n","            with torch.inference_mode():\n","                eval_outputs = self.model(**batch_dict)\n","\n","            eval_loss = eval_outputs.loss.item()\n","            running_eval_loss += eval_loss\n","\n","            # Post process segmentation\n","            original_images = batch[\"original_images\"]\n","            target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","            predicted_segmentation_maps = self.processor.post_process_semantic_segmentation(eval_outputs, target_sizes=target_sizes)\n","\n","            #torchmetrics\n","            target = torch.stack([torch.from_numpy(arr) for arr in batch[\"original_segmentation_maps\"]]).to(\"cpu\")\n","            preds = torch.stack([t.detach().cpu() for t in predicted_segmentation_maps])\n","\n","            iou_per_cls_batch = self.iou_per_cls(preds, target)\n","            acc_per_cls_batch = self.acc_per_cls(preds, target)\n","            mIoU_batch = self.mIoU_weighted(preds, target)\n","            acc_overall_batch = self.acc_overall_weighted(preds, target)\n","\n","            # Jaccard iou\n","            running_iou.append(self.jaccard_index(preds, target).item())\n","\n","            # Get ground truth segmentation maps\n","            ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","            predicted_segmentation_maps_arr = [t.detach().cpu().numpy() for t in predicted_segmentation_maps]\n","            metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps_arr)\n","            val_batch_size = batch[\"pixel_values\"].size(0)\n","            num_samples_eval += val_batch_size\n","            print(f\"Val Loss : {eval_loss:.3f}\\n\")\n","            # Wandb logging\n","            global_step += 1\n","            if self.log:\n","                self.run.log({\n","                    \"custom_step\": int(global_step),\n","                    \"epoch\": epoch,\n","                    \"Val_loss\": eval_loss\n","                })\n","            del batch, eval_outputs\n","        # Wandb Logging of images and predictions\n","        if self.log:\n","            print(f\"[INFO]: Logging Validation Images, on device: {self.device}\")\n","            table_data = wandb_mask_log(\n","                original_images, ground_truth_segmentation_maps,\n","                predicted_segmentation_maps_arr, self.id2label, self.run\n","            )\n","            self.run.log({\"predictions\": table_data})\n","\n","        del original_images, predicted_segmentation_maps\n","\n","        #after dataloader\n","        iou_per_cls_epoch = self.iou_per_cls.compute()\n","        mIoU_epoch = self.mIoU_weighted.compute()\n","        acc_per_cls_epoch = self.acc_per_cls.compute()\n","        acc_overall_epoch = self.acc_overall_weighted.compute()\n","        mean_acc_epoch = torch.nanmean(acc_per_cls_epoch)\n","        val_metrics_epoch = {'mean_iou': mIoU_epoch.item(),\n","                       'mean_accuracy': mean_acc_epoch.item(),\n","                       'overall_accuracy': acc_overall_epoch.item(),\n","                       'per_category_iou': iou_per_cls_epoch,\n","                       'per_category_accuracy': acc_per_cls_epoch}\n","        eval_loss_per_epoch = running_eval_loss / len(val_dataloader)\n","        eval_iou_per_epoch = metric.compute(num_labels=len(self.id2label),\n","                                            ignore_index=255)\n","        self.iou_per_cls.reset()\n","        self.mIoU_weighted.reset()\n","        self.acc_per_cls.reset()\n","        self.acc_overall_weighted.reset()\n","        #Jaccard\n","        print(running_iou)\n","        jaccval_iou = statistics.mean(running_iou)\n","        return eval_loss_per_epoch, eval_iou_per_epoch, global_step, jaccval_iou, val_metrics_epoch\n","\n","    def train(self):\n","        print(\"[INFO]: Started Training the model...\")\n","        # Datasets\n","        train_dataset, val_dataset = self._get_datasets()\n","        # DataLoaders\n","        train_dataloader = self._prepare_dataloader(\n","            train_dataset, batch_size=self.batch_size,\n","            is_train=self.is_train\n","        )\n","        val_dataloader = self._prepare_dataloader(\n","            val_dataset, batch_size=self.batch_size, is_train=False\n","        )\n","\n","        # wandb.watch(self.model, log_freq=4)\n","        model_results = {\"train_loss\": [], \"train_miou\": [], \"train_perCatAcc\": [], \"train_perCatIoU\": [], \"train_overallacc\": [], \"train_mAcc\": [],\n","                         \"val_loss\": [], \"val_miou\": [], \"val_perCatAcc\": [], \"val_perCatIoU\": [], \"val_overallacc\": [], \"val_mAcc\": [],\n","                        \"train_miou_t\": [], \"train_perCatAcc_t\": [], \"train_perCatIoU_t\": [], \"train_overallacc_t\": [], \"train_mAcc_t\": [], \"jacktrain_iou\": [],\n","                         \"val_miou_t\": [], \"val_perCatAcc_t\": [], \"val_perCatIoU_t\": [], \"val_overallacc_t\": [], \"val_mAcc_t\": [], \"jackval_iou\": [],\n","                         }\n","        global_step = 0\n","\n","        print(\"[INFO]: Setting the custom step\")\n","        self.run.define_metric(\"custom_step\")\n","        self.run.define_metric(\"Train_loss\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"Val_loss\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"train_epoch_loss\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"train_mIoU\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"train_mAcc\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"train_OverallAcc\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"val_epoch_loss\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"val_mIoU\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"val_mAcc\", step_metric=\"custom_step\")\n","        self.run.define_metric(\"val_OverallAcc\", step_metric=\"custom_step\")\n","\n","        train_miou = None\n","        # Training Loop\n","        for epoch in tqdm(range(self.start_epoch, self.epochs)):\n","            batch_size = train_dataloader.batch_size\n","            print(f\"\\n-------[GPU: {self.device}] | Epoch: {epoch} | \"\n","                    f\"Batchsize: {batch_size} | Steps: \"\n","                    f\"{len(train_dataloader)}-------\")\n","\n","            train_loss, train_miou, global_step, jacktrain_iou, train_metrics_epoch = self.train_step(\n","                train_dataloader, global_step\n","            )\n","            eval_loss, eval_miou, global_step, jackval_iou, val_metrics_epoch = self.val_step(\n","                val_dataloader, epoch, global_step\n","            )\n","            self.scheduler.step(eval_loss)\n","\n","            #Saving results per epoch\n","            model_results[\"train_loss\"].append(train_loss)\n","            model_results[\"train_miou\"].append(train_miou[\"mean_iou\"])\n","            model_results[\"train_perCatAcc\"].append(train_miou[\"per_category_accuracy\"])\n","            model_results[\"train_perCatIoU\"].append(train_miou[\"per_category_iou\"])\n","            model_results[\"train_overallacc\"].append(train_miou[\"overall_accuracy\"])\n","            model_results[\"train_mAcc\"].append(train_miou[\"mean_accuracy\"])\n","            model_results[\"val_loss\"].append(eval_loss)\n","            model_results[\"val_miou\"].append(eval_miou[\"mean_iou\"])\n","            model_results[\"val_perCatAcc\"].append(eval_miou[\"per_category_accuracy\"])\n","            model_results[\"val_perCatIoU\"].append(eval_miou[\"per_category_iou\"])\n","            model_results[\"val_overallacc\"].append(eval_miou[\"overall_accuracy\"])\n","            model_results[\"val_mAcc\"].append(eval_miou[\"mean_accuracy\"])\n","\n","            model_results[\"train_miou_t\"].append(train_metrics_epoch[\"mean_iou\"])\n","            model_results[\"train_perCatAcc_t\"].append(train_metrics_epoch[\"per_category_accuracy\"])\n","            model_results[\"train_perCatIoU_t\"].append(train_metrics_epoch[\"per_category_iou\"])\n","            model_results[\"train_overallacc_t\"].append(train_metrics_epoch[\"overall_accuracy\"])\n","            model_results[\"train_mAcc_t\"].append(train_metrics_epoch[\"mean_accuracy\"])\n","            model_results[\"jacktrain_iou\"].append(jacktrain_iou)\n","            model_results[\"val_miou_t\"].append(val_metrics_epoch[\"mean_iou\"])\n","            model_results[\"val_perCatAcc_t\"].append(val_metrics_epoch[\"per_category_accuracy\"])\n","            model_results[\"val_perCatIoU_t\"].append(val_metrics_epoch[\"per_category_iou\"])\n","            model_results[\"val_overallacc_t\"].append(val_metrics_epoch[\"overall_accuracy\"])\n","            model_results[\"val_mAcc_t\"].append(val_metrics_epoch[\"mean_accuracy\"])\n","            model_results[\"jackval_iou\"].append(jackval_iou)\n","\n","            print(f\"\\nEpoch : {epoch} | Train Loss: {train_loss} | \"\n","                  f\"Train mIoU: {train_miou['mean_iou']} | \"\n","                  f\"Train mAcc: {train_miou['mean_accuracy']} | \"\n","                  f\"Eval Loss: {eval_loss} | \"\n","                  f\"Eval mIoU: {eval_miou['mean_iou']} | \"\n","                  f\"Eval mAcc: {eval_miou['mean_accuracy']}\\n\")\n","            if self.log:\n","                print(\"Logging Train epoch metrics\")\n","                self.run.log({\n","                            \"custom_step\": int(global_step),\n","                            \"train_epoch_loss\": train_loss,\n","                            \"train_mIoU\": train_miou[\"mean_iou\"],\n","                            \"train_mAcc\": train_miou[\"mean_accuracy\"],\n","                            \"train_OverallAcc\": train_miou[\"overall_accuracy\"],\n","                })\n","                self.run.log({\n","                            \"custom_step\": int(global_step),\n","                            \"train_mIoU_t\": train_metrics_epoch[\"mean_iou\"],\n","                            \"train_mAcc_t\": train_metrics_epoch[\"mean_accuracy\"],\n","                            \"train_OverallAcc_t\": train_metrics_epoch[\"overall_accuracy\"],\n","                })\n","                print(\"Logging Val epoch metrics\")\n","                self.run.log({\n","                            \"custom_step\": int(global_step),\n","                            \"val_epoch_loss\": eval_loss,\n","                            \"val_mIoU\": eval_miou[\"mean_iou\"],\n","                            \"val_mAcc\": eval_miou[\"mean_accuracy\"],\n","                            \"val_OverallAcc\": eval_miou[\"overall_accuracy\"],\n","                })\n","                self.run.log({\n","                            \"custom_step\": int(global_step),\n","                            \"val_mIoU_t\": val_metrics_epoch[\"mean_iou\"],\n","                            \"val_mAcc_t\": val_metrics_epoch[\"mean_accuracy\"],\n","                            \"val_OverallAcc_t\": val_metrics_epoch[\"overall_accuracy\"],})\n","            # Saving checkpoint\n","            if eval_loss < self.best_loss and self.device == 0:\n","                self.best_loss = eval_loss\n","\n","            self.start_epoch += 1\n","\n","        if self.log:\n","            if train_miou is not None:\n","                print(\"Logging Charts\")\n","                # Log average per_cat_acc of from model_results\n","                per_cat_acc_array = np.array(model_results['train_perCatAcc'])\n","                # Replace NaN values with 0 cuz sometimes you have nan values for a single category\n","                per_cat_acc_array[np.isnan(per_cat_acc_array)] = 0\n","                # Calculate the average across instances for each label, ignoring NaN values\n","                avg_per_cat_acc_array = np.nanmean(per_cat_acc_array, axis=0)\n","                data = [[label, acc] for (label, acc) in zip(\n","                    self.processor.image_processor.metadata[\"class_names\"],\n","                    avg_per_cat_acc_array)\n","                ]\n","                avg_acc_table = wandb.Table(data=data,\n","                                        columns=[\"Class_Name\", \"Average_Accuracy\"])\n","                self.run.log({\n","                    \"avg_per_category_acc_bar_chart\": wandb.plot.bar(\n","                        avg_acc_table, \"Class_Name\", \"Average_Accuracy\",\n","                        title=\"Average Per category accuracy\")\n","                })\n","\n","\n","                # Log average per_cat_iou of from model_results\n","                per_cat_iou_array = np.array(model_results['train_perCatIoU'])\n","                per_cat_iou_array[np.isnan(per_cat_iou_array)] = 0\n","                # Calculate the average across instances for each label, ignoring NaN values\n","                avg_per_cat_iou_array = np.nanmean(per_cat_iou_array, axis=0)\n","                data = [[label, acc] for (label, acc) in zip(\n","                    self.processor.image_processor.metadata[\"class_names\"],\n","                    avg_per_cat_iou_array)\n","                ]\n","                avg_iou_table = wandb.Table(data=data,\n","                                        columns=[\"Class_Name\", \"Average_IoU\"])\n","                self.run.log({\n","                    \"avg_per_category_iou_bar_chart\": wandb.plot.bar(\n","                        avg_iou_table, \"Class_Name\", \"Average_IoU\",\n","                        title=\"Average Per category IOU\")\n","                })\n","\n","                #log IoU and Acc of all categories from last epoch\n","                data = [[label, acc] for (label, acc) in zip(\n","                    self.processor.image_processor.metadata[\"class_names\"],\n","                    train_miou[\"per_category_accuracy\"])\n","                ]\n","                acc_table = wandb.Table(data=data,\n","                                        columns=[\"Class_Name\", \"Accuracy\"])\n","                self.run.log({\n","                    \"per_category_acc_bar_chart\": wandb.plot.bar(\n","                        acc_table, \"Class_Name\", \"Accuracy\",\n","                        title=\"Per category accuracy\")\n","                })\n","                data = [[label, acc] for (label, acc) in zip(\n","                    self.processor.image_processor.metadata[\"class_names\"],\n","                    train_miou[\"per_category_iou\"])\n","                ]\n","                iou_table = wandb.Table(data=data, columns=[\"Class_Name\", \"IoU\"])\n","                self.run.log({\n","                    \"per_category_iou_bar_chart\": wandb.plot.bar(\n","                        iou_table, \"Class_Name\", \"IoU\",\n","                        title=\"Per category IOU\")\n","                })\n","                #log IoU and Acc of all categories from last epoch from torch metrics\n","                data = [[label, acc] for (label, acc) in zip(\n","                    self.processor.image_processor.metadata[\"class_names\"],\n","                    train_metrics_epoch[\"per_category_accuracy\"])\n","                ]\n","                acc_table = wandb.Table(data=data,\n","                                        columns=[\"Class_Name_t\", \"Accuracy_t\"])\n","                self.run.log({\n","                    \"per_category_acc_bar_chart_t\": wandb.plot.bar(\n","                        acc_table, \"Class_Name_t\", \"Accuracy_t\",\n","                        title=\"Per category accuracy torchmetrics\")\n","                })\n","                data = [[label, acc] for (label, acc) in zip(\n","                    self.processor.image_processor.metadata[\"class_names\"],\n","                    train_metrics_epoch[\"per_category_iou\"])\n","                ]\n","                iou_table = wandb.Table(data=data, columns=[\"Class_Name_t\", \"IoU_t\"])\n","                self.run.log({\n","                    \"per_category_iou_bar_chart_t\": wandb.plot.bar(\n","                        iou_table, \"Class_Name_t\", \"IoU_t\",\n","                        title=\"Per category IOU torchmetrics\")\n","                })\n","                # Log average per_cat_acc of from model_results\n","                per_cat_acc_array_t = model_results['train_perCatAcc_t']\n","\n","                # Replace NaN values with 0 cuz sometimes you have nan values for a single category\n","                nan_masks = [torch.isnan(tensor) for tensor in per_cat_acc_array_t]\n","                per_cat_acc_array_t = [torch.where(nan_mask, torch.tensor(0.0), tensor) for tensor, nan_mask in zip(per_cat_acc_array_t, nan_masks)]\n","\n","                # Calculate the average across instances for each label, ignoring NaN values\n","                avg_per_cat_acc_array_t = torch.stack(per_cat_acc_array_t).nanmean(dim=0)\n","                #avg_per_cat_acc_array_t = torch.nanmean(per_cat_acc_array_t, axis=0)\n","                data = [[label, acc] for (label, acc) in zip(\n","                    self.processor.image_processor.metadata[\"class_names\"],\n","                    avg_per_cat_acc_array_t)\n","                ]\n","                avg_acc_table_t = wandb.Table(data=data,\n","                                        columns=[\"Class_Name_t\", \"Average_Accuracy_t\"])\n","                self.run.log({\n","                    \"avg_per_category_acc_bar_chart_t\": wandb.plot.bar(\n","                        avg_acc_table_t, \"Class_Name_t\", \"Average_Accuracy_t\",\n","                        title=\"Average Per category accuracy torchmetrics\")\n","                })\n","\n","\n","                # Log average per_cat_iou of from model_results\n","                per_cat_iou_array_t = model_results['train_perCatIoU_t']\n","                nan_masks = [torch.isnan(tensor) for tensor in per_cat_iou_array_t]\n","                per_cat_iou_array_t = [torch.where(nan_mask, torch.tensor(0.0), tensor) for tensor, nan_mask in zip(per_cat_iou_array_t, nan_masks)]\n","                #per_cat_iou_array_t[torch.isnan(per_cat_iou_array_t)] = 0\n","\n","                # Calculate the average across instances for each label, ignoring NaN values\n","                avg_per_cat_iou_array_t = torch.stack(per_cat_iou_array_t).nanmean(dim=0)\n","                #avg_per_cat_iou_array_t = torch.nanmean(per_cat_iou_array_t, axis=0)\n","                data = [[label, acc] for (label, acc) in zip(\n","                    self.processor.image_processor.metadata[\"class_names\"],\n","                    avg_per_cat_iou_array_t)\n","                ]\n","                avg_iou_table_t = wandb.Table(data=data,\n","                                        columns=[\"Class_Name_t\", \"Average_IoU_t\"])\n","                self.run.log({\n","                    \"avg_per_category_iou_bar_chart\": wandb.plot.bar(\n","                        avg_iou_table_t, \"Class_Name_t\", \"Average_IoU_t\",\n","                        title=\"Average Per category IOU torchmetrics\")\n","                })\n","        return model_results\n",""],"metadata":{"id":"N1srpW2wxSHo","executionInfo":{"status":"ok","timestamp":1707047814318,"user_tz":-60,"elapsed":548,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"bK16vgsk4tzO","executionInfo":{"status":"ok","timestamp":1707040181206,"user_tz":-60,"elapsed":10133,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"8c0228b9-e522-4147-821b-e4c2e1f75c4c"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VByHOZbh5_4U","executionInfo":{"status":"ok","timestamp":1706957506179,"user_tz":-60,"elapsed":15404,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"b3b340e5-efff-4c1e-a413-e81e1a2acc09"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"code","source":["import torch\n","import gc\n","import wandb\n","import subprocess, os, shutil\n","\n","import argparse\n","import os\n","from typing import List, Dict, Tuple\n","import warnings\n","from datetime import datetime, timedelta\n","\n","\n","def main(run, args):\n","    if torch.cuda.is_available():\n","        rank = 0  # sets device=cuda:0 explicilty if gpu exist & only 1 available\n","    else:\n","        rank = \"cpu\"\n","    if args.is_train:\n","        print(\"[INFO]: Finetuning the Oneformer model with\"\n","              f\" {args.ckpt_model_name} backbone trained on\"\n","              f\" {args.ckpt_dataset_name} dataset on device {rank}.\")\n","        trainer = OneformerTrainer(args, run, rank)\n","        model_results = trainer.train()\n","        return model_results\n","\n","class myNamespace:\n","    def __init__(self, **kwargs):\n","        self.__dict__.update(kwargs)\n","\n","args = myNamespace(is_train=True, ckpt_model_name=\"swin\", ckpt_dataset_name=\"cityscapes\",\n","                   num_epochs=2, lr=1e-4, batch_size=2, seed=1)\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))\n","wandb_project_name = (\n","        f\"OF_Finetuning_{args.ckpt_dataset_name}_{args.ckpt_model_name}_\"\n","        f\"railSem19_small_id2label\"\n","    )\n","wandb_config = dict(\n","        epochs=args.num_epochs,\n","        batch_size=args.batch_size,\n","        dataset=f\"{args.ckpt_dataset_name}_RailSem19\",\n","        architecture=f\"OF_{args.ckpt_model_name}_L\",\n","        project_name=wandb_project_name,\n","    )\n","print(f\"[INFO]: Initializing wandb\")\n","wandb.require(\"service\")\n","run =  wandb.init(\n","        project=wandb_project_name,\n","        config=wandb_config,\n","        entity=os.environ.get(\"WANDB_ENTITY\"),\n","        resume=True,\n","        mode=\"offline\"\n",")\n","rdir = os.path.split(run.dir)[0]\n","\n","\n","print(f\"Cuda support: {torch.cuda.is_available()},\"\n","        f\"devices exist : {torch.cuda.device_count()}\")\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.reset_max_memory_allocated()\n","model_results = main(run, args)\n","# mp.spawn(fn=main, args=(run, args), nprocs=args.world_size)\n","print(\"[INFO] Finishing the run....\")\n","run.finish()\n","print(\"[INFO] Run Finished....\")\n","if rdir:\n","    print(f\"[INFO]: Syncing {rdir}\")\n","    subprocess.run([\"wandb\", \"sync\", rdir])\n","    print(f\"Removing {rdir}....\")\n","    shutil.rmtree(rdir)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7e71bf5cf09d474e9836a692f518d4c2","1ed19b134a234973ae6dad5ca75313e9","28a6d0a437dc45378d499d350a1dea6a","b31000a22e074c73bc675bff9730ef9e","94b804b4daf5444b84f4ffe23a027ec5","decb57633893453cb6739f0ea384ba9f","4517bbb5fbf4441c9ade373b9e9da899","ac69f7a25fcd4583b93eca1fe4a46e10","480b117d39bc443b843e5a1aadf6ac83","dfa7fe6d930d4a55b88468e5582dfd18","9218217026af4fa8b7ffaf93b4cc3085","77b355d3b5134f229a187606f2fd8ecc","b52479baf50442889bf177f4c1197f4f","5351c0efcb4a4688a9d5c3fa5df944d6","6fb5a4252d7e4063b464c73704548262","551a13f70fd44365b92cbe4b8ef6cfc0","7b08f40110f9430f9de5bcc0aebf5b84","bf20a796a2e34e329472e519aba7cfe3","96b376e622ef42ba95426fc8310f2fc4","2ec3061465fb4716916b08a900c43a96","c066e176bfcd4ee6a724d328270be69d","cb1e0868ff064802b7b07a7ea5ba99e6","f0b88e7e6b9a4172bfaaa61a02c140f4","bf88725ff8264c7fb5535e6bc3846c6f","3f313d62c13d4e3289949d0357ee0c47","6cfd43bb17ec4cff9658293fe67beada","ecfc76ed7ded4d7f996702bac02af37a","06a15119b78244b288c7c84ed69288b0","c09f2b83e3d7415586dcc4555329d332","a865c0f2d51c4c4b88679b27432df5b5","39be3817181a49fd8bf686a63d33e8c2","170e7c1162e643c3a1399f33520ea586","ecc23be158244fc893a7597ea6f660e7","8f2ff019e15f4dccb4fb14d03c9a48d8","bc688151c18d4929ae6002003e01fa28","bb58692154a5421897aaa5b1efe4750c","7f6629575bb549f882055a9873bdf2d3","fa4bf27ef6b44d93aa0bea897a8333b7","1f4a873b38ac4b55a883a063dad5cee0","314867ec7deb4d7dacfbc8f7f572ec7a","ac4b31fb561b4030af638765e8e5dfc8","f7a9bd13a37f4f74a8f457472c76417b","73c9fde8532144d08c5c2c58357403c0","dcaac50f620c4d408c98a05fc70ea669","c916bd6e5f4a4a018745cd9fd89beb80","75b2f8fc1e5042a4910a0a0f0dc0a054","d2bc8eaba2be41638eb4885d26f587f1","4807876841e0491986fa8340b5bfe534","c4f055f29b604c42a16f3e3c69dd651d","ab61550561724ffc90b7aa93b93f36d5","1c71c562a6034c1e8f53438da61efefa","af4986724938470984a0491084afa9a4","81a96af4445142aab84a28f6f659e2e0","ac79644bdd434980900d74b31559aab8","0f17b8a432e74b009e6e48598441a2aa","e95d6c76a89444089d2f23c2e699cd4d","9c75bd90c33b4fe3a9eefa62cde622b6","d9d1bcaceecc455ea90487e5b5936af6","572850b6f55748dc976362ae5efb750d","06c11dc7e2b445928ef2361b63659e10","a05bf836439e444c82360c49a785457d","1aa2f942d67f459eaf0bc11c7c5f4f43","2e22485b3aa541c0a9808fb9eac76cad"]},"id":"S1NFG-j51Juc","executionInfo":{"status":"ok","timestamp":1707048126173,"user_tz":-60,"elapsed":271371,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"7b5ca247-01ea-453f-f0c3-79c2cfdc306b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO]: Initializing wandb\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 0xbqj6b9.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cuda support: True,devices exist : 1\n","[INFO]: Finetuning the Oneformer model with swin backbone trained on cityscapes dataset on device 0.\n","[INFO]: Extracting Railsem19 dataset from hub on device 0\n","[INFO]: Found labels_info.json.Skipping Download...\n","[INFO]: id2label: {'0': 'road', '1': 'sidewalk', '2': 'construction', '3': 'tram-track', '4': 'fence', '5': 'pole', '6': 'traffic-light', '7': 'traffic-sign', '8': 'vegetation', '9': 'terrain', '10': 'sky', '11': 'human', '12': 'rail-track', '13': 'car', '14': 'truck', '15': 'trackbed', '16': 'on-rails', '17': 'rail-raised', '18': 'rail-embedded'}, color_palette for background:[0, 0, 0]\n","[INFO]: Setting the seed 1\n","[INFO]: Prepare the Model and the Processor...\n","[INFO]: Model is in training mode...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized: ['model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_projector.layers.1.0.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[INFO]: Run object: <wandb.sdk.wandb_run.Run object at 0x7c86f70e2260>\n","[INFO]: Started Training the model...\n","[INFO]: Preparing the Datasets...\n","[INFO]: No splits found.. Generating splits\n","[INFO]: Total images: 8500\n","[INFO]: Total Training images: 8\n","[INFO]: Total Validation images: 4\n","[INFO]: Setting the custom step\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e71bf5cf09d474e9836a692f518d4c2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","-------[GPU: 0] | Epoch: 0 | Batchsize: 2 | Steps: 4-------\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b355d3b5134f229a187606f2fd8ecc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Train Loss after 000002 examples:112.040\n","\n","train stats per batch\n","tensor([0.0013, 0.0000, 0.2276, 0.0000, 0.0000, 0.0023, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.2410, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor([0.6197, 0.0000, 0.8243, 0.0000, 0.0000, 0.0028, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.2412, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor(0.1046) tensor(0.1668)\n","Train Loss after 000004 examples:90.701\n","\n","train stats per batch\n","tensor([0.0088, 0.0000, 0.4360, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0890,\n","        0.0008, 0.4037, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor([0.9896, 0.0000, 0.8218, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2322,\n","        0.0013, 0.5239, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor(0.1732) tensor(0.2764)\n","Train Loss after 000006 examples:73.307\n","\n","train stats per batch\n","tensor([0.0027, 0.3499, 0.0969, 0.0000, 0.0338, 0.0014, 0.0000, 0.0000, 0.2475,\n","        0.0014, 0.4888, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor([0.5231, 0.5963, 0.2318, 0.0000, 0.0372, 0.0047, 0.0000, 0.0000, 0.2849,\n","        0.6758, 0.5880, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor(0.1997) tensor(0.2601)\n","Train Loss after 000008 examples:89.227\n","\n","train stats per batch\n","tensor([0.0020, 0.1193, 0.6608, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor([1.0000, 0.2103, 0.6654, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000, 0.0045, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor(0.2342) tensor(0.2419)\n"]},{"output_type":"stream","name":"stderr","text":["/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n","  iou = total_area_intersect / total_area_union\n","/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n","  acc = total_area_intersect / total_area_label\n"]},{"output_type":"stream","name":"stdout","text":["train running iou: [0.10464301705360413, 0.17320246994495392, 0.19972413778305054, 0.2341662347316742]\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b88e7e6b9a4172bfaaa61a02c140f4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Val Loss : 84.464\n","\n","Val Loss : 85.005\n","\n","[INFO]: Logging Validation Images, on device: 0\n","[0.4565805196762085, 0.666791558265686]\n","\n","Epoch : 0 | Train Loss: 91.31876373291016 | Train mIoU: 0.05784268044884665 | Train mAcc: 0.1399579598620287 | Eval Loss: 84.73437118530273 | Eval mIoU: 0.16822505301377538 | Eval mAcc: 0.26652573174732\n","\n","Logging Train epoch metrics\n","Logging Val epoch metrics\n","\n","-------[GPU: 0] | Epoch: 1 | Batchsize: 2 | Steps: 4-------\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f2ff019e15f4dccb4fb14d03c9a48d8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Train Loss after 000002 examples:75.356\n","\n","train stats per batch\n","tensor([0.0035, 0.0000, 0.6700, 0.0000, 0.0199, 0.0000, 0.0000, 0.0000, 0.5128,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor([0.4028, 0.0000, 0.9545, 0.0000, 0.6263, 0.0000, 0.0000, 0.0000, 0.6633,\n","        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor(0.3153) tensor(0.4415)\n","Train Loss after 000004 examples:82.999\n","\n","train stats per batch\n","tensor([0.0063, 0.0000, 0.1341, 0.0000, 0.0188, 0.0010, 0.0000, 0.0000, 0.0085,\n","        0.0000, 0.5242, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor([1.0000, 0.0000, 0.1971, 0.0000, 0.0315, 0.0024, 0.0000, 0.0000, 0.0191,\n","        0.0000, 0.5317, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor(0.1906) tensor(0.2037)\n","Train Loss after 000006 examples:100.351\n","\n","train stats per batch\n","tensor([2.0561e-03, 0.0000e+00, 3.1377e-01, 0.0000e+00, 2.0725e-02, 1.6125e-04,\n","        0.0000e+00, 0.0000e+00, 1.7880e-01, 0.0000e+00, 1.5049e-01, 0.0000e+00,\n","        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6499e-03,\n","        0.0000e+00]) tensor([2.7582e-01, 0.0000e+00, 8.4791e-01, 0.0000e+00, 4.8929e-02, 1.9347e-04,\n","        0.0000e+00, 0.0000e+00, 2.1888e-01, 0.0000e+00, 1.6269e-01, 0.0000e+00,\n","        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7595e-03,\n","        0.0000e+00]) tensor(0.1364) tensor(0.2714)\n","Train Loss after 000008 examples:74.222\n","\n","train stats per batch\n","tensor([0.0011, 0.0000, 0.0000, 0.0000, 0.0000, 0.0161, 0.0000, 0.0000, 0.3570,\n","        0.0000, 0.7165, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor([0.9867, 0.0000, 0.0000, 0.0000, 0.0000, 0.0530, 0.0000, 0.0000, 0.3738,\n","        0.0000, 0.9342, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","        0.0000]) tensor(0.3186) tensor(0.3829)\n","train running iou: [0.3153480589389801, 0.19062405824661255, 0.1364264339208603, 0.318645715713501]\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c916bd6e5f4a4a018745cd9fd89beb80"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Val Loss : 85.804\n","\n","Val Loss : 85.189\n","\n","[INFO]: Logging Validation Images, on device: 0\n","[0.46043431758880615, 0.6631090044975281]\n","\n","Epoch : 1 | Train Loss: 83.23211097717285 | Train mIoU: 0.06192975512121375 | Train mAcc: 0.1437388201516635 | Eval Loss: 85.49643325805664 | Eval mIoU: 0.16447002032798042 | Eval mAcc: 0.26479161880938074\n","\n","Logging Train epoch metrics\n","Logging Val epoch metrics\n","Logging Charts\n","[INFO] Finishing the run....\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e95d6c76a89444089d2f23c2e699cd4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train_loss</td><td>█▇▅▁</td></tr><tr><td>Val_loss</td><td>▁▄█▅</td></tr><tr><td>custom_step</td><td>▁▂▃▄▄▄▄▄▅▇▇█████</td></tr><tr><td>epoch</td><td>▁▁██</td></tr><tr><td>train_OverallAcc</td><td>▁█</td></tr><tr><td>train_OverallAcc_t</td><td>▁█</td></tr><tr><td>train_epoch_loss</td><td>█▁</td></tr><tr><td>train_mAcc</td><td>▁█</td></tr><tr><td>train_mAcc_t</td><td>▁█</td></tr><tr><td>train_mIoU</td><td>▁█</td></tr><tr><td>train_mIoU_t</td><td>▁█</td></tr><tr><td>val_OverallAcc</td><td>▁█</td></tr><tr><td>val_OverallAcc_t</td><td>▁█</td></tr><tr><td>val_epoch_loss</td><td>▁█</td></tr><tr><td>val_mAcc</td><td>█▁</td></tr><tr><td>val_mAcc_t</td><td>█▁</td></tr><tr><td>val_mIoU</td><td>█▁</td></tr><tr><td>val_mIoU_t</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train_loss</td><td>74.22228</td></tr><tr><td>Val_loss</td><td>85.18881</td></tr><tr><td>custom_step</td><td>12</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>train_OverallAcc</td><td>0.32578</td></tr><tr><td>train_OverallAcc_t</td><td>0.32578</td></tr><tr><td>train_epoch_loss</td><td>83.23211</td></tr><tr><td>train_mAcc</td><td>0.14374</td></tr><tr><td>train_mAcc_t</td><td>0.12861</td></tr><tr><td>train_mIoU</td><td>0.06193</td></tr><tr><td>train_mIoU_t</td><td>0.23183</td></tr><tr><td>val_OverallAcc</td><td>0.62116</td></tr><tr><td>val_OverallAcc_t</td><td>0.62116</td></tr><tr><td>val_epoch_loss</td><td>85.49643</td></tr><tr><td>val_mAcc</td><td>0.26479</td></tr><tr><td>val_mAcc_t</td><td>0.26479</td></tr><tr><td>val_mIoU</td><td>0.16447</td></tr><tr><td>val_mIoU_t</td><td>0.56033</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /content/wandb/offline-run-20240204_115733-0xbqj6b9<code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/offline-run-20240204_115733-0xbqj6b9/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[INFO] Run Finished....\n","[INFO]: Syncing /content/wandb/offline-run-20240204_115733-0xbqj6b9\n","Removing /content/wandb/offline-run-20240204_115733-0xbqj6b9....\n"]}]},{"cell_type":"code","source":["model_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYkyyoPr3FgH","executionInfo":{"status":"ok","timestamp":1707048126173,"user_tz":-60,"elapsed":30,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d25e60be-92b1-4d37-c1e7-4d4ba2e280c7"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'train_loss': [91.31876373291016, 83.23211097717285],\n"," 'train_miou': [0.05784268044884665, 0.06192975512121375],\n"," 'train_perCatAcc': [array([8.21905080e-01, 3.70219742e-01, 6.53655685e-01,            nan,\n","         2.97820844e-02, 1.74697773e-03, 0.00000000e+00, 0.00000000e+00,\n","         1.08041038e-01, 4.06684909e-04, 3.93528025e-01, 0.00000000e+00,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","         0.00000000e+00, 0.00000000e+00,            nan]),\n","  array([0.70542981, 0.        , 0.79735935,        nan, 0.07359265,\n","         0.00703782, 0.        , 0.        , 0.35163228, 0.        ,\n","         0.50594178, 0.        , 0.        , 0.        , 0.        ,\n","         0.        , 0.        , 0.00256625,        nan])],\n"," 'train_perCatIoU': [array([3.04958937e-03, 2.19193129e-01, 3.68422619e-01, 0.00000000e+00,\n","         2.51784075e-02, 1.01483692e-03, 0.00000000e+00, 0.00000000e+00,\n","         8.11429437e-02, 3.28314455e-04, 3.42838407e-01, 0.00000000e+00,\n","         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n","         0.00000000e+00, 0.00000000e+00,            nan]),\n","  array([0.0034798 , 0.        , 0.42620802, 0.        , 0.01947608,\n","         0.00425921, 0.        , 0.        , 0.26705721, 0.        ,\n","         0.45364075, 0.        , 0.        , 0.        , 0.        ,\n","         0.        , 0.        , 0.00254427, 0.        ])],\n"," 'train_overallacc': [0.23719039058481217, 0.32578274242470023],\n"," 'train_mAcc': [0.1399579598620287, 0.1437388201516635],\n"," 'val_loss': [84.73437118530273, 85.49643325805664],\n"," 'val_miou': [0.16822505301377538, 0.16447002032798042],\n"," 'val_perCatAcc': [array([0.59368497, 0.04669223, 0.34237777, 0.        , 0.10991793,\n","         0.377713  , 0.03935185, 0.0387022 , 0.77106195, 0.66084887,\n","         0.97964325, 0.        , 0.        , 0.15766029, 0.        ,\n","         0.        , 0.94633459, 0.        , 0.        ]),\n","  array([0.61194195, 0.05160367, 0.34875893, 0.        , 0.06582547,\n","         0.37871111, 0.03877315, 0.0013905 , 0.78225386, 0.65812972,\n","         0.97989703, 0.        , 0.        , 0.1606543 , 0.        ,\n","         0.        , 0.95310106, 0.        , 0.        ])],\n"," 'val_perCatIoU': [array([0.03476293, 0.04565877, 0.17211713, 0.        , 0.10437506,\n","         0.31460183, 0.03592182, 0.00427307, 0.72493155, 0.56691945,\n","         0.94344438, 0.        , 0.        , 0.13222685, 0.        ,\n","         0.        , 0.11704316, 0.        , 0.        ]),\n","  array([3.73458457e-02, 5.06480967e-02, 1.36030581e-01, 0.00000000e+00,\n","         6.46112533e-02, 3.14514703e-01, 3.49504434e-02, 1.75070028e-04,\n","         7.34841678e-01, 5.58025917e-01, 9.45223439e-01, 0.00000000e+00,\n","         0.00000000e+00, 1.32874313e-01, 0.00000000e+00, 0.00000000e+00,\n","         1.15689047e-01, 0.00000000e+00, 0.00000000e+00])],\n"," 'val_overallacc': [0.6189463440258886, 0.6211584087131797],\n"," 'val_mAcc': [0.26652573174732, 0.26479161880938074],\n"," 'train_miou_t': [0.16573292016983032, 0.23182986676692963],\n"," 'train_perCatAcc_t': [tensor([8.2191e-01, 3.7022e-01, 6.5366e-01, 0.0000e+00, 2.9782e-02, 1.7470e-03,\n","          0.0000e+00, 0.0000e+00, 1.0804e-01, 4.0668e-04, 3.9353e-01, 0.0000e+00,\n","          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","          0.0000e+00]),\n","  tensor([0.7054, 0.0000, 0.7974, 0.0000, 0.0736, 0.0070, 0.0000, 0.0000, 0.3516,\n","          0.0000, 0.5059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0026,\n","          0.0000])],\n"," 'train_perCatIoU_t': [tensor([3.0496e-03, 2.1919e-01, 3.6842e-01, 0.0000e+00, 2.5178e-02, 1.0148e-03,\n","          0.0000e+00, 0.0000e+00, 8.1143e-02, 3.2831e-04, 3.4284e-01, 0.0000e+00,\n","          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n","          0.0000e+00]),\n","  tensor([0.0035, 0.0000, 0.4262, 0.0000, 0.0195, 0.0043, 0.0000, 0.0000, 0.2671,\n","          0.0000, 0.4536, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0025,\n","          0.0000])],\n"," 'train_overallacc_t': [0.237190380692482, 0.32578274607658386],\n"," 'train_mAcc_t': [0.12522554397583008, 0.12860842049121857],\n"," 'jacktrain_iou': [0.1779339648783207, 0.24026106670498848],\n"," 'val_miou_t': [0.5609647631645203, 0.5603287220001221],\n"," 'val_perCatAcc_t': [tensor([0.5937, 0.0467, 0.3424, 0.0000, 0.1099, 0.3777, 0.0394, 0.0387, 0.7711,\n","          0.6608, 0.9796, 0.0000, 0.0000, 0.1577, 0.0000, 0.0000, 0.9463, 0.0000,\n","          0.0000]),\n","  tensor([0.6119, 0.0516, 0.3488, 0.0000, 0.0658, 0.3787, 0.0388, 0.0014, 0.7823,\n","          0.6581, 0.9799, 0.0000, 0.0000, 0.1607, 0.0000, 0.0000, 0.9531, 0.0000,\n","          0.0000])],\n"," 'val_perCatIoU_t': [tensor([0.0348, 0.0457, 0.1721, 0.0000, 0.1044, 0.3146, 0.0359, 0.0043, 0.7249,\n","          0.5669, 0.9434, 0.0000, 0.0000, 0.1322, 0.0000, 0.0000, 0.1170, 0.0000,\n","          0.0000]),\n","  tensor([3.7346e-02, 5.0648e-02, 1.3603e-01, 0.0000e+00, 6.4611e-02, 3.1451e-01,\n","          3.4950e-02, 1.7507e-04, 7.3484e-01, 5.5803e-01, 9.4522e-01, 0.0000e+00,\n","          0.0000e+00, 1.3287e-01, 0.0000e+00, 0.0000e+00, 1.1569e-01, 0.0000e+00,\n","          0.0000e+00])],\n"," 'val_overallacc_t': [0.6189463138580322, 0.6211584210395813],\n"," 'val_mAcc_t': [0.2665257155895233, 0.2647915780544281],\n"," 'jackval_iou': [0.5616860389709473, 0.5617716610431671]}"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["from torch import tensor\n","tensor_list = [tensor(0.1046), tensor(0.1732), tensor(0.1997), tensor(0.2342)]\n","tensor_list_2 = [0.1046, 0.1732, 0.1997, 0.2342]\n","manual_mean = statistics.mean([float(value) for value in tensor_list])\n","manual_mean, statistics.mean(tensor_list_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTdJ3gZw4U6C","executionInfo":{"status":"ok","timestamp":1707046300472,"user_tz":-60,"elapsed":31,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"bc029d96-4378-4164-f470-a3bf74fb170c"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.1779249981045723, 0.177925)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["import torch\n","torch.manual_seed(1)\n","# import our library\n","import torchmetrics\n","\n","# initialize metric\n","metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=5)\n","\n","# move the metric to device you want computations to take place\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","metric.to(device)\n","running_acc = []\n","n_batches = 10\n","for i in range(n_batches):\n","    # simulate a classification problem\n","    preds = torch.randn(10, 5).softmax(dim=-1).to(device)\n","    target = torch.randint(5, (10,)).to(device)\n","\n","    # metric on current batch\n","    acc = metric(preds, target)\n","\n","    running_acc.append(acc.item())\n","    print(f\"Accuracy on batch {i}: {acc}\")\n","\n","# metric on all batches using custom accumulation\n","acc = metric.compute()\n","print(f\"Accuracy on all data: {acc}\")\n","print(running_acc)\n","print(statistics.mean(running_acc)) #0.25000000447035"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e29XjWeCu62Q","executionInfo":{"status":"ok","timestamp":1707047705806,"user_tz":-60,"elapsed":321,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"b82e9aba-018e-420e-9c4e-3cace09ecfd7"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on batch 0: 0.4000000059604645\n","Accuracy on batch 1: 0.0\n","Accuracy on batch 2: 0.10000000149011612\n","Accuracy on batch 3: 0.4000000059604645\n","Accuracy on batch 4: 0.20000000298023224\n","Accuracy on batch 5: 0.30000001192092896\n","Accuracy on batch 6: 0.20000000298023224\n","Accuracy on batch 7: 0.5\n","Accuracy on batch 8: 0.10000000149011612\n","Accuracy on batch 9: 0.30000001192092896\n","Accuracy on all data: 0.25\n","[0.4000000059604645, 0.0, 0.10000000149011612, 0.4000000059604645, 0.20000000298023224, 0.30000001192092896, 0.20000000298023224, 0.5, 0.10000000149011612, 0.30000001192092896]\n","0.25000000447034837\n"]}]},{"cell_type":"code","source":["import wandb\n","import torch\n","\n","run = wandb.init(project=\"table-test\", mode=\"offline\")\n","torch.manual_seed(1)\n","\n","mytensor = [torch.randint(low=0, high=18, size=(19,)), torch.randint(low=0, high=18, size=(19,))]\n","mytensor_with_nan = [torch.where(tensor == 5, float('nan'), tensor) for tensor in mytensor]\n","mylabels = [\"a\"] * 19\n","nan_masks = [torch.isnan(tensor) for tensor in mytensor_with_nan]\n","mytensor_with_nan = [torch.where(nan_mask, torch.tensor(0.0), tensor) for tensor, nan_mask in zip(mytensor_with_nan, nan_masks)]\n","avg_per_cat_iou_array_t = torch.stack(mytensor_with_nan).nanmean(dim=0)\n","data = [[label, acc] for (label, acc) in zip(mylabels, avg_per_cat_iou_array_t)]\n","avg_acc_table_t = wandb.Table(data=data, columns=[\"Class_Name_t\", \"Average_Accuracy_t\"])\n","run.log({\"avg_per_category_acc_bar_chart_t\": wandb.plot.bar(avg_acc_table_t, \"Class_Name_t\", \"Average_Accuracy_t\", title=\"Average Per category accuracy torchmetrics\")})\n","\n","mytensor_with_nan, avg_per_cat_iou_array_t\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277,"referenced_widgets":["8e935d3eba194ee794733bf3d4365148","96f471ca881349ba95f745137a3b5b05","ae9c2b2167e24cf188867156c18b686f","9c628b0e3447482ba5f53963fe4fbef0","f308264465214dec89e156535d213a9f","d1825ba49004423f8b7236e10de203dc","04a216d14d884718ab1deb80167460be","447b647550bf40f5b0a8a3a1e8ade7f7"]},"id":"RjIInuEwjwUH","executionInfo":{"status":"ok","timestamp":1706976132815,"user_tz":-60,"elapsed":6595,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"7b458a2e-925c-4458-ed0b-5f1048c5405e"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:5msms1k2) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e935d3eba194ee794733bf3d4365148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /content/wandb/offline-run-20240203_160136-5msms1k2<code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/offline-run-20240203_160136-5msms1k2/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:5msms1k2). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["([tensor([13.,  0.,  0., 14.,  7.,  1., 11.,  0., 17.,  0.,  2.,  3., 10., 17.,\n","          13.,  0., 12., 15.,  2.]),\n","  tensor([16.,  6., 15.,  8., 16., 15.,  0., 16.,  8., 17.,  4., 14.,  2., 15.,\n","          15.,  7., 15., 17.,  6.])],\n"," tensor([14.5000,  3.0000,  7.5000, 11.0000, 11.5000,  8.0000,  5.5000,  8.0000,\n","         12.5000,  8.5000,  3.0000,  8.5000,  6.0000, 16.0000, 14.0000,  3.5000,\n","         13.5000, 16.0000,  4.0000]))"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Convert the list of arrays to a 2D NumPy array\n","per_cat_acc_array = np.array(model_results['train_perCatAcc'])\n","\n","# Replace NaN values with 0 cuz sometimes you have nan values for a single category\n","per_cat_acc_array[np.isnan(per_cat_acc_array)] = 0\n","\n","# Calculate the average across instances for each label, ignoring NaN values\n","avg_label_acc = np.nanmean(per_cat_acc_array, axis=0)\n","\n","# Print or use avg_label_acc as needed\n","print(\"Average Accuracy for Each Label:\", avg_label_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zCviQkExlMqT","executionInfo":{"status":"ok","timestamp":1706741071190,"user_tz":-60,"elapsed":412,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"426212df-947b-4eeb-8753-3d9a360ea494"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Accuracy for Each Label: [0.85262336 0.00118313 0.26033974 0.         0.09822552 0.0104603\n"," 0.         0.0794996  0.21571637 0.01176805 0.46409303 0.\n"," 0.         0.         0.         0.         0.         0.\n"," 0.        ]\n"]}]},{"cell_type":"code","source":["per_cat_acc_array.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOokNMolsGyo","executionInfo":{"status":"ok","timestamp":1706740989371,"user_tz":-60,"elapsed":497,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"62990903-bbe0-493d-96e1-e0d6edbe85df"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 19)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"GD0W4ZYNsG_V","executionInfo":{"status":"ok","timestamp":1706743016825,"user_tz":-60,"elapsed":4423,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d3e633d9-6fd9-47a3-a184-db2c97942865"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id nuxn9vj8.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"ObwDii1Ez5gF","executionInfo":{"status":"ok","timestamp":1706743024210,"user_tz":-60,"elapsed":746,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"32d166df-6836-42ae-9603-ab40652f5f0a"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The mode property of wandb.run is deprecated and will be removed in a future release.\n"]},{"output_type":"execute_result","data":{"text/plain":["'dryrun'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4prrrC8l0k8Z","executionInfo":{"status":"ok","timestamp":1706743203093,"user_tz":-60,"elapsed":836,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"bb888c1e-723b-4412-ac8e-0bd46d9fcf13"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"8Paa6cWhz8TC","executionInfo":{"status":"ok","timestamp":1706743042124,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"b45d46c8-0f76-46e8-f45c-4873e9cc79d6"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/wandb/offline-run-20240131_231650-nuxn9vj8/files'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"MMQsf2eJ0AwU","executionInfo":{"status":"ok","timestamp":1706743129389,"user_tz":-60,"elapsed":961,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"5e136aa4-595b-48fb-de27-8e609c846fad"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'OF_Finetuning_cityscapes_swin_railSem19_small_id2label'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":[],"metadata":{"id":"Ctdr2j6P0Vt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoProcessor\n","# Processor\n","model_ckpt = \"shi-labs/oneformer_cityscapes_swin_large\"\n","processor = AutoProcessor.from_pretrained(\n","    model_ckpt,\n","    ignore_index=255,\n","    do_reduce_labels=False,\n","    do_normalize=False,\n","    do_rescale=False,\n","    do_resize=False\n",")\n","processor.image_processor.num_text = 234\n","\n","def _collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","\n","    segmentation_maps = inputs[1]\n","    batch = processor(images,\n","                            segmentation_maps=segmentation_maps,\n","                            task_inputs=[\"semantic\"] * len(images),\n","                            return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","def _prepare_dataloader(dataset: torch.utils.data.Dataset):\n","    sampler = None\n","    is_train=True\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=2,\n","        pin_memory=True,\n","        shuffle=(is_train and sampler is None),\n","        sampler=sampler,\n","        collate_fn=_collate_fn,\n","    )\n","    return dataloader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333,"referenced_widgets":["400a890f36ad4d1780126fd6f8d7c50c","d2c11647e8424589a463af80bf2e0aa3","6d892574051f4d9d8c08e9fa4b5e77c0","70a5789a501c46e087e85612b7430971","131dbceca4b74faa86cf3ab05da5a725","89f9f60d0cb64cb1b805de81408da53f","6432706f49034076bf11a8ab21569e03","5b72b27b221d4879aad48464b1aa486d","66860b4711a4471ba4f7771d0ca2f7b5","797f5a74924f4e19b906a4fa606c1ade","caa0113755094d7eb5f77722a82e095c","8b114c8aaca84299b99574eb11cd1cd4","a50a72dcf71049f49b14ca7184b45595","ebbe5ea6331b4f799e3c51948ab581f4","941cb5306034478eb87b2ba999968cfb","9a4d47cdda444fa29cab35bc0eb52bac","af1a3bd7c8f6403c87c28cadd0b0de9b","4b0418eb1b194455878e2c2bfcc7ed5f","3123da0890f94da4abf9cf49b0050c64","d03f2410a3684103bf5071d4c724f9b5","8d02621c99b342faa55fb3f88467dfbf","fe2f341aa903434d9c8a54d1e89ace6f","a0964733332e44d1867e8697cb701444","0b8f2ced088b4a27a7a3a45837a0880e","0eb17912e5d54bc895d8e24e309768ef","c6a2c906b9024658b4f925de80c51692","eb95faa69dec40c59ed1d47cb094900f","755ff5b87884442397d76589ad3f0019","c8c2c7ac260a4e4e8493ed8360714693","628eb6eb7aad4596af6d8c9d44a23500","9aea4bca0381409a9172bf95aa263997","05f67e8428dc43098236e84bc79fdc3c","049fa0f43c0f4f7e9e9bb7734eccb89b","d397056ea6a442f2a1bfbdc70c46e4b8","15f7c7565dfd44c496552251275af529","b322450bcadf45eebe67343318db9952","d5342d76fee848f29f5b37766c9b842b","ab96900d34554c2f9313db3ba2673bc4","1049215fd02f4b0ba1aa6e266850b705","e2c3f120a9594da1932f67927e423a26","84964c11cc6d4da4929ec495d5b677ff","9109a94bec234433875f7680234aec4e","214a74494cd14800a265677f6827c3b0","80dd98e6d9204c498475d6098ec41a08","cba63574d4af4f41a8436814e09420a2","77d3f543afef45949c1734100f434286","44cc1ce53d7a4328b376a3f3877e3695","e7604fd34ab2410bac8cec7240aaaa0d","540115fa49ea46cbb662bddecf729587","6854b11803894ea495620a151b1d1b06","04f56db1d88145bfad889016c1b623e8","b4bc3f1cd861460491a2a031db4052f0","d47a48cbd3484b009d198cf5fea44936","fecc216d075846d09664d2d22efc001f","0278f34a1abe45028d259cdbad24d8b4","0c6204fceef940c687f8b172fc874909","402eab69caca4e0486c46d6aa6f38596","a52b8c45a91042d19acd168104e66c3c","9e206e03efdd47548dc38b3052adfc69","b1363add63484791af11d21d42f646af","e575ef6db23f44c882ddfb74c6df4b39","b14d1c11130145f7be26036fd78d61f6","36b26a1dbbe846849ae5a70470ffeaeb","edad101a27384d17917cbc41b9cadec8","e8d2c548e1f443338cc6ce25d0d7de1d","6f7b8bd996194e1cbc29398290c81ead"]},"id":"eG3lvl29PbFQ","executionInfo":{"status":"ok","timestamp":1707221439319,"user_tz":-60,"elapsed":3444,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d66d3230-1fb9-41f6-98df-7a6e98a351bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"400a890f36ad4d1780126fd6f8d7c50c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["cityscapes_panoptic.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b114c8aaca84299b99574eb11cd1cd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/812 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0964733332e44d1867e8697cb701444"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d397056ea6a442f2a1bfbdc70c46e4b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba63574d4af4f41a8436814e09420a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c6204fceef940c687f8b172fc874909"}},"metadata":{}}]},{"cell_type":"code","source":["mydataset = load_railsem_dataset()\n","\n","total_size = len(mydataset[\"data\"])\n","print(f\"[INFO]: Total images: {total_size}\")\n","\n","train_size = 8160\n","train_percentage = train_size / total_size\n","\n","# Use train_test_split with specified percentages\n","splits = mydataset[\"data\"].train_test_split(\n","    train_size=train_percentage, shuffle=True\n",")\n","train_split = splits[\"train\"]\n","val_split = splits[\"test\"].select(indices=(range(320))) #320\n","test_split = splits[\"test\"].select(indices=(range(20)))\n","\n","print(f\"[INFO]: Total Training images: {len(train_split)}\")\n","print(f\"[INFO]: Total Validation images: {len(val_split)}\")\n","print(f\"[INFO]: Total Test images: {len(test_split)}\")\n","\n","train_transforms, val_transforms, test_transforms = get_transforms()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_hzAuXSE2Ct","executionInfo":{"status":"ok","timestamp":1707221962812,"user_tz":-60,"elapsed":1386,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"22f9f8c7-e38e-4e32-dc5a-9da8448c9caa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO]: Extracting Railsem19 dataset from hub on device None\n","[INFO]: Total images: 8500\n","[INFO]: Total Training images: 8160\n","[INFO]: Total Validation images: 320\n","[INFO]: Total Test images: 20\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","#customdataset = CustomDataset(val_split, transforms=val_transforms)\n","customdataset = CustomDataset(val_split)\n","valdataloader = _prepare_dataloader(customdataset)\n","for batch in tqdm(valdataloader):\n","    print(batch.keys())\n","    print(batch[\"pixel_values\"].shape, batch[\"pixel_values\"].shape, batch[\"text_inputs\"].shape, batch[\"task_inputs\"].shape, len(batch[\"mask_labels\"]), len(batch[\"class_labels\"]))\n","    print(batch[\"original_images\"][0].shape, batch[\"original_images\"][1].shape)\n","    print(batch[\"original_segmentation_maps\"][0].shape, batch[\"original_segmentation_maps\"][1].shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["35ddc5e6961e41e6b941d6ed83a265d1","18ca9f3e0a304119b71ef1e5cbdf37ee","71a86822cb1740c4b0e23e15340d7547","c951b3a6baa849bdae64e879e88d475d","91d19a63fbbe468aa38b930f3011e776","234cef568b954ae0bdc519c1880adc5d","8c47d8dd3b564a24815abfd66c90f09a","be9b8a5f4d3044a2bd1225501a6c290f","e79aadecb2de4340b96f2082a30d40f3","af88f801045d42598f4fdd0719f3c647","a6370750f593400fa513827bec7832e1"]},"id":"ZWOzlNMDV5BE","executionInfo":{"status":"ok","timestamp":1707222288322,"user_tz":-60,"elapsed":132419,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"78c1ca8c-8d03-4db1-9aa4-954aef1a07f4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/160 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35ddc5e6961e41e6b941d6ed83a265d1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","#print(np.array(mydataset[\"data\"][0][\"image\"]).shape, np.array(mydataset[\"data\"][0][\"semantic_mask_label\"]).shape)\n","#subset = mydataset[\"data\"].select(range(20))\n","\n","#customdataset = CustomDataset(train_split, transforms=train_transforms)\n","customdataset = CustomDataset(train_split)\n","traindataloader = _prepare_dataloader(customdataset)\n","for batch in tqdm(traindataloader):\n","    print(batch.keys())\n","    print(batch[\"pixel_values\"].shape, batch[\"pixel_values\"].shape, batch[\"text_inputs\"].shape, batch[\"task_inputs\"].shape, len(batch[\"mask_labels\"]), len(batch[\"class_labels\"]))\n","    print(batch[\"original_images\"][0].shape, batch[\"original_images\"][1].shape)\n","    print(batch[\"original_segmentation_maps\"][0].shape, batch[\"original_segmentation_maps\"][1].shape)\n","\n","#mybatch = next(iter(mydataloader))\n","\n","# print(mybatch[\"pixel_values\"].shape, mybatch[\"pixel_values\"].shape, mybatch[\"text_inputs\"].shape, mybatch[\"task_inputs\"].shape, len(mybatch[\"mask_labels\"]), len(mybatch[\"class_labels\"]))\n","# print(type(mybatch[\"original_images\"][0]))\n","# print(type( mybatch[\"original_segmentation_maps\"][0]))\n","# print(type(mybatch[\"original_images\"][0]))\n","# print(type( mybatch[\"original_segmentation_maps\"][0]))\n","# print(mybatch[\"original_images\"][0].shape, mybatch[\"original_images\"][1].shape)\n","# print(mybatch[\"original_segmentation_maps\"][0].shape, mybatch[\"original_segmentation_maps\"][1].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["362e97282f334f2babfc727cad72181b","b12fc392b8ea4ed7bd1c14eaa2441fad","e5988900baa7474b906cb6945e610b8f","ec35e933f8c54b9fa883848b320ae396","1a73f9e66c6e48f9a4bf857d636e8c71","b47a19bcecda4a849a2c1b15d6e7a582","9d274484e91548c6a3ce2a75f93d97f9","94a2546462f147b180a8b43dbe967ceb","d80595ee489941558a510546369c6df0","7837bea51a014be3bd6057f003f4f4f0","d1a77d55599541eb9d9980c543507762"]},"id":"Xbi61rj2TiU9","executionInfo":{"status":"ok","timestamp":1707225701480,"user_tz":-60,"elapsed":3171720,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"7ee78594-13d9-491a-b9d4-65b4d5a59c1a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4080 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"362e97282f334f2babfc727cad72181b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1080, 1920]) torch.Size([2, 3, 1080, 1920]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","print(np.array(mydataset[\"data\"][0][\"image\"]).shape, np.array(mydataset[\"data\"][0][\"semantic_mask_label\"]).shape)\n","train_transforms, val = get_transforms()\n","subset = mydataset[\"data\"].select(range(20))\n","customdataset = CustomDataset(subset, train_transforms)\n","example = customdataset[0]\n","mydataloader = _prepare_dataloader(customdataset)\n","mybatch = next(iter(mydataloader))\n","print(mybatch.keys())\n","#mybatch[\"text_inputs\"].shape, mybatch[\"task_inputs\"].shape, mybatch[\"original_images\"].shape, mybatch[\"original_segmentation_maps\"].shape\n","\n","print(mybatch[\"pixel_values\"].shape, mybatch[\"pixel_values\"].shape, mybatch[\"text_inputs\"].shape, mybatch[\"task_inputs\"].shape, len(mybatch[\"mask_labels\"]), len(mybatch[\"class_labels\"]))\n","\n","print(type(mybatch[\"original_images\"]))\n","print(type( mybatch[\"original_segmentation_maps\"]))\n","print(mybatch[\"original_images\"][0].shape, mybatch[\"original_images\"][1].shape)\n","print(mybatch[\"original_segmentation_maps\"][0].shape, mybatch[\"original_segmentation_maps\"][1].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okxdiT85FDm1","executionInfo":{"status":"ok","timestamp":1707170595198,"user_tz":-60,"elapsed":654,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d93d52fb-05c7-437f-b447-664d8cbcf79e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1080, 1920, 3) (1080, 1920)\n","(3, 512, 512) (512, 512) (1080, 1920, 3) (1080, 1920)\n","(3, 512, 512) (512, 512) (1080, 1920, 3) (1080, 1920)\n","(3, 512, 512) (512, 512) (1080, 1920, 3) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 512, 512]) torch.Size([2, 3, 512, 512]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","<class 'tuple'>\n","<class 'tuple'>\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n"]}]},{"cell_type":"code","source":["def get_transforms():\n","    # Transforms\n","    imagenet_mean = np.array([0.485, 0.456, 0.406])\n","    imagenet_std = np.array([0.229, 0.224, 0.225])\n","    train_transforms = A.Compose([\n","        A.LongestMaxSize(max_size=1333),\n","        A.CropNonEmptyMaskIfExists(width=512, height=512, ignore_values=[255]),\n","        # A.RandomCrop(width=512, height=512),\n","        A.HorizontalFlip(p=0.5),\n","        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n","    ])\n","    val_transforms = A.Compose([\n","        A.CropNonEmptyMaskIfExists(width=512, height=512, ignore_values=[255]),\n","        #A.Resize(width=512, height=512),\n","        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n","    ])\n","    test_transforms = A.Compose([\n","        A.Resize(width=1024, height=1024),\n","        A.Normalize(mean=imagenet_mean, std=imagenet_std),\n","    ])\n","    return train_transforms, val_transforms, test_transforms"],"metadata":{"id":"GuuDXP3xzfP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","customdataset = CustomDataset(test_split, transforms=test_transforms)\n","#customdataset = CustomDataset(test_split)\n","testdataloader = _prepare_dataloader(customdataset)\n","for batch in tqdm(testdataloader):\n","    print(batch.keys())\n","    print(batch[\"pixel_values\"].shape, batch[\"pixel_values\"].shape, batch[\"text_inputs\"].shape, batch[\"task_inputs\"].shape, len(batch[\"mask_labels\"]), len(batch[\"class_labels\"]))\n","    print(batch[\"original_images\"][0].shape, batch[\"original_images\"][1].shape)\n","    print(batch[\"original_segmentation_maps\"][0].shape, batch[\"original_segmentation_maps\"][1].shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":743,"referenced_widgets":["0ede393c82b14ed0b68273c0870f5f6b","6bd0033a42a44d08aa3c80d70718c14f","a143e26f073c4ce490b75bf78f2de688","e5643d030f3349c3a5acb18f6dac6d18","7c5cd4efb42945f6a97b261b68258f54","2c195cc21cda4f74b83729e1ad0893c8","2f7cb0b826cf4ff293410f3456678c8f","44654eb8d9b54730b30df4b071f6a5df","f7892f9814044c9784a56d9c6635d870","f6dae2a515644ccbaf9836837a39eea5","bff5c00d0add4a168f5ce48ece9c8f4f"]},"executionInfo":{"status":"ok","timestamp":1707228975809,"user_tz":-60,"elapsed":6195,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"fe5bcb8b-90f8-4780-b44f-94716430a06d","id":"cuBsFtq3zXo1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ede393c82b14ed0b68273c0870f5f6b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n","dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs', 'original_images', 'original_segmentation_maps'])\n","torch.Size([2, 3, 1024, 1024]) torch.Size([2, 3, 1024, 1024]) torch.Size([2, 234, 77]) torch.Size([2, 77]) 2 2\n","(1080, 1920, 3) (1080, 1920, 3)\n","(1080, 1920) (1080, 1920)\n"]}]}]}