{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONQE6TpEuDF6uygAYVO9p3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"road\": [128, 64, 128],\n","\"sidewalk\": [244, 35, 232],\n","\"construction\": [70, 70, 70],\n","\"tram-track\": [192, 0, 128],\n","\"fence\": [190, 153, 153],\n","\"pole\": [153, 153, 153],\n","\"traffic-light\": [250, 170, 30],\n","\"traffic-sign\": [220, 220, 0],\n","\"vegetation\": [107, 142, 35],\n","\"terrain\": [152, 251, 152],\n","\"sky\": [70, 130, 180],\n","\"human\": [220, 20, 60],\n","\"rail-track\": [230, 150, 140],\n","\"car\": [0, 0, 142],\n","\"truck\": [0, 0, 70],\n","\"trackbed\": [90, 40, 40],\n","\"on-rails\": [0, 80, 100],\n","\"rail-raised\": [0, 254, 254],\n","\"rail-embedded\": [0, 68, 63]]\n","\n","'road': [128, 64, 128],\n"," 'sidewalk': [244, 35, 232],\n"," 'construction_fence': [70, 70, 70]\n"," 'rail_raised_rail_embedded': [0, 254, 254]\n"," 'pole_traffic_light_traffic_sign': [153, 153, 153]\n"," 'sky': [70, 130, 180]\n"," 'human': [220, 20, 60]\n"," 'tram_track_rail_track': [230, 150, 140]\n"," 'car_truck': [0, 0, 142]\n"," 'on_rails': [0, 80, 100]\n"," 'vegetation': [107, 142, 35]\n"," 'trackbed': [90, 40, 40]\n"," 'background_terrain': [152, 251, 152]\n","\n","                                     [[128, 64, 128], [244, 35, 232], [70, 70, 70], [0, 254, 254], [153, 153, 153], [70, 130, 180],\n","                                      [220, 20, 60], [230, 150, 140], [0, 0, 142], [0, 80, 100], [107, 142, 35], [90, 40, 40], [152, 251, 152]]"],"metadata":{"id":"zURge_oQO3KQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7g0aCq8sgt1y","executionInfo":{"status":"ok","timestamp":1710244455843,"user_tz":-60,"elapsed":11198,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"2f8c7222-c252-4ff4-825c-734acbeabd30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}]},{"cell_type":"code","source":["def print_trainable_parameters(model, print_msg):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for name, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(f\"\\n-----{print_msg}......\")\n","    print(\n","        f\"trainable params: {trainable_params / 1e+6:.2f}M || all params: {all_param / 1e+6:.2f}M || trainable%: {100 * trainable_params / all_param:.2f}%\"\n","    )"],"metadata":{"id":"h4Ac0Aotg66D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n","\n","\n","# load Mask2Former fine-tuned on Cityscapes semantic segmentation\n","processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-cityscapes-semantic\") # tiny-384x384, small - 384x384, large-384x384\n","model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-tiny-cityscapes-semantic\", # tiny-47.41M, small-68.27M, large-215.46M\n","                                                            ignore_mismatched_sizes=True)\n","\n","print_trainable_parameters(model, print_msg=\"Base Model\")\n","\n","for name, params in model.named_parameters():\n","    if name.startswith(\"model.pixel_level_module\"):\n","        params.requires_grad = False\n","print_trainable_parameters(\n","    model, print_msg=\"Frozen Model Trainable Params\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":217,"referenced_widgets":["bd5a3efa31dc4edd91b90d6215e4f6c8","95454d3e39904665aad090e3238141de","f6b5de6f391a4eb88ecb778f53fbbe4c","c377c4a280264500872fd49cf097f4a1","2fb68cd4e0784b1ebec05145b1a0f12a","0cda5a951caa4fa18a562b696c6588c6","f802571ec2cd49feb65567fe0e944a26","6aeca6bc6c6e402ca54243a1880bf6e7","d8db549779d0477897a799d585e6a460","c699324ccfc34f998fce85c702c0697c","7b176391ceda479789294ec1c4b96a36","e1a17d2cdeb04d7e98ceb72b4f8a58da","049c4f38f49445e99ab845061eb01e98","ac93851aa01d413586bdc73457ba7672","c82b162a0c1b4a16833dbdbd0abb639f","dbef22749dfd4933878ee7e94feb51b0","b3659ba9ddf246fd99bdaa72a4afb0da","85693135d8304a21af6f2160d7394688","de462ea1e882448e939d8ee403be2100","46fe3e4acb8640e7882e7547fe8fda71","19ac12537ce54860a1d73831ad5f0241","e14f284006d142d8a3e82490568ec1f0","d09ca0e213c64eb19bad734ddc0eab73","e1546c66f3c04fd9a13ec8ae62dec049","298161d12b6b418891bd7cd91ef3fe26","bc34cdf154044eb7897530e07f28f4f0","c8dac9a87c7e4d0587af03b0e7031b22","5963e163998240a9ba0a28b341a19fd8","8e08d1d80e144ab2bb6220b79a673463","801c9fef61bf493980bc40a79cf1f911","00918cd485654b04a043079303bb51ce","858ea6525ba549259b393fe1d191ac25","6efbd92ee2834c1e8b4f0404f5b93df7"]},"id":"6ULFr49TgqQz","executionInfo":{"status":"ok","timestamp":1710244498715,"user_tz":-60,"elapsed":11712,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"660a5875-e469-4ffe-d496-1b06e66e2a80"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5a3efa31dc4edd91b90d6215e4f6c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/77.3k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1a17d2cdeb04d7e98ceb72b4f8a58da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/190M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d09ca0e213c64eb19bad734ddc0eab73"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","-----Base Model......\n","trainable params: 47.41M || all params: 47.41M || trainable%: 100.00%\n","\n","-----Frozen Model Trainable Params......\n","trainable params: 14.46M || all params: 47.41M || trainable%: 30.51%\n"]}]},{"cell_type":"code","source":["import torch\n","\n","def extract_patches(self, image_pair, samples_per_image, patch_size, stride):\n","    image_patches = []\n","    height, width = image_pair['image'].shape[-2], image_pair['image'].shape[-1]\n","    for i in range(samples_per_image):\n","        for y in range(0, height - patch_size[0] + 1, stride[0]):\n","            for x in range(0, width - patch_size[1] + 1, stride[1]):\n","                cropped_image = crop(image_pair['image'], y, x, patch_size[0], patch_size[1])\n","                cropped_labels = crop(image_pair['label'], y, x, patch_size[0], patch_size[1])\n","                image_patches.append({'image': cropped_image, 'label': cropped_labels})\n","    return image_patches\n","\n","def crop(image, top, left, height, width):\n","    return image[..., top:top+height, left:left+width]\n","\n","Example usage:\n","image_pair = {'image': torch.randn(1, 3, 1080, 1920), 'label': torch.randn(1, 1, 1080, 1920)}\n","samples_per_image = 10\n","patch_size = (256, 256)\n","stride = (128, 128)\n","patches = extract_patches(image_pair, samples_per_image, patch_size, stride)\n"],"metadata":{"id":"oyvHgoPfTB47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.transforms.functional import crop\n","import torch\n","\n","def extract_patches(image_pair, patch_size, stride):\n","    image_patches = []\n","    image_height, image_width = image_pair['image'].shape[-2], image_pair['image'].shape[-1]\n","\n","    # Calculate the number of patches along each dimension\n","    num_patches_height = (image_height - patch_size[0]) // stride + 1\n","    print(num_patches_height)\n","    num_patches_width = (image_width - patch_size[1]) // stride + 1\n","    print(num_patches_width)\n","    for i in range(num_patches_height):\n","        for j in range(num_patches_width):\n","            top = i * stride\n","            left = j * stride\n","            cropped_image = crop(image_pair['image'], top, left, patch_size[0], patch_size[1])\n","            cropped_labels = crop(image_pair['label'], top, left, patch_size[0], patch_size[1])\n","            image_patches.append({'image': cropped_image, 'label': cropped_labels})\n","    return image_patches\n","\n","image_pair = {'image': torch.randn(1, 3, 1080, 1920), 'label': torch.randn(1, 1, 1080, 1920)}\n","patch_size = (256, 256)\n","stride = 128\n","patches = extract_patches(image_pair, patch_size, stride)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KC5Z-KHAcSX","executionInfo":{"status":"ok","timestamp":1709733358717,"user_tz":-60,"elapsed":409,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"dc4f7af6-572f-448e-dbcf-88bc623dc8af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7\n","14\n"]}]},{"cell_type":"code","source":["len(patches), type(patches), type(patches[0]), patches[0].keys() #98"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHDZKu8KBFVr","executionInfo":{"status":"ok","timestamp":1709732939388,"user_tz":-60,"elapsed":461,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"5bc03ab3-5ae1-42b6-d002-95fe5c6a392c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(98, list, dict, dict_keys(['image', 'label']))"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["for i, patch in enumerate(patches):\n","    print(patch[\"image\"].shape, patch[\"label\"].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40mdIrY-BPvj","executionInfo":{"status":"ok","timestamp":1709733034654,"user_tz":-60,"elapsed":292,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"a1f37997-2275-46f2-9b2e-fac46c8a54eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n","torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n"]}]},{"cell_type":"code","source":["def extract_patches(image_pair, samples_per_image, patch_size):\n","    image_patches = []\n","    image_height, image_width = image_pair['image'].shape[-2], image_pair['image'].shape[-1]\n","\n","    # Calculate the step size to cover the image with 5 non-overlapping patches\n","    step_height = image_height // samples_per_image\n","    step_width = image_width // samples_per_image\n","    print(step_height)\n","    print(step_width)\n","    for i in range(samples_per_image):\n","        for j in range(samples_per_image):\n","            top = i * step_height\n","            left = j * step_width\n","            cropped_image = crop(image_pair['image'], top, left, patch_size[1], patch_size[0])\n","            cropped_labels = crop(image_pair['label'], top, left, patch_size[1], patch_size[0])\n","            image_patches.append({'image': cropped_image, 'label': cropped_labels})\n","\n","    return image_patches\n","\n","image_pair = {'image': torch.randn(1, 3, 1080, 1920), 'label': torch.randn(1, 1, 1080, 1920)}\n","patch_size = (512, 512)\n","patches = extract_patches(image_pair, samples_per_image=5, patch_size=patch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lDjgDC8NBxBu","executionInfo":{"status":"ok","timestamp":1709735363902,"user_tz":-60,"elapsed":275,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"a033aa1c-c3ba-42b7-c049-fb86fe92e492"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["216\n","384\n"]}]},{"cell_type":"code","source":["len(patches)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XPX4s-jLKnSx","executionInfo":{"status":"ok","timestamp":1709735366198,"user_tz":-60,"elapsed":316,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"5eb9bb24-37d7-4a3b-8061-3f2969c37553"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["for i, patch in enumerate(patches):\n","    print(patch[\"image\"].shape, patch[\"label\"].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kHWLKXGhKTdt","executionInfo":{"status":"ok","timestamp":1709735368327,"user_tz":-60,"elapsed":278,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"bf929747-f5a3-4b64-f107-46f9ce471aed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n","torch.Size([1, 3, 512, 512]) torch.Size([1, 1, 512, 512])\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","def extract_overlapping_patches(image, patch_size, stride):\n","    img_height, img_width = image.shape[:2]\n","    num_patches = 6  # You can adjust this as needed\n","\n","    patches = []\n","    for i in range(num_patches):\n","        top = i * stride\n","        for j in range(num_patches):\n","            left = j * stride\n","            patch = image[top : top + patch_size[0], left : left + patch_size[1]]\n","            patches.append(patch)\n","\n","    return patches\n","\n","# Example usage:\n","image = np.random.rand(1080, 1920)  # Replace with your actual image\n","patch_size = (512, 512)\n","stride = 512 #256\n","all_patches = extract_overlapping_patches(image, patch_size, stride)\n"],"metadata":{"id":"vesBrQctKjCM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(all_patches)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-8hsnKR7Pih","executionInfo":{"status":"ok","timestamp":1710167756875,"user_tz":-60,"elapsed":10,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"6720fe95-f3ed-417c-a967-33d53cbaf821"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["36"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["!pip install kornia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9oveT-ov-COO","executionInfo":{"status":"ok","timestamp":1710168236036,"user_tz":-60,"elapsed":8249,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"13474837-6d08-4d73-8a2d-3d48d36b7fbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kornia\n","  Downloading kornia-0.7.1-py2.py3-none-any.whl (756 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/756.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/756.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.2)\n","Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n","Installing collected packages: kornia\n","Successfully installed kornia-0.7.1\n"]}]},{"cell_type":"code","source":["import torch\n","# input = torch.arange(9.).view(1, 1, 3, 3)\n","from kornia.contrib import extract_tensor_patches\n","input = torch.randn(1, 3, 1080, 1920)\n","patches = extract_tensor_patches(input, (512, 512))"],"metadata":{"id":"HB6CpVwu9kXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["patches.shape"],"metadata":{"id":"d8FJJIcLL_iK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","class PatchQueue(Dataset):\n","    '''\n","    Extract random patches from an image -> put them in queue\n","    -> feed them to the dataloader\n","    ref : https://github.com/fepegar/torchio/blob/main/src/torchio/data/queue.py\n","    '''\n","    def __init__(self, dataset, max_length,  # Queue Length\n","                 samples_per_image,  # Patches per image\n","                 queue_workers=4,  # Num Workers\n","                 patch_size=[512, 512],  # Patch Size\n","                 shuffle=False):\n","        self.dataset = dataset\n","        self.max_length = max_length\n","        self.queue_workers = queue_workers\n","        self.samples_per_image = samples_per_image\n","        self.shuffle_queue = shuffle\n","        self.patch_size = patch_size\n","        self._images_iterable = None\n","        self.patch_list = list()\n","        self._num_sampled_images = 0\n","        self.steps_per_epoch = len(self.dataset) * self.samples_per_image\n","        self.resize_transform = tvt.Resize(patch_size[::-1])\n","\n","    def __len__(self):\n","        return self.steps_per_epoch\n","\n","    def __getitem__(self, item):\n","        if not self.patch_list:\n","            print(\"[WARN]: Patch List is empty...\")\n","            self._fill()\n","            self.patch_list.reverse()\n","        sample_patch = self.patch_list.pop()\n","        return sample_patch\n","\n","    @staticmethod\n","    def _get_first_item(batch):\n","        return batch[0]\n","\n","    def initialize_images_iterable(self):\n","        self._images_iterable = self._get_images_iterable()\n","\n","    @property\n","    def images_iterable(self):\n","        if self._images_iterable is None:\n","            self.initialize_images_iterable()\n","        return self.images_iterable\n","\n","    def _get_images_iterable(self):\n","        if torch.distributed.is_available() and torch.distributed.is_initialized():\n","            sampler = DistributedSampler(\n","                self.dataset,\n","                num_replicas=torch.distributed.get_world_size(),\n","                rank=torch.distributed.get_rank(), shuffle=self.shuffle_queue\n","            )\n","        else:\n","            sampler = None\n","        loader = DataLoader(\n","            self.dataset,  # num_workers=self.queue_workers, #in multigpu, >0 not wrkng\n","            batch_size=1, collate_fn=self._get_first_item, pin_memory=True,\n","            shuffle=self.shuffle_queue, sampler=sampler\n","        )\n","        self._num_sampled_images = 0\n","        return iter(loader)\n","\n","    def _get_next_image(self):\n","        try:\n","            image = next(self._images_iterable)\n","        except Exception as e:\n","            print(f\"[WARN]:Exception while getting image for patching: {e}\")\n","            self.initialize_images_iterable()\n","            image = next(self._images_iterable)\n","        return image\n","\n","    def extract_non_overlapping_patches_with_padding(self, image_pair, patch_size):\n","        image = image_pair['image']\n","        label = image_pair['mask']\n","        # Check if image size is smaller than patch size\n","        if orig_height < patch_size[0] or orig_width < patch_size[1]:\n","            # Resize the image to match the patch size\n","            image = self.resize_transform(image)\n","            label = self.resize_transform(label)\n","\n","        orig_height, orig_width = image.shape[-2], image.shape[-1]\n","        # Calculate padding needed to make dimensions divisible by patch_size\n","        pad_height = patch_size[0] - (orig_height % patch_size[0])\n","        pad_width = patch_size[1] - (orig_width % patch_size[1])\n","        # Pad the image and label\n","        padded_image = F.pad(image, (0, pad_width, 0, pad_height), mode=\"replicate\")\n","        padded_label = F.pad(label, (0, pad_width, 0, pad_height), mode=\"replicate\")\n","        num_vertical_patches = padded_image.shape[-2] // patch_size[0]\n","        num_horizontal_patches = padded_image.shape[-1] // patch_size[1]\n","\n","        image_patches = []\n","        for i in range(num_vertical_patches):\n","            for j in range(num_horizontal_patches):\n","                top = i * patch_size[0]\n","                left = j * patch_size[1]\n","                cropped_image = padded_image[:, top: top + patch_size[0], left: left + patch_size[1]]\n","                cropped_labels = padded_label[:, top: top + patch_size[0], left: left + patch_size[1]]\n","                cropped_img_name = image_pair['name']\n","                image_patches.append({'image': cropped_image, 'label': cropped_labels, 'name': cropped_img_name})\n","        return image_patches\n","\n","    def _fill(self):\n","        while True:\n","            image_pair = self._get_next_image()\n","            samples_per_image = self.samples_per_image\n","            patch_size = self.patch_size\n","            patches = self.extract_patches(\n","                image_pair, samples_per_image, patch_size\n","            )\n","            self.patch_list.extend(patches)\n","            self._num_sampled_images += 1\n","            islistfull = len(self.patch_list) >= self.max_length\n","            if islistfull:\n","                break"],"metadata":{"id":"_3G95tXXkUmf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["modified_id2label = {\n","        0: \"road\", 1: \"sidewalk\", 2: \"construction_fence\",\n","        3: \"rail_raised_rail_embedded\", 4: \"pole_traffic_light_traffic_sign\",\n","        5: \"sky\", 6: \"human\", 7: \"tram_track_rail_track\", 8: \"car_truck\",\n","        9: \"on_rails\", 10: \"vegetation\", 11: \"trackbed\",\n","        12: \"background_terrain\"\n","    }"],"metadata":{"id":"5PlCoOysZdNV"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","def extract_non_overlapping_patches_with_padding(image_pair, patch_size, padding, important_classes=[0, 1, 3, 6, 7, 8, 9, 11]):\n","        image = image_pair['image']\n","        label = image_pair['mask']\n","        # Check if image size is smaller than patch size\n","        # if orig_height < patch_size[0] or orig_width < patch_size[1]:\n","        #     # Resize the image to match the patch size\n","        #     image = self.resize_transform(image)\n","        #     label = self.resize_transform(label)\n","\n","        orig_height, orig_width = image.shape[-2], image.shape[-1]\n","        # Calculate padding needed to make dimensions divisible by patch_size\n","        if padding:\n","            pad_height = patch_size[0] - (orig_height % patch_size[0])\n","            pad_width = patch_size[1] - (orig_width % patch_size[1])\n","            print(pad_height, pad_width)\n","            # Pad the image and label\n","            image = F.pad(image, (0, pad_width, 0, pad_height), mode=\"constant\")\n","            label = F.pad(label, (0, pad_width, 0, pad_height), mode=\"constant\")\n","            print(image.shape, label.shape) #1080+456, 1920+128 torch.Size([1, 3, 1536, 2048]) torch.Size([1, 1, 1536, 2048])\n","\n","        num_vertical_patches = image.shape[-2] // patch_size[0]\n","        num_horizontal_patches = image.shape[-1] // patch_size[1]\n","\n","        image_patches = []\n","        for i in range(num_vertical_patches):\n","            for j in range(num_horizontal_patches):\n","                top = i * patch_size[0]\n","                left = j * patch_size[1]\n","                cropped_image = image[:, :, top: top + patch_size[0], left: left + patch_size[1]]\n","                cropped_labels = label[:, :, top: top + patch_size[0], left: left + patch_size[1]]\n","                cropped_img_name = image_pair['name']\n","                # Check if any of the unique classes are present in the list of important classes\n","                # if not any(class_label.item() in important_classes for class_label in torch.unique(cropped_labels)):\n","                #     continue\n","                image_patches.append({'image': cropped_image, 'mask': cropped_labels, 'name': cropped_img_name, 'top': top, 'left': left})\n","        return image_patches\n","\n","image_pair = {'image': torch.randn(1, 3, 1080, 1920), 'mask': torch.randint(0, 13, size=(1, 1, 1080, 1920)), 'name': '001'}\n","patch_size = [1024, 1024] #[512, 512]\n","patches = extract_non_overlapping_patches_with_padding(image_pair, patch_size=patch_size, padding=False)\n","len(patches), patches[0][\"mask\"].shape, patches[0][\"image\"].shape\n","#padding True\n","#456 128\n","# torch.Size([1, 3, 1536, 2048]) torch.Size([1, 1, 1536, 2048])\n","# (12, torch.Size([1, 1, 512, 512]), torch.Size([1, 3, 512, 512]))\n","#padding False\n","#(6, torch.Size([1, 1, 512, 512]), torch.Size([1, 3, 512, 512]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cn5EYUJrMHN2","executionInfo":{"status":"ok","timestamp":1710247571067,"user_tz":-60,"elapsed":258,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"9432cc44-b207-473f-bec6-4fa3a6e39237"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, torch.Size([1, 1, 1024, 1024]), torch.Size([1, 3, 1024, 1024]))"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["image_pair = {'image': torch.randn(1, 3, 1080, 1920), 'mask': torch.randn(1, 1, 1080, 1920), 'name': '001'}\n","patch_size = [512, 512]\n","patches = extract_non_overlapping_patches_with_padding(image_pair, patch_size=patch_size)\n","len(patches), patches[0][\"mask\"].shape, patches[0][\"image\"].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5YPuAjBpK7i","executionInfo":{"status":"ok","timestamp":1710186385273,"user_tz":-60,"elapsed":252,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"6dad0147-a216-452a-da9c-148d3b034490"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["456 128\n","torch.Size([1, 3, 1536, 2048]) torch.Size([1, 1, 1536, 2048])\n"]},{"output_type":"execute_result","data":{"text/plain":["(12, torch.Size([1, 1, 512, 512]), torch.Size([1, 3, 512, 512]))"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["image_pair = {'image': torch.randn(1, 3, 1080, 1920), 'mask': torch.randn(1, 1, 1080, 1920), 'name': '001'}\n","patch_size = [1024, 1024]\n","patches = extract_non_overlapping_patches_with_padding(image_pair, patch_size=patch_size)\n","len(patches)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0MkIFB0pLh2","executionInfo":{"status":"ok","timestamp":1710186390278,"user_tz":-60,"elapsed":262,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"6671cd56-23b8-47cc-bc86-e4d9e90c1812"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["968 128\n","torch.Size([1, 3, 2048, 2048]) torch.Size([1, 1, 2048, 2048])\n"]},{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["def reconstruct_original_from_patches(patches, orig_height, orig_width, patch_size):\n","    # Initialize an empty tensor to hold the reconstructed mask\n","    reconstructed_mask = torch.zeros((1, 1, orig_height, orig_width), dtype=torch.float32)\n","    # Get the padded height and widths\n","    pad_height = patch_size[0] - (orig_height % patch_size[0])\n","    pad_width = patch_size[1] - (orig_width % patch_size[1])\n","    # Loop over all patches and copy them into the appropriate locations in the reconstructed mask\n","    for patch in patches:\n","        cropped_mask = patch['mask']\n","        top = patch['top']\n","        left = patch['left']\n","        # Unpad the patch to remove the padding that was added during patch extraction\n","        cropped_mask = cropped_mask[:, :, :patch_size[0] - pad_height, :patch_size[1] - pad_width]\n","        reconstructed_mask[:, :, top: top + patch_size[0] - pad_height, left: left + patch_size[1] - pad_width] = cropped_mask\n","\n","    return reconstructed_mask\n","\n","image_pair = {'image': torch.randn(1, 3, 1080, 1920), 'mask': torch.randn(1, 1, 1080, 1920), 'name': '001'}\n","patch_size = [512, 512]\n","patches = extract_non_overlapping_patches_with_padding(image_pair, patch_size=patch_size)\n","\n","# Assuming you have the list of patches, original image dimensions, and patch size\n","reconstructed_mask = reconstruct_original_from_patches(patches, orig_height=1080, orig_width=1920, patch_size=[512, 512])\n","\n","reconstructed_mask.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDi1zxzm9q0U","executionInfo":{"status":"ok","timestamp":1710187033567,"user_tz":-60,"elapsed":299,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"37b4525d-99e6-4813-9624-8f714a8df889"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["456 128\n","torch.Size([1, 3, 1536, 2048]) torch.Size([1, 1, 1536, 2048])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1, 1080, 1920])"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["def reconstruct_mask_from_patches(patches, orig_height, orig_width, patch_size):\n","    # Initialize an empty tensor to hold the reconstructed mask\n","    reconstructed_mask = torch.zeros((1, 1, orig_height, orig_width), dtype=torch.float32)\n","    pad_height = patch_size[0] - (orig_height % patch_size[0])\n","    pad_width = patch_size[1] - (orig_width % patch_size[1])\n","    # Loop over all patches and copy them into the appropriate locations in the reconstructed mask\n","    for patch in patches:\n","        cropped_mask = patch['mask']\n","        top = patch['top']\n","        left = patch['left']\n","\n","        # Calculate the valid region within the patch (unpadding)\n","        valid_height = min(patch_size[0] - pad_height, cropped_mask.shape[-2])\n","        valid_width = min(patch_size[1] - pad_width, cropped_mask.shape[-1])\n","        # Copy the unpadded region of the patch to the corresponding location in the reconstructed mask\n","        reconstructed_mask[:, :, top: top + valid_height, left: left + valid_width] = cropped_mask[:, :, :valid_height, :valid_width]\n","\n","    return reconstructed_mask\n","\n","\n","image_pair = {'image': torch.randn(1, 3, 1080, 1920), 'mask': torch.randn(1, 1, 1080, 1920), 'name': '001'}\n","patch_size = [512, 512]\n","patches = extract_non_overlapping_patches_with_padding(image_pair, patch_size=patch_size)\n","\n","# Assuming you have the list of patches, original image dimensions, and patch size\n","reconstructed_mask = reconstruct_mask_from_patches(patches, orig_height=1080, orig_width=1920, patch_size=[512, 512])\n","reconstructed_mask.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nk37Ns8JGV-2","executionInfo":{"status":"ok","timestamp":1710187667831,"user_tz":-60,"elapsed":488,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"951137ec-358c-43eb-c277-401920762127"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["456 128\n","torch.Size([1, 3, 1536, 2048]) torch.Size([1, 1, 1536, 2048])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1, 1080, 1920])"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["import torch\n","\n","def extract_patches_without_padding(image, patch_size, stride):\n","    \"\"\"\n","    Extract overlapping patches from an image without padding.\n","\n","    Args:\n","        image (torch.Tensor): Input image tensor (C, H, W).\n","        patch_size (tuple): Size of the patches (height, width).\n","        stride (int): Stride for overlapping patches.\n","\n","    Returns:\n","        list: List of image patches.\n","    \"\"\"\n","    image_patches = {}\n","    height, width = image.shape[-2], image.shape[-1]\n","\n","    num_vertical_patches = (height - patch_size[0]) // stride + 1\n","    num_horizontal_patches = (width - patch_size[1]) // stride + 1\n","    print(num_vertical_patches)\n","    print(num_horizontal_patches)\n","\n","    image_patches = []\n","    for i in range(num_vertical_patches):\n","        for j in range(num_horizontal_patches):\n","            top = i * stride\n","            left = j * stride\n","            cropped_image = image[:, :, top: top + patch_size[0], left: left + patch_size[1]]\n","            image_patches.append(cropped_image)\n","\n","    return image_patches\n","\n","# Example usage:\n","image = torch.randn(1, 3, 1080, 1920)  # Replace with your actual image tensor\n","patch_size = (512, 512)\n","stride = 512  # Adjust as needed based on your requirements\n","patches = extract_patches_without_padding(image, patch_size, stride)\n","print(f\"Extracted {len(patches)} overlapping patches.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkj9DBlus-zt","executionInfo":{"status":"ok","timestamp":1710194332068,"user_tz":-60,"elapsed":7438,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"44a06bf4-f85a-47be-d7f7-d60170eaa3d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","3\n","Extracted 6 overlapping patches.\n"]}]},{"cell_type":"code","source":["import torch\n","from kornia.contrib import ExtractTensorPatches, extract_tensor_patches, combine_tensor_patches\n","\n","image = torch.randn(1, 3, 1080, 1920)\n","out = extract_tensor_patches(image, window_size=(512, 512), stride=(512, 512), padding=0) #, allow_auto_padding=False\n","#combine_tensor_patches(out, original_size=(4, 3), window_size=(3, 3), stride=(3, 3), unpadding=padding)\n"],"metadata":{"id":"Kv90-2tCtBfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EODB-2_3zdEv","executionInfo":{"status":"ok","timestamp":1710182490459,"user_tz":-60,"elapsed":253,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"936763ed-c500-4bd7-9c00-1a6d6bde7787"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 6, 3, 512, 512])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["reconstruct_img = combine_tensor_patches(out, original_size=(1080, 1920), window_size=(512, 512), stride=(512, 512))"],"metadata":{"id":"pTwhgtIzzeM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reconstruct_img.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AFVxPQdc0Vsz","executionInfo":{"status":"ok","timestamp":1710182459273,"user_tz":-60,"elapsed":363,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"004daecf-d419-44dd-9ac7-1832bff6025a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 1080, 1920])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"i-UNPXMp0WSx"},"execution_count":null,"outputs":[]}]}