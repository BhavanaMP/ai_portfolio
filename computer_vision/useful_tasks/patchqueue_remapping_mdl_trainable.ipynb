{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNwO6NtcZxlxkLfS+/wIbiw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c0d2b75c708345c28bb4c9c39b8f6ac8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8611d2ed8900462995c5df628cbce2b7","IPY_MODEL_acebbc469f27487996a54a55706bb378","IPY_MODEL_3e55208d89544917be4470f5441a19c0"],"layout":"IPY_MODEL_ac1461b3c1464e36ae1bc7238fe571d6"}},"8611d2ed8900462995c5df628cbce2b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_195da20b09d64d0f9c86c1506724ac9e","placeholder":"​","style":"IPY_MODEL_c5c8d6d8d8f144fd86ba3b4cd2b70c59","value":"preprocessor_config.json: 100%"}},"acebbc469f27487996a54a55706bb378":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af8fb4aee6034b28be87f1839f1ed1f8","max":1525,"min":0,"orientation":"horizontal","style":"IPY_MODEL_981a6e2da71448e0be21871a277675ed","value":1525}},"3e55208d89544917be4470f5441a19c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_800e264358a4440c94f5a171238e46fa","placeholder":"​","style":"IPY_MODEL_73cec770850f450aae1e769a0eba6945","value":" 1.52k/1.52k [00:00&lt;00:00, 33.7kB/s]"}},"ac1461b3c1464e36ae1bc7238fe571d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"195da20b09d64d0f9c86c1506724ac9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5c8d6d8d8f144fd86ba3b4cd2b70c59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af8fb4aee6034b28be87f1839f1ed1f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"981a6e2da71448e0be21871a277675ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"800e264358a4440c94f5a171238e46fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73cec770850f450aae1e769a0eba6945":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe481fc2d44247788faa227a496e4420":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1c3c9e836624f6499d3a3dbeff609ff","IPY_MODEL_d5bfb4d90a344b208df19eb305f01f5d","IPY_MODEL_4d542687a16c45d5875fd08aa983b75d"],"layout":"IPY_MODEL_fbd69f5933bd40eebd335581d5a331cc"}},"d1c3c9e836624f6499d3a3dbeff609ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b60775184b64af1a0bfb04cb5357cb1","placeholder":"​","style":"IPY_MODEL_c0e269c245344beab27a95f6df915e58","value":"cityscapes_panoptic.json: 100%"}},"d5bfb4d90a344b208df19eb305f01f5d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d9c4a86a1ae4dee8f4b6820a413ff2e","max":838,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17766a485b2745fc9d6b65c89bf3f085","value":838}},"4d542687a16c45d5875fd08aa983b75d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cb566c9cfd24e7883729a35fca1bf71","placeholder":"​","style":"IPY_MODEL_f3f80832a08045b98bc716f5d0ba7789","value":" 838/838 [00:00&lt;00:00, 21.9kB/s]"}},"fbd69f5933bd40eebd335581d5a331cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b60775184b64af1a0bfb04cb5357cb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0e269c245344beab27a95f6df915e58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d9c4a86a1ae4dee8f4b6820a413ff2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17766a485b2745fc9d6b65c89bf3f085":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cb566c9cfd24e7883729a35fca1bf71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3f80832a08045b98bc716f5d0ba7789":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02b0d85ed0a04385b1138d346e082de6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_615e164be27e424088cc28ee00fa61b1","IPY_MODEL_ea0d2ddec9044dbe9fbdb9762a7eb527","IPY_MODEL_0f0d6cabb31c45b59ba3e830a7134614"],"layout":"IPY_MODEL_6c043b48c06e49659ae58a2886fc6715"}},"615e164be27e424088cc28ee00fa61b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b271fd645703428f83f7feb1726562b7","placeholder":"​","style":"IPY_MODEL_38bfd39f1ed944599b1577fb8c2ced94","value":"tokenizer_config.json: 100%"}},"ea0d2ddec9044dbe9fbdb9762a7eb527":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_181a6aa15f594c7db961cdb23f0d4b28","max":812,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb9f6527675d43d4aa24003bd808cacd","value":812}},"0f0d6cabb31c45b59ba3e830a7134614":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e4bcd2d84bc47c89c0972693afabf81","placeholder":"​","style":"IPY_MODEL_3f910c7f86f64c8bac36ec0bd4efdb53","value":" 812/812 [00:00&lt;00:00, 26.6kB/s]"}},"6c043b48c06e49659ae58a2886fc6715":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b271fd645703428f83f7feb1726562b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38bfd39f1ed944599b1577fb8c2ced94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"181a6aa15f594c7db961cdb23f0d4b28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb9f6527675d43d4aa24003bd808cacd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e4bcd2d84bc47c89c0972693afabf81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f910c7f86f64c8bac36ec0bd4efdb53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a826b50ffc1a4ad79c50ac3593438f75":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf74f8bddc5b413eaf15397fd256dd83","IPY_MODEL_a3075457d1fa44c3bd45f46df8eadf41","IPY_MODEL_7448db50f3694a1b94b20af0e7681fff"],"layout":"IPY_MODEL_f5bbd93df6664cb18c1d84583041d963"}},"bf74f8bddc5b413eaf15397fd256dd83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6a83acd88534142a824118372abe011","placeholder":"​","style":"IPY_MODEL_65dadaa327664d54b354b8c4455f8d66","value":"vocab.json: 100%"}},"a3075457d1fa44c3bd45f46df8eadf41":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_075bbaa4e3f84c8ba3bba2612ab75dd5","max":1059962,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb18d363042748ec8c5de260de6f73cd","value":1059962}},"7448db50f3694a1b94b20af0e7681fff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4c1d448722448cc9669b97e79bf5ac9","placeholder":"​","style":"IPY_MODEL_053c75265f2d449aa39394781b095bf8","value":" 1.06M/1.06M [00:00&lt;00:00, 12.8MB/s]"}},"f5bbd93df6664cb18c1d84583041d963":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6a83acd88534142a824118372abe011":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65dadaa327664d54b354b8c4455f8d66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"075bbaa4e3f84c8ba3bba2612ab75dd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb18d363042748ec8c5de260de6f73cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4c1d448722448cc9669b97e79bf5ac9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"053c75265f2d449aa39394781b095bf8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2e9c8c63de74deeafc8ef3884ceea36":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38dfebcd24fe4a90918f60201925672a","IPY_MODEL_929eb6255d8d4991b3c7cbe4752e72a1","IPY_MODEL_6298960b8faa40729b33e9f0f5be0dee"],"layout":"IPY_MODEL_ef16c4696d0c4a069c1a32d43cf7e6d3"}},"38dfebcd24fe4a90918f60201925672a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e47162336054a4b82d678550aecc3d1","placeholder":"​","style":"IPY_MODEL_602ce654978c4386b23d8b537d8688fc","value":"merges.txt: 100%"}},"929eb6255d8d4991b3c7cbe4752e72a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec792921b9d04f55b74c6212c48f443b","max":524619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f59341670704ac7bf7681fc6e0cf6e1","value":524619}},"6298960b8faa40729b33e9f0f5be0dee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82fd5b58656c4606a68aee1be8c9fa22","placeholder":"​","style":"IPY_MODEL_85398c747bfb4541b3f8338c4a5820b8","value":" 525k/525k [00:00&lt;00:00, 7.45MB/s]"}},"ef16c4696d0c4a069c1a32d43cf7e6d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e47162336054a4b82d678550aecc3d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"602ce654978c4386b23d8b537d8688fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec792921b9d04f55b74c6212c48f443b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f59341670704ac7bf7681fc6e0cf6e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82fd5b58656c4606a68aee1be8c9fa22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85398c747bfb4541b3f8338c4a5820b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6298faad238149dd91afdfced96767dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4c60ebf3019498d9c33a237f4b64c54","IPY_MODEL_fc6c16653c1f436697f6a418ad4ecff9","IPY_MODEL_2adc93874c1945ddac2a2c76487245c7"],"layout":"IPY_MODEL_31f22aec82d849508238cc8e0e91dcac"}},"f4c60ebf3019498d9c33a237f4b64c54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa151924141b41b985075988ba8eb54c","placeholder":"​","style":"IPY_MODEL_242386d1b86a42ca9b431b584b14ccd1","value":"special_tokens_map.json: 100%"}},"fc6c16653c1f436697f6a418ad4ecff9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b16a7e130e19448d824aee77ad620c07","max":472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_989483bc71a24a3086e2a45847c36ff3","value":472}},"2adc93874c1945ddac2a2c76487245c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e383ecfd8c6840faa924b8229a6ef488","placeholder":"​","style":"IPY_MODEL_2ab40b564ed848f8aede3983f1243c5a","value":" 472/472 [00:00&lt;00:00, 14.5kB/s]"}},"31f22aec82d849508238cc8e0e91dcac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa151924141b41b985075988ba8eb54c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"242386d1b86a42ca9b431b584b14ccd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b16a7e130e19448d824aee77ad620c07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"989483bc71a24a3086e2a45847c36ff3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e383ecfd8c6840faa924b8229a6ef488":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ab40b564ed848f8aede3983f1243c5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8381d4b656949288a5738bc6edd9ee2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aadda932fb1e44319e05c1f25ef31b98","IPY_MODEL_b9e63e2f8600406385dfebcfe707352a","IPY_MODEL_89cca8dd604049339a12ed65aa4367a2"],"layout":"IPY_MODEL_bf0d0c209666490080129e6c0bf95a72"}},"aadda932fb1e44319e05c1f25ef31b98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38246ac20c3c4f7f973630453a5e8f25","placeholder":"​","style":"IPY_MODEL_f04538698e5547aa8c294b59cee741c6","value":"config.json: 100%"}},"b9e63e2f8600406385dfebcfe707352a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c537a11b9b6433783df72c170952917","max":77336,"min":0,"orientation":"horizontal","style":"IPY_MODEL_125a9695d3b84ed9b30f13fb92460d99","value":77336}},"89cca8dd604049339a12ed65aa4367a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_88e9de4499304a3390d030e1d3233518","placeholder":"​","style":"IPY_MODEL_0d947b40269343b2848b845a3f4741aa","value":" 77.3k/77.3k [00:00&lt;00:00, 3.31MB/s]"}},"bf0d0c209666490080129e6c0bf95a72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38246ac20c3c4f7f973630453a5e8f25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f04538698e5547aa8c294b59cee741c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c537a11b9b6433783df72c170952917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"125a9695d3b84ed9b30f13fb92460d99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88e9de4499304a3390d030e1d3233518":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d947b40269343b2848b845a3f4741aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25119b1d0470441bafc452b4bfa4d296":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27bc0b941e534ef8a7e45e02d9a0d8f7","IPY_MODEL_c4a90c7fc8ab4bcdbfb62af1267e3450","IPY_MODEL_89b7229b67e1447c964070314a991ca4"],"layout":"IPY_MODEL_29a2ab516ae348cd8293c7900334984e"}},"27bc0b941e534ef8a7e45e02d9a0d8f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86fc0681716f420fb9fafcaa23c0f32e","placeholder":"​","style":"IPY_MODEL_f6f45ecc9e5846a28b741cc519bffa57","value":"model.safetensors: 100%"}},"c4a90c7fc8ab4bcdbfb62af1267e3450":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09f96bd1a0c444b090df490f7fa88afe","max":866121664,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d04dfcd4151f4a509e104b0e670b6db4","value":866121664}},"89b7229b67e1447c964070314a991ca4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a80b9c749c140b7a6878385bb8b470e","placeholder":"​","style":"IPY_MODEL_fc9d60fc88b447dea2c946ad9baf2d55","value":" 866M/866M [00:07&lt;00:00, 80.6MB/s]"}},"29a2ab516ae348cd8293c7900334984e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86fc0681716f420fb9fafcaa23c0f32e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6f45ecc9e5846a28b741cc519bffa57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09f96bd1a0c444b090df490f7fa88afe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d04dfcd4151f4a509e104b0e670b6db4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a80b9c749c140b7a6878385bb8b470e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc9d60fc88b447dea2c946ad9baf2d55":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db8e70155bad40edbd1a4cd8188be96f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d510a12815f4a888ff71a911e1cc368","IPY_MODEL_b2267415467a4cfe95d54fe2b5b6885f","IPY_MODEL_1e00d5f5d2604df8a7d20a97fab49c9b"],"layout":"IPY_MODEL_97f6409964974b57b94ced638c0772d1"}},"9d510a12815f4a888ff71a911e1cc368":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8ba589739014807bbc966b85ad1a372","placeholder":"​","style":"IPY_MODEL_447d4744fd654ccc91e92471c2a667f8","value":"config.json: 100%"}},"b2267415467a4cfe95d54fe2b5b6885f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_559993fb929340a9a2cc6dec58572ccb","max":77742,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b486ca7ea9d478faac3d03728fe30f8","value":77742}},"1e00d5f5d2604df8a7d20a97fab49c9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fba3a4470cf4b01814a649718265269","placeholder":"​","style":"IPY_MODEL_369020b12a7840399065b6cf4db03456","value":" 77.7k/77.7k [00:00&lt;00:00, 3.48MB/s]"}},"97f6409964974b57b94ced638c0772d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8ba589739014807bbc966b85ad1a372":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447d4744fd654ccc91e92471c2a667f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"559993fb929340a9a2cc6dec58572ccb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b486ca7ea9d478faac3d03728fe30f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5fba3a4470cf4b01814a649718265269":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"369020b12a7840399065b6cf4db03456":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b7b31821d84487b869dc51cc04cdb1b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cba562c4a97648348bc995d5ed57867e","IPY_MODEL_6a244c873d304e9ca3148e7cdff5e9d2","IPY_MODEL_02ca73867f584a578ff2feb2b9a492e5"],"layout":"IPY_MODEL_794d3f9296ef4527b3c09f3e71a98901"}},"cba562c4a97648348bc995d5ed57867e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dccad9c121a34bb0bda1c109cb5e78f4","placeholder":"​","style":"IPY_MODEL_a420cbe8418742ed9cf2ffaa2b04910d","value":"pytorch_model.bin: 100%"}},"6a244c873d304e9ca3148e7cdff5e9d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce46314fb4994337a0879b2f84c84696","max":879382349,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90dbdbf6a50d409e969c3f8d49acb78a","value":879382349}},"02ca73867f584a578ff2feb2b9a492e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cd2f815ed9e47958f37c337a0544c05","placeholder":"​","style":"IPY_MODEL_82f7461844da45e0bef3ad2ad56945d7","value":" 879M/879M [00:10&lt;00:00, 75.3MB/s]"}},"794d3f9296ef4527b3c09f3e71a98901":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dccad9c121a34bb0bda1c109cb5e78f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a420cbe8418742ed9cf2ffaa2b04910d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce46314fb4994337a0879b2f84c84696":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90dbdbf6a50d409e969c3f8d49acb78a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8cd2f815ed9e47958f37c337a0544c05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82f7461844da45e0bef3ad2ad56945d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d9916ed016542fda4d21fa043acabdb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43f2bc0c746849ab9bb5642e413ae118","IPY_MODEL_508a8049bf8f4086b70cd4e024c76454","IPY_MODEL_1577d1adf09840a5aeb73b9bc667800e"],"layout":"IPY_MODEL_1fe233ef30724de8b15797fe3330fa5a"}},"43f2bc0c746849ab9bb5642e413ae118":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_144fceffa7c442ef9e1788d897259e8d","placeholder":"​","style":"IPY_MODEL_d012749f64dd49728feb23baa63ee88d","value":"100%"}},"508a8049bf8f4086b70cd4e024c76454":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e45abb612fca42b0a7e2c5c8f183f074","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6cb65dad5354a6f825916ca7f2ba3c0","value":2}},"1577d1adf09840a5aeb73b9bc667800e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_890ab7d812074f2097d5134cb0cc257b","placeholder":"​","style":"IPY_MODEL_3f807236e9c24d92b7cc8965311e1acd","value":" 2/2 [00:18&lt;00:00,  8.86s/it]"}},"1fe233ef30724de8b15797fe3330fa5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"144fceffa7c442ef9e1788d897259e8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d012749f64dd49728feb23baa63ee88d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e45abb612fca42b0a7e2c5c8f183f074":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6cb65dad5354a6f825916ca7f2ba3c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"890ab7d812074f2097d5134cb0cc257b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f807236e9c24d92b7cc8965311e1acd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11317e2f89c047c5afc77a9e47da63a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab714a51d20f4963a334b511a4302bb9","IPY_MODEL_da6375de2d2d4e78ada517da08db2c1e","IPY_MODEL_c488b0f845394020b19351490a4626f7"],"layout":"IPY_MODEL_4c9bd7ee8e074dcdb7b46018ab0ba881"}},"ab714a51d20f4963a334b511a4302bb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d13e51498259426889a9b4b836768c43","placeholder":"​","style":"IPY_MODEL_268c5e42ceca454589c42b4aa101f820","value":"100%"}},"da6375de2d2d4e78ada517da08db2c1e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b9b464647304e958d68cd886381c43e","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3804a916120040128407f6096e796e13","value":8}},"c488b0f845394020b19351490a4626f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff52f4781985498a9d965cbef3393f80","placeholder":"​","style":"IPY_MODEL_c4bf3257a3454b0dab5fb6d30b54132d","value":" 8/8 [00:10&lt;00:00,  1.15s/it]"}},"4c9bd7ee8e074dcdb7b46018ab0ba881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d13e51498259426889a9b4b836768c43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"268c5e42ceca454589c42b4aa101f820":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b9b464647304e958d68cd886381c43e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3804a916120040128407f6096e796e13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff52f4781985498a9d965cbef3393f80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4bf3257a3454b0dab5fb6d30b54132d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d3b721f01794c8e9ee5bd09888f92c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f54f382f3dd041a182272a4bac466060","IPY_MODEL_8174a56b0ce143feb7c8b0f3ba9cf2bc","IPY_MODEL_22121c6b2941450d92886f2aed745fe0"],"layout":"IPY_MODEL_7456bae620dc44ed9c4f6b62290d1abe"}},"f54f382f3dd041a182272a4bac466060":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_047e73bd5c8d4465a1dd65e37ace7306","placeholder":"​","style":"IPY_MODEL_09ab5df035844cc1b49ebd6da7a2017d","value":"100%"}},"8174a56b0ce143feb7c8b0f3ba9cf2bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71afcf57f8884835aaf4d0b15efeb67c","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f46ea018beef4463a0c7de32376daf84","value":8}},"22121c6b2941450d92886f2aed745fe0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e125fe796e14a919851f19932007589","placeholder":"​","style":"IPY_MODEL_9b42184410064c72a7a74b2f4808400c","value":" 8/8 [00:06&lt;00:00,  1.32it/s]"}},"7456bae620dc44ed9c4f6b62290d1abe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"047e73bd5c8d4465a1dd65e37ace7306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09ab5df035844cc1b49ebd6da7a2017d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71afcf57f8884835aaf4d0b15efeb67c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f46ea018beef4463a0c7de32376daf84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5e125fe796e14a919851f19932007589":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b42184410064c72a7a74b2f4808400c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fad0d0168b16406ebd88d3c1f9926b22":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_736effe10373488196e48007ec4df33e","IPY_MODEL_3f3d61dcf8374639985b8ffc9ee1616d"],"layout":"IPY_MODEL_d46db6769cfd4ec1ba7419ef2cd14302"}},"736effe10373488196e48007ec4df33e":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be86ff207337453d801991b0f179f06b","placeholder":"​","style":"IPY_MODEL_1ebfdd4e288944cf9262df90984166f6","value":"0.000 MB of 0.000 MB uploaded\r"}},"3f3d61dcf8374639985b8ffc9ee1616d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e68f03011fa44a49a509d9531eab95a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50e9789d29174b01a7462de5c58bfedc","value":1}},"d46db6769cfd4ec1ba7419ef2cd14302":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be86ff207337453d801991b0f179f06b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ebfdd4e288944cf9262df90984166f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e68f03011fa44a49a509d9531eab95a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50e9789d29174b01a7462de5c58bfedc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D2yiKpk1U_aV","executionInfo":{"status":"ok","timestamp":1709023212646,"user_tz":-60,"elapsed":22188,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"ec7f91e7-573f-46a1-bb8e-f59b2ff4932d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n","Collecting datasets\n","  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchmetrics\n","  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Collecting ftfy\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.40.5-py2.py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Installing collected packages: torchinfo, smmap, setproctitle, sentry-sdk, lightning-utilities, ftfy, docker-pycreds, dill, responses, multiprocess, gitdb, torchmetrics, GitPython, wandb, datasets, evaluate\n","Successfully installed GitPython-3.1.42 datasets-2.17.1 dill-0.3.8 docker-pycreds-0.4.0 evaluate-0.4.1 ftfy-6.1.3 gitdb-4.0.11 lightning-utilities-0.10.1 multiprocess-0.70.16 responses-0.18.0 sentry-sdk-1.40.5 setproctitle-1.3.3 smmap-5.0.1 torchinfo-1.8.0 torchmetrics-1.3.1 wandb-0.16.3\n"]}],"source":["!pip install transformers datasets evaluate wandb torchmetrics torchinfo ftfy regex"]},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"aYL7yCoDtHJ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709023226152,"user_tz":-60,"elapsed":13583,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"de54d8c0-40b4-44d1-8396-592f2e0a74b6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-mlpqt0j7\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-mlpqt0j7\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu121)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=2ea3cf0b2e0064f0be681bc825a6a48dcff1490dd0491c9a4e39639253df16af\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-cnbodan0/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}]},{"cell_type":"code","source":["from huggingface_hub import login\n","from google.colab import userdata\n","import wandb\n","\n","login(token=userdata.get('HF_TOKEN'))\n","wandb.login(key=userdata.get('WANDB_API_KEY'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZapDRG5lVP5B","executionInfo":{"status":"ok","timestamp":1709023233219,"user_tz":-60,"elapsed":7115,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"3a78a131-045d-4e22-f41c-afde94b69029"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.cuda.amp import GradScaler, autocast\n","from transformers import AutoModelForUniversalSegmentation, AutoProcessor\n","from torchmetrics.classification import Accuracy, MulticlassAccuracy, MulticlassJaccardIndex, JaccardIndex\n","import albumentations as A\n","from tqdm.auto import tqdm\n","from huggingface_hub import notebook_login\n","from datasets import load_dataset\n","import wandb\n","import evaluate\n","import statistics\n","from copy import deepcopy\n","from huggingface_hub import hf_hub_download\n","import torchinfo\n","\n","import numpy as np\n","import pandas as pd\n","import random\n","import requests\n","import json\n","from pathlib import Path\n","import os\n","from typing import List, Dict, Tuple\n","import warnings\n","from PIL import Image as PILImage\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","#warnings.filterwarnings(\"ignore\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.empty_cache()\n","    torch.backends.cudnn.benchmark = True\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.enabled = True"],"metadata":{"id":"T2VfsGEoVLUW","executionInfo":{"status":"ok","timestamp":1709023250505,"user_tz":-60,"elapsed":17308,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model_ckpt = f\"shi-labs/oneformer_cityscapes_swin_large\"\n","test_processor = AutoProcessor.from_pretrained(model_ckpt)\n","test_processor.image_processor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c0d2b75c708345c28bb4c9c39b8f6ac8","8611d2ed8900462995c5df628cbce2b7","acebbc469f27487996a54a55706bb378","3e55208d89544917be4470f5441a19c0","ac1461b3c1464e36ae1bc7238fe571d6","195da20b09d64d0f9c86c1506724ac9e","c5c8d6d8d8f144fd86ba3b4cd2b70c59","af8fb4aee6034b28be87f1839f1ed1f8","981a6e2da71448e0be21871a277675ed","800e264358a4440c94f5a171238e46fa","73cec770850f450aae1e769a0eba6945","fe481fc2d44247788faa227a496e4420","d1c3c9e836624f6499d3a3dbeff609ff","d5bfb4d90a344b208df19eb305f01f5d","4d542687a16c45d5875fd08aa983b75d","fbd69f5933bd40eebd335581d5a331cc","2b60775184b64af1a0bfb04cb5357cb1","c0e269c245344beab27a95f6df915e58","0d9c4a86a1ae4dee8f4b6820a413ff2e","17766a485b2745fc9d6b65c89bf3f085","0cb566c9cfd24e7883729a35fca1bf71","f3f80832a08045b98bc716f5d0ba7789","02b0d85ed0a04385b1138d346e082de6","615e164be27e424088cc28ee00fa61b1","ea0d2ddec9044dbe9fbdb9762a7eb527","0f0d6cabb31c45b59ba3e830a7134614","6c043b48c06e49659ae58a2886fc6715","b271fd645703428f83f7feb1726562b7","38bfd39f1ed944599b1577fb8c2ced94","181a6aa15f594c7db961cdb23f0d4b28","eb9f6527675d43d4aa24003bd808cacd","9e4bcd2d84bc47c89c0972693afabf81","3f910c7f86f64c8bac36ec0bd4efdb53","a826b50ffc1a4ad79c50ac3593438f75","bf74f8bddc5b413eaf15397fd256dd83","a3075457d1fa44c3bd45f46df8eadf41","7448db50f3694a1b94b20af0e7681fff","f5bbd93df6664cb18c1d84583041d963","f6a83acd88534142a824118372abe011","65dadaa327664d54b354b8c4455f8d66","075bbaa4e3f84c8ba3bba2612ab75dd5","cb18d363042748ec8c5de260de6f73cd","e4c1d448722448cc9669b97e79bf5ac9","053c75265f2d449aa39394781b095bf8","f2e9c8c63de74deeafc8ef3884ceea36","38dfebcd24fe4a90918f60201925672a","929eb6255d8d4991b3c7cbe4752e72a1","6298960b8faa40729b33e9f0f5be0dee","ef16c4696d0c4a069c1a32d43cf7e6d3","6e47162336054a4b82d678550aecc3d1","602ce654978c4386b23d8b537d8688fc","ec792921b9d04f55b74c6212c48f443b","5f59341670704ac7bf7681fc6e0cf6e1","82fd5b58656c4606a68aee1be8c9fa22","85398c747bfb4541b3f8338c4a5820b8","6298faad238149dd91afdfced96767dc","f4c60ebf3019498d9c33a237f4b64c54","fc6c16653c1f436697f6a418ad4ecff9","2adc93874c1945ddac2a2c76487245c7","31f22aec82d849508238cc8e0e91dcac","aa151924141b41b985075988ba8eb54c","242386d1b86a42ca9b431b584b14ccd1","b16a7e130e19448d824aee77ad620c07","989483bc71a24a3086e2a45847c36ff3","e383ecfd8c6840faa924b8229a6ef488","2ab40b564ed848f8aede3983f1243c5a"]},"id":"UPZ70hE4qAWb","executionInfo":{"status":"ok","timestamp":1709023255131,"user_tz":-60,"elapsed":4629,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"3a69753c-3d72-47b2-f396-25df33735ede"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d2b75c708345c28bb4c9c39b8f6ac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["cityscapes_panoptic.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe481fc2d44247788faa227a496e4420"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/812 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b0d85ed0a04385b1138d346e082de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a826b50ffc1a4ad79c50ac3593438f75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e9c8c63de74deeafc8ef3884ceea36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6298faad238149dd91afdfced96767dc"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["OneFormerImageProcessor {\n","  \"_max_size\": 2048,\n","  \"class_info_file\": \"cityscapes_panoptic.json\",\n","  \"do_normalize\": true,\n","  \"do_reduce_labels\": false,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"ignore_index\": 255,\n","  \"image_mean\": [\n","    0.48500001430511475,\n","    0.4560000002384186,\n","    0.4059999883174896\n","  ],\n","  \"image_processor_type\": \"OneFormerImageProcessor\",\n","  \"image_std\": [\n","    0.2290000021457672,\n","    0.2239999920129776,\n","    0.22499999403953552\n","  ],\n","  \"metadata\": {\n","    \"0\": \"road\",\n","    \"1\": \"building\",\n","    \"10\": \"sky\",\n","    \"11\": \"person\",\n","    \"12\": \"rider\",\n","    \"13\": \"car\",\n","    \"14\": \"truck\",\n","    \"15\": \"bus\",\n","    \"16\": \"train\",\n","    \"17\": \"motorcycle\",\n","    \"18\": \"bicycle\",\n","    \"2\": \"sidewalk\",\n","    \"3\": \"wall\",\n","    \"4\": \"fence\",\n","    \"5\": \"pole\",\n","    \"6\": \"traffic light\",\n","    \"7\": \"traffic sign\",\n","    \"8\": \"vegetation\",\n","    \"9\": \"terrain\",\n","    \"class_names\": [\n","      \"road\",\n","      \"building\",\n","      \"sidewalk\",\n","      \"wall\",\n","      \"fence\",\n","      \"pole\",\n","      \"traffic light\",\n","      \"traffic sign\",\n","      \"vegetation\",\n","      \"terrain\",\n","      \"sky\",\n","      \"person\",\n","      \"rider\",\n","      \"car\",\n","      \"truck\",\n","      \"bus\",\n","      \"train\",\n","      \"motorcycle\",\n","      \"bicycle\"\n","    ],\n","    \"thing_ids\": [\n","      11,\n","      12,\n","      13,\n","      14,\n","      15,\n","      16,\n","      17,\n","      18\n","    ]\n","  },\n","  \"num_labels\": 19,\n","  \"num_text\": null,\n","  \"processor_class\": \"OneFormerProcessor\",\n","  \"repo_path\": \"shi-labs/oneformer_demo\",\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"longest_edge\": 2048,\n","    \"shortest_edge\": 1024\n","  }\n","}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["model_ckpt_mask2former = f\"facebook/mask2former-swin-large-cityscapes-panoptic\"\n","test_model_mask2former = AutoModelForUniversalSegmentation.from_pretrained(model_ckpt_mask2former, ignore_mismatched_sizes=True)\n","test_model_mask2former"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d8381d4b656949288a5738bc6edd9ee2","aadda932fb1e44319e05c1f25ef31b98","b9e63e2f8600406385dfebcfe707352a","89cca8dd604049339a12ed65aa4367a2","bf0d0c209666490080129e6c0bf95a72","38246ac20c3c4f7f973630453a5e8f25","f04538698e5547aa8c294b59cee741c6","7c537a11b9b6433783df72c170952917","125a9695d3b84ed9b30f13fb92460d99","88e9de4499304a3390d030e1d3233518","0d947b40269343b2848b845a3f4741aa","25119b1d0470441bafc452b4bfa4d296","27bc0b941e534ef8a7e45e02d9a0d8f7","c4a90c7fc8ab4bcdbfb62af1267e3450","89b7229b67e1447c964070314a991ca4","29a2ab516ae348cd8293c7900334984e","86fc0681716f420fb9fafcaa23c0f32e","f6f45ecc9e5846a28b741cc519bffa57","09f96bd1a0c444b090df490f7fa88afe","d04dfcd4151f4a509e104b0e670b6db4","3a80b9c749c140b7a6878385bb8b470e","fc9d60fc88b447dea2c946ad9baf2d55"]},"id":"6sfMPUrd_3tK","executionInfo":{"status":"ok","timestamp":1709023268642,"user_tz":-60,"elapsed":13551,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"86cb931b-21fb-4a1d-90ee-238329dff60e"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/77.3k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8381d4b656949288a5738bc6edd9ee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/866M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25119b1d0470441bafc452b4bfa4d296"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["Mask2FormerForUniversalSegmentation(\n","  (model): Mask2FormerModel(\n","    (pixel_level_module): Mask2FormerPixelLevelModule(\n","      (encoder): SwinBackbone(\n","        (embeddings): SwinEmbeddings(\n","          (patch_embeddings): SwinPatchEmbeddings(\n","            (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n","          )\n","          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (encoder): SwinEncoder(\n","          (layers): ModuleList(\n","            (0): SwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x SwinLayer(\n","                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (attention): SwinAttention(\n","                    (self): SwinSelfAttention(\n","                      (query): Linear(in_features=192, out_features=192, bias=True)\n","                      (key): Linear(in_features=192, out_features=192, bias=True)\n","                      (value): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): SwinSelfOutput(\n","                      (dense): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): SwinDropPath(p=0.3)\n","                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): SwinIntermediate(\n","                    (dense): Linear(in_features=192, out_features=768, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): SwinOutput(\n","                    (dense): Linear(in_features=768, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): SwinPatchMerging(\n","                (reduction): Linear(in_features=768, out_features=384, bias=False)\n","                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (1): SwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x SwinLayer(\n","                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (attention): SwinAttention(\n","                    (self): SwinSelfAttention(\n","                      (query): Linear(in_features=384, out_features=384, bias=True)\n","                      (key): Linear(in_features=384, out_features=384, bias=True)\n","                      (value): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): SwinSelfOutput(\n","                      (dense): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): SwinDropPath(p=0.3)\n","                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): SwinIntermediate(\n","                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): SwinOutput(\n","                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): SwinPatchMerging(\n","                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n","                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (2): SwinStage(\n","              (blocks): ModuleList(\n","                (0-17): 18 x SwinLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): SwinAttention(\n","                    (self): SwinSelfAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): SwinSelfOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): SwinDropPath(p=0.3)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): SwinIntermediate(\n","                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): SwinOutput(\n","                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): SwinPatchMerging(\n","                (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n","                (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (3): SwinStage(\n","              (blocks): ModuleList(\n","                (0-1): 2 x SwinLayer(\n","                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (attention): SwinAttention(\n","                    (self): SwinSelfAttention(\n","                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): SwinSelfOutput(\n","                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): SwinDropPath(p=0.3)\n","                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): SwinIntermediate(\n","                    (dense): Linear(in_features=1536, out_features=6144, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): SwinOutput(\n","                    (dense): Linear(in_features=6144, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","            )\n","          )\n","        )\n","        (hidden_states_norms): ModuleDict(\n","          (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (decoder): Mask2FormerPixelDecoder(\n","        (position_embedding): Mask2FormerSinePositionEmbedding()\n","        (input_projections): ModuleList(\n","          (0): Sequential(\n","            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","          (1): Sequential(\n","            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","          (2): Sequential(\n","            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","        )\n","        (encoder): Mask2FormerPixelDecoderEncoderOnly(\n","          (layers): ModuleList(\n","            (0-5): 6 x Mask2FormerPixelDecoderEncoderLayer(\n","              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","              )\n","              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","        )\n","        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (adapter_1): Sequential(\n","          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        )\n","        (layer_1): Sequential(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          (2): ReLU()\n","        )\n","      )\n","    )\n","    (transformer_module): Mask2FormerTransformerModule(\n","      (position_embedder): Mask2FormerSinePositionEmbedding()\n","      (queries_embedder): Embedding(200, 256)\n","      (queries_features): Embedding(200, 256)\n","      (decoder): Mask2FormerMaskedAttentionDecoder(\n","        (layers): ModuleList(\n","          (0-8): 9 x Mask2FormerMaskedAttentionDecoderLayer(\n","            (self_attn): Mask2FormerAttention(\n","              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","            )\n","            (activation_fn): ReLU()\n","            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (cross_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","            )\n","            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n","            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n","            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mask_predictor): Mask2FormerMaskPredictor(\n","          (mask_embedder): Mask2FormerMLPPredictionHead(\n","            (0): Mask2FormerPredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): ReLU()\n","            )\n","            (1): Mask2FormerPredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): ReLU()\n","            )\n","            (2): Mask2FormerPredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): Identity()\n","            )\n","          )\n","        )\n","      )\n","      (level_embed): Embedding(3, 256)\n","    )\n","  )\n","  (class_predictor): Linear(in_features=256, out_features=20, bias=True)\n","  (criterion): Mask2FormerLoss(\n","    (matcher): Mask2FormerHungarianMatcher()\n","  )\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["model_ckpt = f\"shi-labs/oneformer_cityscapes_swin_large\"\n","test_model = AutoModelForUniversalSegmentation.from_pretrained(\n","    model_ckpt, is_training=True, ignore_mismatched_sizes=True\n",")\n","\n","def print_trainable_parameters(model, print_msg):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for name, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(f\"\\n-----{print_msg}......\")\n","    print(\n","        f\"trainable params: {trainable_params / 1e+6:.2f}M || all params: {all_param / 1e+6:.2f}M || trainable%: {100 * trainable_params / all_param:.2f}%\"\n","    )\n","\n","print_trainable_parameters(test_model, print_msg=\"Base Model Trainable Params\")\n","# Freeze the model backbone params\n","for name, params in test_model.named_parameters():\n","    if name.startswith(\"model.pixel_level_module\"): #195.20M'\n","            mdl_keywords = [\"decoder.mask_projection\", \"decoder.adapter_1\", \"decoder.layer_1\"] #35.33 to 36.03M when added\n","            if not any(keyword in name for keyword in mdl_keywords):\n","                params.requires_grad = False\n","# Get the last layer of the decoder encoder layers and Set require_grad to True\n","last_layer_params = test_model.model.pixel_level_module.decoder.encoder.layers[-1].parameters() #36.77M\n","for param in last_layer_params:\n","    param.requires_grad = True\n","\n","print_trainable_parameters(test_model, print_msg=\"Frozen Model Trainable Params\")\n","\n","#test_model.model.pixel_level_module.decoder.encoder.layers[-1], test_model.model.pixel_level_module.decoder.mask_projection, test_model.model.pixel_level_module.decoder.adapter_1, test_model.model.pixel_level_module.decoder.layer_1,"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275,"referenced_widgets":["db8e70155bad40edbd1a4cd8188be96f","9d510a12815f4a888ff71a911e1cc368","b2267415467a4cfe95d54fe2b5b6885f","1e00d5f5d2604df8a7d20a97fab49c9b","97f6409964974b57b94ced638c0772d1","f8ba589739014807bbc966b85ad1a372","447d4744fd654ccc91e92471c2a667f8","559993fb929340a9a2cc6dec58572ccb","4b486ca7ea9d478faac3d03728fe30f8","5fba3a4470cf4b01814a649718265269","369020b12a7840399065b6cf4db03456","2b7b31821d84487b869dc51cc04cdb1b","cba562c4a97648348bc995d5ed57867e","6a244c873d304e9ca3148e7cdff5e9d2","02ca73867f584a578ff2feb2b9a492e5","794d3f9296ef4527b3c09f3e71a98901","dccad9c121a34bb0bda1c109cb5e78f4","a420cbe8418742ed9cf2ffaa2b04910d","ce46314fb4994337a0879b2f84c84696","90dbdbf6a50d409e969c3f8d49acb78a","8cd2f815ed9e47958f37c337a0544c05","82f7461844da45e0bef3ad2ad56945d7"]},"id":"vsfi0f3M2BlU","executionInfo":{"status":"ok","timestamp":1709023283380,"user_tz":-60,"elapsed":14793,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"138fec95-9100-4e79-8df0-b135c15ec270"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/77.7k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8e70155bad40edbd1a4cd8188be96f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/879M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7b31821d84487b869dc51cc04cdb1b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized: ['model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_projector.layers.1.0.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","-----Base Model Trainable Params......\n","trainable params: 236.32M || all params: 236.32M || trainable%: 100.00%\n","\n","-----Frozen Model Trainable Params......\n","trainable params: 36.77M || all params: 236.32M || trainable%: 15.56%\n"]}]},{"cell_type":"code","source":["params = []\n","for name, param in test_model.named_parameters():\n","        print(name, param.shape)\n","        params.append(param)\n","\n","len(params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRmiVSJl3mIZ","executionInfo":{"status":"ok","timestamp":1709023283745,"user_tz":-60,"elapsed":388,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"62f32507-2ef8-4de1-9919-f372beab2db7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.weight torch.Size([192, 3, 4, 4])\n","model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.bias torch.Size([192])\n","model.pixel_level_module.encoder.embeddings.norm.weight torch.Size([192])\n","model.pixel_level_module.encoder.embeddings.norm.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.weight torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_before.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.relative_position_bias_table torch.Size([529, 6])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.weight torch.Size([192, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.query.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.weight torch.Size([192, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.key.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.weight torch.Size([192, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.self.value.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.weight torch.Size([192, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.attention.output.dense.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.weight torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.layernorm_after.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.weight torch.Size([768, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.intermediate.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.weight torch.Size([192, 768])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.0.output.dense.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.weight torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_before.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.relative_position_bias_table torch.Size([529, 6])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.weight torch.Size([192, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.query.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.weight torch.Size([192, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.key.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.weight torch.Size([192, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.self.value.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.weight torch.Size([192, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.attention.output.dense.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.weight torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.layernorm_after.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.weight torch.Size([768, 192])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.intermediate.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.weight torch.Size([192, 768])\n","model.pixel_level_module.encoder.encoder.layers.0.blocks.1.output.dense.bias torch.Size([192])\n","model.pixel_level_module.encoder.encoder.layers.0.downsample.reduction.weight torch.Size([384, 768])\n","model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.0.downsample.norm.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.weight torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_before.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.relative_position_bias_table torch.Size([529, 12])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.weight torch.Size([384, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.query.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.weight torch.Size([384, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.key.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.weight torch.Size([384, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.self.value.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.weight torch.Size([384, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.attention.output.dense.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.weight torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.layernorm_after.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.weight torch.Size([1536, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.intermediate.dense.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.weight torch.Size([384, 1536])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.0.output.dense.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.weight torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_before.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.relative_position_bias_table torch.Size([529, 12])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.weight torch.Size([384, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.query.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.weight torch.Size([384, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.key.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.weight torch.Size([384, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.self.value.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.weight torch.Size([384, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.attention.output.dense.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.weight torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.layernorm_after.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.weight torch.Size([1536, 384])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.intermediate.dense.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.weight torch.Size([384, 1536])\n","model.pixel_level_module.encoder.encoder.layers.1.blocks.1.output.dense.bias torch.Size([384])\n","model.pixel_level_module.encoder.encoder.layers.1.downsample.reduction.weight torch.Size([768, 1536])\n","model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.weight torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.1.downsample.norm.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.0.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.1.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.2.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.3.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.4.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.5.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.6.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.7.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.8.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.9.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.10.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.11.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.12.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.13.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.14.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.15.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.16.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_before.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.relative_position_bias_table torch.Size([529, 24])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.query.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.key.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.self.value.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.weight torch.Size([768, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.attention.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.weight torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.layernorm_after.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.weight torch.Size([3072, 768])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.intermediate.dense.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.weight torch.Size([768, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.blocks.17.output.dense.bias torch.Size([768])\n","model.pixel_level_module.encoder.encoder.layers.2.downsample.reduction.weight torch.Size([1536, 3072])\n","model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.weight torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.2.downsample.norm.bias torch.Size([3072])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.weight torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_before.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.relative_position_bias_table torch.Size([529, 48])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.weight torch.Size([1536, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.query.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.weight torch.Size([1536, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.key.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.weight torch.Size([1536, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.self.value.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.weight torch.Size([1536, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.attention.output.dense.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.weight torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.layernorm_after.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.weight torch.Size([6144, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.intermediate.dense.bias torch.Size([6144])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.weight torch.Size([1536, 6144])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.0.output.dense.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.weight torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_before.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.relative_position_bias_table torch.Size([529, 48])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.weight torch.Size([1536, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.query.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.weight torch.Size([1536, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.key.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.weight torch.Size([1536, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.self.value.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.weight torch.Size([1536, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.attention.output.dense.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.weight torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.layernorm_after.bias torch.Size([1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.weight torch.Size([6144, 1536])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.intermediate.dense.bias torch.Size([6144])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.weight torch.Size([1536, 6144])\n","model.pixel_level_module.encoder.encoder.layers.3.blocks.1.output.dense.bias torch.Size([1536])\n","model.pixel_level_module.encoder.hidden_states_norms.stage1.weight torch.Size([192])\n","model.pixel_level_module.encoder.hidden_states_norms.stage1.bias torch.Size([192])\n","model.pixel_level_module.encoder.hidden_states_norms.stage2.weight torch.Size([384])\n","model.pixel_level_module.encoder.hidden_states_norms.stage2.bias torch.Size([384])\n","model.pixel_level_module.encoder.hidden_states_norms.stage3.weight torch.Size([768])\n","model.pixel_level_module.encoder.hidden_states_norms.stage3.bias torch.Size([768])\n","model.pixel_level_module.encoder.hidden_states_norms.stage4.weight torch.Size([1536])\n","model.pixel_level_module.encoder.hidden_states_norms.stage4.bias torch.Size([1536])\n","model.pixel_level_module.decoder.level_embed torch.Size([3, 256])\n","model.pixel_level_module.decoder.input_projections.0.0.weight torch.Size([256, 1536, 1, 1])\n","model.pixel_level_module.decoder.input_projections.0.0.bias torch.Size([256])\n","model.pixel_level_module.decoder.input_projections.0.1.weight torch.Size([256])\n","model.pixel_level_module.decoder.input_projections.0.1.bias torch.Size([256])\n","model.pixel_level_module.decoder.input_projections.1.0.weight torch.Size([256, 768, 1, 1])\n","model.pixel_level_module.decoder.input_projections.1.0.bias torch.Size([256])\n","model.pixel_level_module.decoder.input_projections.1.1.weight torch.Size([256])\n","model.pixel_level_module.decoder.input_projections.1.1.bias torch.Size([256])\n","model.pixel_level_module.decoder.input_projections.2.0.weight torch.Size([256, 384, 1, 1])\n","model.pixel_level_module.decoder.input_projections.2.0.bias torch.Size([256])\n","model.pixel_level_module.decoder.input_projections.2.1.weight torch.Size([256])\n","model.pixel_level_module.decoder.input_projections.2.1.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.weight torch.Size([192, 256])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets.bias torch.Size([192])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.weight torch.Size([96, 256])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights.bias torch.Size([96])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.0.fc1.weight torch.Size([1024, 256])\n","model.pixel_level_module.decoder.encoder.layers.0.fc1.bias torch.Size([1024])\n","model.pixel_level_module.decoder.encoder.layers.0.fc2.weight torch.Size([256, 1024])\n","model.pixel_level_module.decoder.encoder.layers.0.fc2.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.weight torch.Size([192, 256])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets.bias torch.Size([192])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.weight torch.Size([96, 256])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights.bias torch.Size([96])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.1.fc1.weight torch.Size([1024, 256])\n","model.pixel_level_module.decoder.encoder.layers.1.fc1.bias torch.Size([1024])\n","model.pixel_level_module.decoder.encoder.layers.1.fc2.weight torch.Size([256, 1024])\n","model.pixel_level_module.decoder.encoder.layers.1.fc2.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.weight torch.Size([192, 256])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets.bias torch.Size([192])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.weight torch.Size([96, 256])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights.bias torch.Size([96])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.2.fc1.weight torch.Size([1024, 256])\n","model.pixel_level_module.decoder.encoder.layers.2.fc1.bias torch.Size([1024])\n","model.pixel_level_module.decoder.encoder.layers.2.fc2.weight torch.Size([256, 1024])\n","model.pixel_level_module.decoder.encoder.layers.2.fc2.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.weight torch.Size([192, 256])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets.bias torch.Size([192])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.weight torch.Size([96, 256])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights.bias torch.Size([96])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.3.fc1.weight torch.Size([1024, 256])\n","model.pixel_level_module.decoder.encoder.layers.3.fc1.bias torch.Size([1024])\n","model.pixel_level_module.decoder.encoder.layers.3.fc2.weight torch.Size([256, 1024])\n","model.pixel_level_module.decoder.encoder.layers.3.fc2.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.weight torch.Size([192, 256])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets.bias torch.Size([192])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.weight torch.Size([96, 256])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights.bias torch.Size([96])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.4.fc1.weight torch.Size([1024, 256])\n","model.pixel_level_module.decoder.encoder.layers.4.fc1.bias torch.Size([1024])\n","model.pixel_level_module.decoder.encoder.layers.4.fc2.weight torch.Size([256, 1024])\n","model.pixel_level_module.decoder.encoder.layers.4.fc2.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.weight torch.Size([192, 256])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets.bias torch.Size([192])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.weight torch.Size([96, 256])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights.bias torch.Size([96])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.weight torch.Size([256, 256])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.5.fc1.weight torch.Size([1024, 256])\n","model.pixel_level_module.decoder.encoder.layers.5.fc1.bias torch.Size([1024])\n","model.pixel_level_module.decoder.encoder.layers.5.fc2.weight torch.Size([256, 1024])\n","model.pixel_level_module.decoder.encoder.layers.5.fc2.bias torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.weight torch.Size([256])\n","model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm.bias torch.Size([256])\n","model.pixel_level_module.decoder.mask_projection.weight torch.Size([256, 256, 1, 1])\n","model.pixel_level_module.decoder.mask_projection.bias torch.Size([256])\n","model.pixel_level_module.decoder.adapter_1.0.weight torch.Size([256, 192, 1, 1])\n","model.pixel_level_module.decoder.adapter_1.1.weight torch.Size([256])\n","model.pixel_level_module.decoder.adapter_1.1.bias torch.Size([256])\n","model.pixel_level_module.decoder.layer_1.0.weight torch.Size([256, 256, 3, 3])\n","model.pixel_level_module.decoder.layer_1.1.weight torch.Size([256])\n","model.pixel_level_module.decoder.layer_1.1.bias torch.Size([256])\n","model.transformer_module.queries_embedder.weight torch.Size([250, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.self_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.self_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm1.weight torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm1.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm2.weight torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm2.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm3.weight torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm3.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.self_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.self_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm1.weight torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm1.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm2.weight torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm2.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm3.weight torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm3.bias torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.norm.weight torch.Size([256])\n","model.transformer_module.decoder.query_transformer.decoder.norm.bias torch.Size([256])\n","model.transformer_module.decoder.decoder_norm.weight torch.Size([256])\n","model.transformer_module.decoder.decoder_norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.0.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.0.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.0.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.0.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.0.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.0.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.0.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.0.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.0.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.0.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.1.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.1.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.1.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.1.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.1.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.1.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.1.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.1.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.1.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.1.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.2.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.2.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.2.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.2.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.2.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.2.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.2.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.2.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.2.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.2.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.3.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.3.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.3.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.3.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.3.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.3.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.3.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.3.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.3.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.3.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.4.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.4.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.4.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.4.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.4.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.4.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.4.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.4.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.4.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.4.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.5.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.5.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.5.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.5.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.5.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.5.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.5.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.5.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.5.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.5.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.6.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.6.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.6.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.6.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.6.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.6.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.6.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.6.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.6.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.6.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.7.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.7.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.7.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.7.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.7.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.7.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.7.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.7.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.7.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.7.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.cross_attn.multihead_attn.in_proj_weight torch.Size([768, 256])\n","model.transformer_module.decoder.layers.8.cross_attn.multihead_attn.in_proj_bias torch.Size([768])\n","model.transformer_module.decoder.layers.8.cross_attn.multihead_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.8.cross_attn.multihead_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.cross_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.8.cross_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.k_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.k_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.v_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.v_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.q_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.q_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.out_proj.weight torch.Size([256, 256])\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.out_proj.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.self_attn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.8.self_attn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.ffn.linear1.weight torch.Size([2048, 256])\n","model.transformer_module.decoder.layers.8.ffn.linear1.bias torch.Size([2048])\n","model.transformer_module.decoder.layers.8.ffn.linear2.weight torch.Size([256, 2048])\n","model.transformer_module.decoder.layers.8.ffn.linear2.bias torch.Size([256])\n","model.transformer_module.decoder.layers.8.ffn.norm.weight torch.Size([256])\n","model.transformer_module.decoder.layers.8.ffn.norm.bias torch.Size([256])\n","model.transformer_module.decoder.query_input_projection.weight torch.Size([256, 256, 1, 1])\n","model.transformer_module.decoder.query_input_projection.bias torch.Size([256])\n","model.transformer_module.decoder.class_embed.weight torch.Size([20, 256])\n","model.transformer_module.decoder.class_embed.bias torch.Size([20])\n","model.transformer_module.decoder.mask_embed.layers.0.0.weight torch.Size([256, 256])\n","model.transformer_module.decoder.mask_embed.layers.0.0.bias torch.Size([256])\n","model.transformer_module.decoder.mask_embed.layers.1.0.weight torch.Size([256, 256])\n","model.transformer_module.decoder.mask_embed.layers.1.0.bias torch.Size([256])\n","model.transformer_module.decoder.mask_embed.layers.2.0.weight torch.Size([256, 256])\n","model.transformer_module.decoder.mask_embed.layers.2.0.bias torch.Size([256])\n","model.transformer_module.level_embed.weight torch.Size([3, 256])\n","model.task_encoder.task_mlp.layers.0.0.weight torch.Size([256, 77])\n","model.task_encoder.task_mlp.layers.0.0.bias torch.Size([256])\n","model.task_encoder.task_mlp.layers.1.0.weight torch.Size([256, 256])\n","model.task_encoder.task_mlp.layers.1.0.bias torch.Size([256])\n","model.text_mapper.text_encoder.positional_embedding torch.Size([77, 256])\n","model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.ln_final.weight torch.Size([256])\n","model.text_mapper.text_encoder.ln_final.bias torch.Size([256])\n","model.text_mapper.text_encoder.token_embedding.weight torch.Size([49408, 256])\n","model.text_mapper.text_projector.layers.0.0.weight torch.Size([256, 256])\n","model.text_mapper.text_projector.layers.0.0.bias torch.Size([256])\n","model.text_mapper.text_projector.layers.1.0.weight torch.Size([256, 256])\n","model.text_mapper.text_projector.layers.1.0.bias torch.Size([256])\n","model.text_mapper.prompt_ctx.weight torch.Size([16, 256])\n","criterion.logit_scale torch.Size([])\n"]},{"output_type":"execute_result","data":{"text/plain":["882"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["for name, param in test_model.named_parameters():\n","    if \"text_mapper\" in name:\n","        print(name, param.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjFMJn4P0ARW","executionInfo":{"status":"ok","timestamp":1709023284143,"user_tz":-60,"elapsed":434,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"43b26667-137d-46f8-a74d-2c8914b753cb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["model.text_mapper.text_encoder.positional_embedding torch.Size([77, 256])\n","model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight torch.Size([768, 256])\n","model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias torch.Size([768])\n","model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight torch.Size([256, 256])\n","model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight torch.Size([1024, 256])\n","model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias torch.Size([1024])\n","model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight torch.Size([256, 1024])\n","model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight torch.Size([256])\n","model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias torch.Size([256])\n","model.text_mapper.text_encoder.ln_final.weight torch.Size([256])\n","model.text_mapper.text_encoder.ln_final.bias torch.Size([256])\n","model.text_mapper.text_encoder.token_embedding.weight torch.Size([49408, 256])\n","model.text_mapper.text_projector.layers.0.0.weight torch.Size([256, 256])\n","model.text_mapper.text_projector.layers.0.0.bias torch.Size([256])\n","model.text_mapper.text_projector.layers.1.0.weight torch.Size([256, 256])\n","model.text_mapper.text_projector.layers.1.0.bias torch.Size([256])\n","model.text_mapper.prompt_ctx.weight torch.Size([16, 256])\n"]}]},{"cell_type":"code","source":["test_model.model.pixel_level_module.decoder.encoder.layers[-1], test_model.model.pixel_level_module.decoder.mask_projection, test_model.model.pixel_level_module.decoder.adapter_1, test_model.model.pixel_level_module.decoder.layer_1, test_model.model.pixel_level_module.decoder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1cvzKkDW5Xvk","executionInfo":{"status":"ok","timestamp":1709023284143,"user_tz":-60,"elapsed":26,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"f86b91a0-5880-4617-84d9-64c524bb0c33"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(OneFormerPixelDecoderEncoderLayer(\n","   (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","     (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","     (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","     (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","     (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","   )\n","   (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","   (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","   (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","   (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n"," ),\n"," Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1)),\n"," Sequential(\n","   (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","   (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n"," ),\n"," Sequential(\n","   (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","   (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","   (2): ReLU()\n"," ),\n"," OneFormerPixelDecoder(\n","   (position_embedding): OneFormerSinePositionEmbedding()\n","   (input_projections): ModuleList(\n","     (0): Sequential(\n","       (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n","       (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","     )\n","     (1): Sequential(\n","       (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","       (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","     )\n","     (2): Sequential(\n","       (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n","       (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","     )\n","   )\n","   (encoder): OneFormerPixelDecoderEncoderOnly(\n","     (layers): ModuleList(\n","       (0-5): 6 x OneFormerPixelDecoderEncoderLayer(\n","         (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","           (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","           (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","           (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","           (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","         )\n","         (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","         (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","         (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","         (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","       )\n","     )\n","   )\n","   (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","   (adapter_1): Sequential(\n","     (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","     (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","   )\n","   (layer_1): Sequential(\n","     (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","     (2): ReLU()\n","   )\n"," ))"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["def prompt_params(model):\n","    params = []\n","    for name, param in model.named_parameters():\n","        if \"text_mapper\" in name:\n","            params.append(param)\n","    return params\n","\n","textmapper_params = prompt_params(test_model)\n","prompt_group = {'params': textmapper_params, \"lr\": 1e-2}\n","print('num of params in prompt textmapper:', len(textmapper_params))\n","adam_parameters = [prompt_group]\n","\n","def pretrained_model_params(model):\n","    params = []\n","    for name, param in model.named_parameters():\n","        if \"text_mapper\" not in name:\n","            params.append(param)\n","    return params\n","\n","\n","pretrained_parameters = pretrained_model_params(test_model)\n","print('num of params in pretrained modules: ', len(pretrained_parameters))\n","base_group = {'params': pretrained_parameters, 'lr': 1e-4}\n","adam_parameters.append(base_group)\n","\n","# optimizer = torch.optim.AdamW(params=adam_parameters, lr=1e-3, weight_decay=0.1)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKdGtE-fz4AA","executionInfo":{"status":"ok","timestamp":1709023284144,"user_tz":-60,"elapsed":24,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"5b346135-4ed7-4918-9f32-9f6f4022ecfd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["num of params in prompt textmapper: 81\n","num of params in pretrained modules:  801\n"]}]},{"cell_type":"code","source":["from copy import deepcopy\n","\n","def get_labels():\n","    data_directory = Path(\".\")\n","    json_file_path = data_directory / \"labels_info.json\"\n","    if json_file_path.is_file():\n","        print(f\"[INFO]: Found {json_file_path}.Skipping Download...\")\n","    else:\n","        print(\"[INFO]: Downloading labels_info.json from hub\")\n","        json_file_path = hf_hub_download(\n","            repo_id=\"BhavanaMalla/railsem19-semantic-expanded\",\n","            filename=\"labels_info.json\",\n","            repo_type=\"dataset\",\n","            local_dir=data_directory\n","        )\n","    with open(json_file_path, \"r\") as f:\n","        labels_info = json.load(f)\n","    id2label = labels_info[\"id2label\"]\n","    label2id = labels_info[\"label2id\"]\n","    labels = labels_info[\"labels\"]\n","    color_palette = labels_info[\"color_palette\"]\n","\n","    #add the background label\n","    id2label[\"19\"] = \"background\"\n","    label2id[\"background\"] = 19\n","    labels.append(\"background\")\n","    color_palette.append([0, 0, 0])\n","\n","    # correcting the labels\n","    id2label = {int(key): value.replace('-', '_') for key, value in id2label.items()}\n","    label2id = {key.replace('-', '_'): value for key, value in label2id.items()}\n","    labels = {label.replace('-', '_') for label in labels}\n","    return {\"id2label\": id2label, \"label2id\": label2id, \"labels\": labels, \"color_palette\": color_palette}\n","\n","def load_railsem_dataset(device=None):\n","    print(f\"[INFO]: Extracting Railsem19 dataset from hub on device {device}\")\n","    railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-expanded\")\n","    return railsem_ds"],"metadata":{"id":"Z9QQJG5tJ2Ub","executionInfo":{"status":"ok","timestamp":1709023284144,"user_tz":-60,"elapsed":18,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import torchvision.transforms as tvt\n","\n","def get_transforms():\n","    # Transforms\n","    image_transforms = tvt.Compose([tvt.ToTensor(),]) #ToTensor scales between 0 and 1, if we use normalize, the values goes beyond 0 and 1, the range could be outside of [0, 1] and would ideally have a zero mean and unit variance.\n","    mask_transforms = tvt.Compose([tvt.PILToTensor(),\n","                                  RemapBackground(),\n","                                  RemapLabels()]) #Converts a PIL Image (H x W x C) to a Tensor of shape (C x H x W). doesnot scale hence using this for masks\n","\n","    return image_transforms, mask_transforms\n","\n","class RemapBackground():\n","    \"\"\" Remap background to label 19 \"\"\"\n","    def __call__(self, mask):\n","        return torch.where(mask > 18, 19, mask)\n","\n","class RemapLabels():\n","    def __init__(self):\n","        self.class_mapping = get_labels()[\"id2label\"]\n","        print(\"class_mapping\")\n","        print(self.class_mapping)\n","        self.class_coding = {v: k for k, v in self.class_mapping.items()}\n","        print(\"class_coding\")\n","        print(self.class_coding)\n","        # MODIFICATIONS TO ORIGINAL CLASSES\n","        self.modified_labels = {\n","            0:[\"road\"], 1:[\"sidewalk\"], 2:['construction', 'fence'], 3:['rail_raised','rail_embedded'], 4:['pole','traffic_light','traffic_sign'],\n","            5:['sky'], 6:['human'], 7:['tram_track', 'rail_track'], 8:['car','truck'], 9:['on_rails'], 10:['vegetation'], 11:['trackbed'], 12:['background','terrain']\n","        }\n","        self.modified_ids = {}\n","        for k, v in self.modified_labels.items():\n","            self.modified_ids[k] = [self.class_coding[label] for label in v]\n","        print(\"modified_ids\")\n","        print(self.modified_ids)\n","\n","    def __call__(self, mask):\n","        final_label = np.zeros_like(mask)\n","        for k, v in self.modified_ids.items():\n","            specific_label = np.zeros_like(mask)\n","            specific_label = np.where(np.isin(mask, np.array(v)), 1, 0)\n","            specific_label *= k\n","            final_label = np.add(final_label, specific_label)\n","        return torch.from_numpy(final_label)"],"metadata":{"id":"uyTmnSROicuh","executionInfo":{"status":"ok","timestamp":1709023284603,"user_tz":-60,"elapsed":476,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image = self.dataset[idx][\"image\"] #.transpose(2, 0, 1) #(1080, 1920, 3) HXWxC\n","        print(f\"original_img shape {type(image)}\")\n","        semantic_mask = self.dataset[idx][\"semantic_mask_label\"] #((1080, 1920)) HXW\n","        print(f\"semantic mask shape {type(semantic_mask)}\")\n","        # original_image = deepcopy(image)\n","        # target_mask = deepcopy(semantic_mask)\n","\n","        if self.transforms:\n","            image_transforms, target_transforms = self.transforms\n","            # random.seed(self.seed_value)\n","            # torch.manual_seed(self.seed_value)\n","            transformed_image = image_transforms(image)\n","            transformed_mask = target_transforms(semantic_mask)\n","            print(\"unique labels\")\n","            my_transform = tvt.Compose([tvt.PILToTensor(),\n","                                        RemapBackground()])\n","            print(torch.unique(my_transform(semantic_mask)))\n","            print(torch.unique(transformed_mask))\n","            # transformed = self.transforms(image=image, mask=semantic_mask)\n","            # transformed_image, transformed_mask = transformed[\"image\"], transformed[\"mask\"]\n","            # # Transpose the transformed image to C x H x W\n","            # transformed_image = transformed_image.transpose(2, 0, 1)\n","        # else:\n","        #     transformed_image, transformed_mask = image.transpose(2, 0, 1), semantic_mask\n","        #print(transformed_image.shape, transformed_mask.shape, original_image.shape, target_mask.shape) #(3, 512, 512) (512, 512) (1080, 1920, 3) (1080, 1920)\n","        else:\n","            image_transforms, mask_transforms = get_transforms()\n","            transformed_image = image_transforms(image)\n","            transformed_mask = mask_transforms(semantic_mask)\n","        # return  transformed_image, transformed_mask, original_image, target_mask\n","        print(f\"transformed_image shape {transformed_image.shape}\")\n","        return  {\"image\": transformed_image, \"mask\": transformed_mask}"],"metadata":{"id":"9xjrSid_VV-a","executionInfo":{"status":"ok","timestamp":1709023284603,"user_tz":-60,"elapsed":7,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def print_trainable_parameters(model, print_msg):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for name, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(f\"\\n-----{print_msg}......\")\n","    print(\n","        f\"trainable params: {trainable_params / 1e+6:.2f}M || all params: {all_param / 1e+6:.2f}M || trainable%: {100 * trainable_params / all_param:.2f}%\"\n","    )\n","\n","def get_model_and_processor(ckpt_dataset_name, ckpt_name,\n","                            id2label=None, label2id=None, labels=None,\n","                            is_train=False):\n","    id2label = {str(k): v for k, v in id2label.items()}\n","    print(\"[INFO]: Prepare the Model and the Processor...\")\n","    model_ckpt = f\"shi-labs/oneformer_{ckpt_dataset_name}_{ckpt_name}_large\"\n","    if is_train:\n","        print(\"[INFO]: Model is in training mode...\")\n","\n","        model = AutoModelForUniversalSegmentation.from_pretrained(\n","            model_ckpt, is_training=is_train,\n","            id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n","        )\n","    # Processor\n","    processor = AutoProcessor.from_pretrained(\n","        model_ckpt,\n","        ignore_index=255,\n","        do_reduce_labels=False,\n","        do_normalize=True,\n","        do_rescale=True,\n","        do_resize=False\n","    )\n","    metadata = deepcopy(id2label)\n","    metadata[\"thing_ids\"] = [11, 12, 13, 14]\n","    metadata[\"class_names\"] = labels\n","    processor.image_processor.metadata = metadata\n","    processor.image_processor.num_labels = len(id2label)\n","    processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx\n","\n","    # Parameter Freezing\n","    if is_train:\n","        # Freeze the model backbone params\n","        for name, params in model.named_parameters():\n","            if name.startswith(\"model.pixel_level_module\"):\n","                params.requires_grad = False\n","        print_trainable_parameters(\n","            model, print_msg=\"Frozen Model Trainable Params\"\n","        )\n","    return model, processor"],"metadata":{"id":"GmQ6e0TqWE4h","executionInfo":{"status":"ok","timestamp":1709023284603,"user_tz":-60,"elapsed":6,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as tvt\n","from torch.utils.data import DataLoader, Dataset, Subset\n","import torchvision\n","from torchvision.transforms.functional import crop\n","from torchvision.utils import make_grid\n","import matplotlib.pyplot as plt\n","import torchvision.transforms.functional as F\n","\n","\n","class PatchQueue(Dataset):\n","    '''\n","    Extract random patches from an image -> put them in queue -> feed them to the dataloader\n","\n","    ref : https://github.com/fepegar/torchio/blob/main/src/torchio/data/queue.py\n","    '''\n","    def __init__(\n","            self, dataset, max_length, # Queue Length\n","            samples_per_image, # Patches per image\n","            queue_workers = 2, # Num Workers\n","            patch_size = [512, 512], # Patch Size\n","            shuffle = False\n","        ):\n","\n","        self.dataset = dataset\n","        self.max_length = max_length\n","        self.queue_workers = queue_workers\n","        self.samples_per_image = samples_per_image\n","        self.shuffle_queue = shuffle\n","        self.patch_size = patch_size\n","        self._images_iterable = None\n","        self.patch_list = []\n","        self._num_sampled_images = 0\n","        self.steps_per_epoch = len(self.dataset) * self.samples_per_image\n","        self.resize_transform = tvt.Resize(patch_size[::-1])  # Reverse patch_size for Resize\n","\n","    def __len__(self):\n","        return self.steps_per_epoch\n","\n","    def __getitem__(self, item):\n","        if not self.patch_list:\n","            print(\"Patch List is empty.\")\n","            self._fill()\n","            self.patch_list.reverse()\n","        sample_patch = self.patch_list.pop()\n","        return sample_patch\n","\n","    @staticmethod\n","    def _get_first_item(batch):\n","        if isinstance(batch[0], dict):\n","            print(batch[0].keys())\n","        else:\n","            print(batch[0])\n","        return batch[0]\n","\n","    def initialize_images_iterable(self):\n","        self._images_iterable = self._get_images_iterable()\n","\n","    @property\n","    def images_iterable(self):\n","        if self._images_iterable is None:\n","            self.initialize_images_iterable()\n","        return self.images_iterable\n","\n","    def _get_images_iterable(self):\n","        print(\"dataloader datset\\n\")\n","        loader = DataLoader(self.dataset,\n","                            num_workers = self.queue_workers,\n","                            batch_size=1,\n","                            collate_fn=self._get_first_item,\n","                            shuffle = self.shuffle_queue\n","                            ) #make it ddp compatible\n","        self._num_sampled_images = 0\n","        return iter(loader)\n","\n","    def _get_next_image(self):\n","        try:\n","            image = next(self._images_iterable)\n","        except Exception as e:\n","            print(f\"Excepton: {e}\")\n","            self.initialize_images_iterable()\n","            image = next(self._images_iterable)\n","        return image\n","\n","    def extract_patches(self, image_pair, samples_per_image, patch_size):\n","        image_patches = []\n","        img_height, img_width = image_pair['image'].shape[-2], image_pair['image'].shape[-1]\n","        # Check if image size is smaller than patch size\n","        if img_height < patch_size[0] or img_width < patch_size[1]:\n","            # Resize the image to match the patch size\n","            image_pair['image'] = self.resize_transform(image_pair['image'])\n","            image_pair['mask'] = self.resize_transform(image_pair['mask'])\n","            img_height, img_width = image_pair['image'].shape[-2], image_pair['image'].shape[-1]\n","        for i in range(samples_per_image):\n","            left = torch.randint(low=0, high=img_width-patch_size[0], size=[1,]).item()\n","            top = torch.randint(low=0, high=img_height-patch_size[1], size=[1,]).item()\n","            cropped_image = crop(img=image_pair['image'], top=top, left=left, height=patch_size[1], width=patch_size[0])\n","            cropped_labels = crop(img=image_pair['mask'], top=top, left=left, height=patch_size[1], width=patch_size[0])\n","            image_patches.append({'image':cropped_image, 'mask':cropped_labels})\n","        return image_patches\n","\n","\n","    def _fill(self):\n","        while True:\n","            image_pair = self._get_next_image()\n","            samples_per_image = self.samples_per_image\n","            patch_size = self.patch_size\n","            print(\"Extract patches\")\n","            patches = self.extract_patches(image_pair, samples_per_image,patch_size)\n","            self.patch_list.extend(patches)\n","            self._num_sampled_images += 1\n","            islistfull = len(self.patch_list) >= self.max_length\n","            print(\"end\")\n","            print(len(self.patch_list))\n","            for i, patch in enumerate(self.patch_list):\n","                print(i, patch.keys(), patch[\"image\"].shape, patch[\"mask\"].shape)\n","            if islistfull:\n","                break\n","\n","\n","\n","\n","# def show(imgs):\n","#     fig, ax = plt.subplots(nrows = 3,ncols = 3, squeeze = False)\n","#     for i, (img, label, im_full) in enumerate(zip(imgs['image'],imgs['label'], imgs['full_image'])):\n","#         image = F.to_pil_image(img)\n","#         im_full = F.to_pil_image(im_full)\n","#         label = label.squeeze()\n","#         ax[0,0].imshow(image)\n","#         ax[1,0].imshow(label,cmap = \"tab20\")\n","#         ax[2,0].imshow(im_full)\n","#     plt.show()\n","\n","\n","\n","# if __name__ == '__main__':\n","#     train_transform_img = tvt.Compose([\n","\n","#         tvt.ToTensor(),\n","#     ])\n","\n","#     train_transform_target = tvt.Compose([\n","\n","#         tvt.PILToTensor(),\n","#     ])\n","\n","#     from dataset_rail import RailSem19Dataset\n","\n","#     # dataset = torchvision.datasets.Cityscapes(root=\"DATA/CITY_SCAPES\", split=\"train\",mode=\"fine\",\n","#     #                                             target_type=\"semantic\",transform=train_transform_img, target_transform=train_transform_target)\n","#     ds = RailSem19Dataset(image_transform=train_transform_img)\n","\n","#     qob = PatchQueue(dataset = ds, max_length=4, samples_per_image = 5, patch_size = [1000,500])\n","#     dl = DataLoader(qob,1)\n","#     for i,data in enumerate(dl):\n","#         show(data)\n","#         if i >= 20:\n","#             break"],"metadata":{"id":"4vDJsTcDaYCG","executionInfo":{"status":"ok","timestamp":1709023284604,"user_tz":-60,"elapsed":7,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import torchvision.transforms.functional as F\n","\n","def show(imgs):\n","    fig, ax = plt.subplots(nrows = 3,ncols = 3, squeeze = False)\n","    # imgs['full_image']\n","    for i, (img, label) in enumerate(zip(imgs['original_images'],imgs['original_segmentation_maps'])):\n","        image = F.to_pil_image(img)\n","        # im_full = F.to_pil_image(im_full)\n","        label = label.squeeze()\n","        ax[0,0].imshow(image)\n","        ax[1,0].imshow(label, cmap = \"tab20\")\n","        # ax[2,0].imshow(im_full)\n","    plt.show()"],"metadata":{"id":"UnWwAVWPrVkv","executionInfo":{"status":"ok","timestamp":1709023284604,"user_tz":-60,"elapsed":6,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from torchvision.transforms.functional import to_pil_image\n","\n","def log_selected_indices_data(selected_indices_data, id2label):\n","    original_images, targets, pred_segmaps = zip(*selected_indices_data)\n","    # Convert to numpy arrays\n","    print(\"log_selected_indices_data before cat\")\n","    print(len(original_images), original_images[0].shape)\n","    print(len(targets), targets[0].shape)\n","    print(len(pred_segmaps), pred_segmaps[0].shape)\n","    original_images = torch.stack(original_images).detach().cpu().numpy()\n","    targets = torch.stack(targets).detach().cpu().numpy()\n","    pred_segmaps = torch.stack(pred_segmaps).detach().cpu().numpy()\n","    print(\"log_selected_indices_data\")\n","    print(original_images.shape)\n","    print(targets.shape)\n","    print(pred_segmaps.shape)\n","    # Log the data using wandb_mask_log\n","    table_data = wandb_mask_log(\n","        original_images, targets, pred_segmaps, id2label\n","    )\n","    return table_data\n","\n","# util function for generating interactive image mask from components\n","def wandb_mask_log(images, true_seg_masks, pred_seg_masks, id2label):\n","    new_id2label = {int(key): value for key, value in id2label.items()}\n","    print(f\"wandbmasklog id2label {new_id2label}\")\n","    table_data = []\n","    for img, gt_mask, pred_mask in zip(images, true_seg_masks, pred_seg_masks):\n","        print(np.unique(gt_mask), np.unique(pred_mask))\n","        try:\n","            img *= 255\n","            img = img.transpose((1, 2, 0))\n","            img = (img).astype(np.uint8)\n","            gt_mask = (gt_mask).astype(np.uint8)\n","            pred_mask = (pred_mask).astype(np.uint8)\n","            pil_image = to_pil_image(img)\n","            table_data.append(wandb.Image(\n","                pil_image, masks={\n","                    \"ground truth\": {\n","                        \"mask_data\": gt_mask,\n","                        \"class_labels\": new_id2label\n","                    },\n","                    \"prediction\": {\n","                        \"mask_data\": pred_mask,\n","                        \"class_labels\": new_id2label\n","                    }\n","                }\n","            ))\n","        except Exception as e:\n","            # Handle the exception related to PILImage broken chunk ocassionally\n","            print(f\"Exception in creating wandb.Image: {e}\")\n","            # Skip this iteration and move to the next image\n","            continue\n","    return table_data"],"metadata":{"id":"6SAb54gIrvv_","executionInfo":{"status":"ok","timestamp":1709024900986,"user_tz":-60,"elapsed":242,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["from torchmetrics.classification import Accuracy, MulticlassAccuracy, MulticlassJaccardIndex, JaccardIndex\n","\n","class ModelResults:\n","    def __init__(self):\n","        pass\n","\n","class OneformerTrainer:\n","    def __init__(self, args, run, gpu_id):\n","        self.seed = args.seed\n","        self.railsem_ds = load_railsem_dataset(gpu_id)\n","        labels_info = get_labels()\n","        self.id2label = labels_info[\"id2label\"]\n","        self.label2id = labels_info[\"label2id\"]\n","        self.labels = labels_info[\"labels\"]\n","        self.color_palette = labels_info[\"color_palette\"]\n","\n","        print(f\"[INFO]: id2label: {self.id2label}\")\n","        print(f\"[INFO]: Setting the seed {self.seed}\")\n","        set_seeds(self.seed)\n","        self.model, self.processor = get_model_and_processor(\n","            args.ckpt_dataset_name, args.ckpt_model_name, id2label=self.id2label, label2id=self.label2id,\n","            labels=self.labels, is_train=args.is_train\n","        )\n","        self.is_distributed = False\n","        self.epochs = args.num_epochs\n","        self.batch_size = args.batch_size\n","        self.learning_rate = args.lr\n","        self.device = gpu_id\n","        self.model.to(self.device)\n","        self.is_train = args.is_train\n","        if self.is_train:\n","            params_groups = self.get_params_groups() #create param groups\n","        else:\n","            params_groups = self.model.parameters()\n","        self.optimizer = AdamW(params=params_groups, lr=self.learning_rate)\n","        self.scheduler = ReduceLROnPlateau(self.optimizer, \"min\", factor=0.5, patience=5, threshold=0.0001, min_lr=1e-6)\n","        self.scaler = GradScaler()\n","        self.best_loss = 1e+5\n","        self.start_epoch = 0\n","        self.global_step = 0\n","        if run is not None:\n","            print(f\"[INFO]: Run object: {run}\")\n","            run.config[\"classes\"] = len(self.id2label)\n","            self.wandb_config = run.config\n","            self.run = run\n","        self.log = self.run is not None\n","        # Metrics and results\n","        self.metric_names = [\"loss\", \"mean_iou\", \"mean_accuracy\", \"overall_accuracy\", \"per_category_iou\", \"per_category_accuracy\"]\n","        self.metric_types = [\"train\", \"val\"]\n","        self.model_results = ModelResults()\n","        #calculates metrics(iou, acc) for each label and applies no reduction, we ignore background class\n","        self.iou_per_cls = JaccardIndex(task=\"multiclass\", num_classes=len(self.id2label), average=\"none\", ignore_index=255).to(\"cpu\")\n","        self.acc_per_cls = Accuracy(task=\"multiclass\", num_classes=len(self.id2label), average=\"none\", ignore_index=255).to(\"cpu\")\n","        # Calculate metrics(miou, Overallacc) for each class separately and average the metrics across classes after weighing each class by its support = TP + FN\n","        self.IoU_weighted = JaccardIndex(task=\"multiclass\", num_classes=len(self.id2label), average='weighted', ignore_index=255).to(\"cpu\")\n","        self.acc_overall_weighted = Accuracy(task=\"multiclass\", num_classes=len(self.id2label), average='weighted', ignore_index=255).to(\"cpu\")\n","        self.jaccard_index = JaccardIndex(task=\"multiclass\", num_classes=len(self.id2label), average='weighted', ignore_index=255).to(\"cpu\")\n","        # Val metrics\n","        self.val_iou_per_cls = JaccardIndex(\n","            task=\"multiclass\", num_classes=len(self.id2label),\n","            average=\"none\", ignore_index=255).to(self.device)\n","        self.val_acc_per_cls = Accuracy(\n","            task=\"multiclass\", num_classes=len(self.id2label),\n","            average=\"none\", ignore_index=255).to(self.device)\n","        # Calculate metrics(miou, Overallacc) for each class separately and average the metrics across classes after weighing each class by its support = TP + FN\n","        self.val_IoU_weighted = JaccardIndex(\n","            task=\"multiclass\", num_classes=len(self.id2label),\n","            average='weighted', ignore_index=255).to(self.device)\n","        self.val_acc_overall_weighted = Accuracy(\n","            task=\"multiclass\", num_classes=len(self.id2label),\n","            average=\"weighted\", ignore_index=255).to(self.device)\n","        self.val_jaccard_index = JaccardIndex(\n","            task=\"multiclass\", num_classes=len(self.id2label),\n","            average=\"weighted\", ignore_index=255).to(self.device)\n","\n","    def _get_datasets(self):\n","        print(\"[INFO]: Preparing the Datasets...\")\n","        try:\n","            if \"data\" in self.railsem_ds:\n","                print(\"[INFO]: No splits found.. Generating splits\")\n","                print(f\"[INFO]: Total images: {len(self.railsem_ds['data'])}\")\n","                splits = self.railsem_ds[\"data\"].train_test_split(\n","                    test_size=0.05, shuffle=True\n","                )\n","                random.seed(1)\n","                train_split = splits[\"train\"].select(indices=(range(5)))\n","                val_split = splits[\"test\"].select(indices=(range(3)))\n","                print(f\"[INFO]: Total Training images: {len(train_split)}\")\n","                print(f\"[INFO]: Total Validation images: {len(val_split)}\")\n","            else:\n","                raise ValueError(\"Data splits not in required format\")\n","        except ValueError as e:\n","            print(e)\n","        # Get the transforms\n","        transforms = get_transforms()\n","        # Custom Dataset\n","        train_dataset = CustomDataset(train_split, transforms)\n","        val_dataset = CustomDataset(val_split, transforms)\n","        print(\"patch queue calling\")\n","        train_queue = PatchQueue(dataset=train_dataset, max_length=10, samples_per_image=5, patch_size=[512, 512])#[1024, 1024]\n","        val_queue = PatchQueue(dataset=val_dataset, max_length=10,samples_per_image=5, patch_size=[512, 512])\n","        print(type(train_queue))\n","        return train_dataset, val_dataset, train_queue, val_queue\n","\n","    def get_params_groups(self):\n","        params_group = []\n","        textmapper_params = []\n","        pretrained_params = []\n","        for name, param in self.model.named_parameters():\n","            if \"text_mapper\" in name:\n","                textmapper_params.append(param)\n","            else:\n","                pretrained_params.append(param)\n","        params_group.append({\"params\": textmapper_params, \"lr\": 1e-2})\n","        params_group.append({\"params\": pretrained_params})  # \"lr\": 1e-4\n","        print(\"[INFO]: Num of params in prompt textmapper group:\", len(textmapper_params))\n","        print(\"[INFO]: Num of params in pretrained group:\", len(pretrained_params))\n","        return params_group\n","\n","    def _prepare_dataloader(self, dataset: torch.utils.data.Dataset,\n","                           batch_size: int, is_train):\n","        print(\"inside\")\n","        sampler = None\n","        dataloader = DataLoader(\n","            dataset,\n","            batch_size=batch_size,\n","            pin_memory=True,\n","            shuffle=(is_train and sampler is None),\n","            sampler=sampler,\n","            collate_fn=self._collate_fn,\n","        )\n","        return dataloader\n","\n","    def _collate_fn(self, batch):\n","        print(\"my collate\")\n","        images = torch.stack([sample['image'] for sample in batch])\n","        segmentation_maps = torch.cat([sample['mask'] for sample in batch])\n","        print(\"collate\")\n","        print(images.shape)\n","        print(segmentation_maps.shape)\n","        batch = self.processor(images,\n","                               segmentation_maps=segmentation_maps,\n","                               task_inputs=[\"semantic\"] * len(images),\n","                               return_tensors=\"pt\")\n","        batch[\"original_images\"] = images\n","        batch[\"original_segmentation_maps\"] = segmentation_maps\n","        return batch\n","\n","    def log_category_chart(self, metric_values, title):\n","        # Ensure metric_values is a list of tensors to handle both single last epoch and all epochs metrics\n","        if not isinstance(metric_values, list):\n","            metric_values = [metric_values]\n","\n","        # Converting nan to zeros\n","        metric_values = [torch.where(torch.isnan(tensor), torch.tensor(0.0), tensor) for tensor in metric_values]\n","\n","        # Stack if it's a list of tensors\n","        avg_metric_values = torch.stack(metric_values).nanmean(dim=0) if len(metric_values) > 1 else metric_values[0]\n","\n","        data = [[label, acc] for (label, acc) in zip(\n","            self.processor.image_processor.metadata[\"class_names\"],\n","            avg_metric_values)\n","        ]\n","        table = wandb.Table(data=data, columns=[\"Class Name\", title])\n","        self.run.log({\n","            f\"{title}\": wandb.plot.bar(\n","                table, \"Class Name\", title,\n","                title=f\"{title}\")\n","        })\n","\n","    def log_metrics(self, epoch, global_step, metrics_to_log=None):\n","        for metric_type in self.metric_types:\n","            print(f\"[INFO]: Logging {metric_type} metrics\")\n","            metric_values = {}\n","            # If metrics_to_log is not specified (or is None), it defaults to logging all metrics\n","            for metric_name in metrics_to_log or self.metric_names:\n","                full_metric_name = f\"{metric_type}_{metric_name}\"\n","                metric_values[full_metric_name] = getattr(self.model_results, full_metric_name)[-1] # get the last one in the respective metric list\n","            # Log all metrics for the specified metric type in a single step\n","            metric_values[\"custom_step\"] = int(global_step)\n","            self.run.log(metric_values)\n","\n","    def get_all_metrics(self):\n","        all_metrics = {}\n","        for metric_type in self.metric_types:\n","            for metric_name in self.metric_names:\n","                full_metric_name = f\"{metric_type}_{metric_name}\"\n","                all_metrics[full_metric_name] = getattr(self.model_results, full_metric_name)\n","        return all_metrics\n","\n","    def train_step(self, train_dataloader: torch.utils.data.DataLoader):\n","        running_train_loss = 0.0\n","        num_samples = 0\n","        total_steps = len(train_dataloader) * self.epochs\n","\n","        running_iou =  []\n","        self.model.train()\n","        for step, batch in enumerate(tqdm(train_dataloader)):\n","            print(\"batch\")\n","            print(batch.keys())\n","            # for k, v in batch.items():\n","            #     if isinstance(v, torch.Tensor):\n","            #         print(k, v.shape)\n","            #     else:\n","            #         print(k, len(v))\n","            self.optimizer.zero_grad()\n","            batch_dict = {\n","                \"pixel_values\": batch[\"pixel_values\"].to(self.device),\n","                \"mask_labels\": [labels.to(self.device)\n","                                for labels in batch[\"mask_labels\"]],\n","                \"class_labels\": [labels.to(self.device)\n","                                 for labels in batch[\"class_labels\"]],\n","                \"pixel_mask\": batch[\"pixel_mask\"].to(self.device),\n","                \"text_inputs\": batch[\"text_inputs\"].to(self.device),\n","                \"task_inputs\": batch[\"task_inputs\"].to(self.device),\n","            }\n","            # Forward Pass\n","            with autocast():\n","                outputs = self.model(**batch_dict)\n","                train_loss = outputs.loss\n","            # print(\"outputs\")\n","            # for k, v in outputs.items():\n","            #     if isinstance(v, torch.Tensor):\n","            #         print(k, v.dtype)\n","            #     else:\n","            #         print(k, len(v))\n","            # print(outputs[\"class_queries_logits\"].float().dtype)\n","            # print(outputs[\"masks_queries_logits\"].float().dtype)\n","            # outputs[\"class_queries_logits\"] = outputs[\"class_queries_logits\"].float()\n","            # outputs[\"masks_queries_logits\"] = outputs[\"masks_queries_logits\"].float()\n","\n","            running_train_loss += train_loss.long().item()\n","            train_batch_size = batch[\"pixel_values\"].size(0)\n","            num_samples += train_batch_size\n","            print(f\"Train Loss after {str(num_samples).zfill(6)} examples:\"\n","                  f\"{train_loss.item():.3f}\\n\")\n","\n","            # Post process segmentation\n","            original_images = batch[\"original_images\"]\n","            print(\"original images\")\n","            for img in original_images:\n","                print(img.shape) #uint8\n","            target_sizes = [(img.shape[-2], img.shape[-1]) for img in original_images]\n","            predicted_segmentation_maps = self.processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n","            print(outputs.keys())\n","            print(outputs[\"masks_queries_logits\"].shape), print(outputs[\"class_queries_logits\"].shape)\n","            print(\"predicted_segmentation_maps\")\n","            print(predicted_segmentation_maps[0].shape, predicted_segmentation_maps[1].shape)\n","            # Get ground truth segmentation maps and move both target and preds to cpu\n","            # target = torch.stack([torch.from_numpy(arr) for arr in batch[\"original_segmentation_maps\"]]).to(\"cpu\")\n","            #`preds` with shape=torch.Size([2, 3, 512]) and `target` with shape=torch.Size([2, 512, 512])\n","            # target = torch.stack([g for g in batch[\"original_segmentation_maps\"]]).to(\"cpu\")\n","            target = batch[\"original_segmentation_maps\"].to(\"cpu\")\n","            preds = torch.stack([t.detach().cpu() for t in predicted_segmentation_maps])\n","            print(\"tensor dtypes\")\n","            print(target.dtype, target.shape) #torch.uint8\n","            print(preds.dtype, preds.shape) #torch.int64\n","            print(target.long().dtype) #torch.int64\n","            print(preds.long().dtype) #torch.int64\n","\n","            # Metrics calculation per batch\n","            iou_per_cls_batch = self.iou_per_cls(preds.long(), target.long())\n","            acc_per_cls_batch = self.acc_per_cls(preds.long(), target.long())\n","            IoU_batch = self.IoU_weighted(preds.long(), target.long())\n","            acc_overall_batch = self.acc_overall_weighted(preds.long(), target.long())\n","\n","            #Jackard metric - can be deleted later it is same as IoU_weighted\n","            running_iou.append(self.jaccard_index(preds.long(), target.long()).item())\n","\n","            # Log loss to wandb every 10 batches i.e after every 10 steps\n","            self.global_step += 1\n","            if ((step + 1) % 10) == 0 or step == len(train_dataloader) - 1:\n","                if self.log:\n","                    self.run.log({\n","                        \"custom_step\": int(self.global_step),\n","                        \"train_step_loss\": train_loss.item()\n","                    })\n","            # Backward pass\n","            self.scaler.scale(train_loss).backward()\n","            self.scaler.step(self.optimizer)\n","            self.scaler.update()\n","            del batch, outputs\n","\n","        # after training the epoch\n","        train_epoch_loss = running_train_loss / len(train_dataloader)\n","        iou_per_cls_epoch = self.iou_per_cls.compute()\n","        mIoU_epoch = self.IoU_weighted.compute()\n","        jacc_miou_train = statistics.mean(running_iou)\n","        acc_per_cls_epoch = self.acc_per_cls.compute()\n","        acc_overall_epoch = self.acc_overall_weighted.compute()\n","        mean_acc_epoch = torch.nanmean(acc_per_cls_epoch)\n","        train_metrics_epoch = {'mean_iou': mIoU_epoch.item(),\n","                       'mean_accuracy': mean_acc_epoch.item(),\n","                       'overall_accuracy': acc_overall_epoch.item(),\n","                       'per_category_iou': iou_per_cls_epoch,\n","                       'per_category_accuracy': acc_per_cls_epoch}\n","        # Reset the metric states after the epoch\n","        self.iou_per_cls.reset()\n","        self.IoU_weighted.reset()\n","        self.acc_per_cls.reset()\n","        self.acc_overall_weighted.reset()\n","\n","        return train_epoch_loss, jacc_miou_train, train_metrics_epoch\n","\n","    def val_step(self, val_dataloader, epoch):\n","        running_val_loss = 0.0\n","        running_iou = []\n","        # Lists to store data for selected indices\n","        selected_indices_data = []\n","        print(f\"val dataloder: {len(val_dataloader)}\")\n","\n","        # Eval Loop\n","        self.model.eval()\n","        for step, batch in enumerate(tqdm(val_dataloader)):\n","            batch_dict = {\n","                \"pixel_values\": batch[\"pixel_values\"].to(self.device),\n","                \"mask_labels\": [labels.to(self.device)\n","                                for labels in batch[\"mask_labels\"]],\n","                \"class_labels\": [labels.to(self.device)\n","                                 for labels in batch[\"class_labels\"]],\n","                \"pixel_mask\": batch[\"pixel_mask\"].to(self.device),\n","                \"text_inputs\": batch[\"text_inputs\"].to(self.device),\n","                \"task_inputs\": batch[\"task_inputs\"].to(self.device),\n","            }\n","\n","            # Eval forward pass\n","            with torch.inference_mode():\n","                val_outputs = self.model(**batch_dict)\n","\n","            val_loss = val_outputs.loss.item()\n","            # running_val_loss += val_loss\n","            batch_size = batch[\"pixel_values\"].size(0)\n","            running_val_loss += val_loss * batch_size\n","            # Post process segmentation\n","            original_images = batch[\"original_images\"]\n","            target_sizes = [(img.shape[-2], img.shape[-1])\n","                            for img in original_images]\n","            predicted_segmentation_maps = \\\n","                self.processor.post_process_semantic_segmentation(\n","                    val_outputs, target_sizes=target_sizes)\n","            # Get ground truth segmentation maps & move target, preds to cpu\n","            target =  batch[\"original_segmentation_maps\"].to(self.device)\n","            preds = torch.stack([\n","                t.detach() for t in predicted_segmentation_maps\n","            ]).to(self.device)\n","\n","            # Metrics calculation per batch\n","            iou_per_cls_batch = self.val_iou_per_cls(preds.long(), target.long())\n","            acc_per_cls_batch = self.val_acc_per_cls(preds.long(), target.long())\n","            IoU_batch = self.val_IoU_weighted(preds.long(), target.long())\n","            acc_overall_batch = self.val_acc_overall_weighted(preds.long(), target.long())\n","\n","            # Jackard metric - can be removed later it is same as IoU_weighted\n","            running_iou.append(self.val_jaccard_index(preds.long(), target.long()).item())\n","\n","            print(f\"Val Loss : {val_loss:.3f}\\n\")\n","\n","            # Store one random index per step\n","            if not len(selected_indices_data) > 4:\n","                random_index = np.random.randint(0, batch_size)\n","                selected_indices_data.append(\n","                    (original_images[random_index],\n","                     target[random_index],\n","                     predicted_segmentation_maps[random_index])\n","                )\n","            # Log loss to wandb every 10 batches i.e after every 10 steps\n","            self.global_step += 1\n","            if ((step + 1) % 50) == 0 or step == len(val_dataloader) - 1:\n","                if self.log:\n","                    self.run.log({\n","                        \"custom_step\": int(self.global_step),\n","                        \"epoch\": epoch,\n","                        \"Val_step_loss\": val_loss\n","                    })\n","            del batch_dict, val_outputs\n","\n","        running_val_loss = torch.tensor([running_val_loss], device=self.device)\n","        if self.is_distributed:\n","            torch.distributed.all_reduce(running_val_loss,\n","                                         op=torch.distributed.ReduceOp.SUM)\n","        val_epoch_loss = running_val_loss.item() / len(self.val_dataset)\n","        iou_per_cls_epoch = self.val_iou_per_cls.compute()\n","        mIoU_epoch = self.val_IoU_weighted.compute()\n","        jacc_miou_val = statistics.mean(running_iou)\n","        acc_per_cls_epoch = self.val_acc_per_cls.compute()\n","        acc_overall_epoch = self.val_acc_overall_weighted.compute()\n","        mean_acc_epoch = torch.nanmean(acc_per_cls_epoch)\n","        val_metrics_epoch = {\n","            \"mean_iou\": mIoU_epoch.item(),\n","            \"mean_accuracy\": mean_acc_epoch.item(),\n","            \"overall_accuracy\": acc_overall_epoch.item(),\n","            \"per_category_iou\": iou_per_cls_epoch,\n","            \"per_category_accuracy\": acc_per_cls_epoch\n","        }\n","\n","        # Reset the metric states after the epoch\n","        self.val_iou_per_cls.reset()\n","        self.val_IoU_weighted.reset()\n","        self.val_acc_per_cls.reset()\n","        self.val_acc_overall_weighted.reset()\n","\n","        # Wandb Logging of images and predictions at the end of validation epoch\n","        if self.log and self.device == 0:\n","            print(\n","                f\"[INFO]: Logging Validation Images, on device: {self.device}\"\n","            )\n","            print(len(selected_indices_data))\n","            table_data = log_selected_indices_data(selected_indices_data,\n","                                                   self.id2label)\n","            self.run.log({\"predictions\": table_data})\n","            del selected_indices_data\n","        return val_epoch_loss, jacc_miou_val, val_metrics_epoch\n","\n","    def train(self):\n","        print(\"[INFO]: Started Training the model...\")\n","        # Datasets\n","        train_dataset, val_dataset, train_queue, val_queue = self._get_datasets()\n","        self.val_dataset = val_queue\n","        # # DataLoaders\n","        # train_dataloader = self._prepare_dataloader(\n","        #     train_queue, batch_size=self.batch_size,\n","        #     is_train=self.is_train\n","        # )\n","        # print(\"train queue\")\n","        # print(len(train_queue))\n","        # print(len(train_dataloader))\n","\n","        print(\"Optimizer Parameter Groups:\")\n","        for i, group in enumerate(self.optimizer.param_groups):\n","            print(f\"Group {i} - Learning Rate: {group['lr']}\")\n","\n","        for epoch in tqdm(range(self.start_epoch, self.epochs)):\n","            print(\"epoch\")\n","            val_dataloader = self._prepare_dataloader(\n","                val_queue, batch_size=self.batch_size, is_train=self.is_train\n","            )\n","            val_loss, jacc_miou_val, val_metrics_epoch = self.val_step(val_dataloader, epoch)\n","            # train_loss, jacc_miou_train, train_metrics_epoch = \\\n","            # self.train_step(train_dataloader)\n","            # print(\"train_loss\")\n","            # print(train_loss)\n","        # for step, batch in enumerate(train_dataloader):\n","        #     print(step)\n","        #     print(batch[\"original_images\"].shape)\n","        #     print(batch[\"original_segmentation_maps\"].shape)\n","            # print(batch.keys())\n","            # for k, v in batch.items():\n","            #     if isinstance(v, torch.Tensor):\n","            #         print(k, v.shape)\n","            #     else:\n","            #         print(k, len(v))\n","\n","\n","            # show(batch)\n","            # if step >= 20:\n","            #     break\n","        # return\n","\n","\n"],"metadata":{"id":"b1Vgj3T7WR1b","executionInfo":{"status":"ok","timestamp":1709025354996,"user_tz":-60,"elapsed":295,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["import torch\n","import gc\n","import wandb\n","import subprocess, os, shutil\n","\n","import argparse\n","import os\n","from typing import List, Dict, Tuple\n","import warnings\n","from datetime import datetime, timedelta\n","\n","\n","def main(run, args):\n","    if torch.cuda.is_available():\n","        rank = 0  # sets device=cuda:0 explicilty if gpu exist & only 1 available\n","    else:\n","        rank = \"cpu\"\n","    if args.is_train:\n","        print(\"[INFO]: Finetuning the Oneformer model with\"\n","              f\" {args.ckpt_model_name} backbone trained on\"\n","              f\" {args.ckpt_dataset_name} dataset on device {rank}.\")\n","        trainer = OneformerTrainer(args, run, rank)\n","        model_results = trainer.train()\n","        return model_results\n","\n","class myNamespace:\n","    def __init__(self, **kwargs):\n","        self.__dict__.update(kwargs)\n","\n","args = myNamespace(is_train=True, ckpt_model_name=\"swin\", ckpt_dataset_name=\"cityscapes\",\n","                   num_epochs=2, lr=1e-4, batch_size=2, seed=1)\n","print(f\"Cuda support: {torch.cuda.is_available()},\"\n","        f\"devices exist : {torch.cuda.device_count()}\")\n","gc.collect()\n","torch.cuda.empty_cache()\n","torch.cuda.reset_max_memory_allocated()\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))\n","# wandb_project_name = (\n","#         f\"OF_Finetuning_{args.ckpt_dataset_name}_{args.ckpt_model_name}_\"\n","#         f\"railSem19_small_id2label\"\n","#     )\n","wandb_project_name = \"OF_Finetuning_cityscapes_swin_notebook\"\n","wandb_config = dict(\n","        epochs=args.num_epochs,\n","        batch_size=args.batch_size,\n","        dataset=f\"{args.ckpt_dataset_name}_RailSem19\",\n","        architecture=f\"OF_{args.ckpt_model_name}_L\",\n","        project_name=wandb_project_name,\n","    )\n","print(f\"[INFO]: Initializing wandb\")\n","wandb.require(\"service\")\n","run =  wandb.init(\n","        project=wandb_project_name,\n","        config=wandb_config,\n","        entity=os.environ.get(\"WANDB_ENTITY\"),\n","        resume=True,\n","        mode=\"offline\"\n",")\n","rdir = os.path.split(run.dir)[0]\n","\n","model_results = main(run, args)\n","\n","print(\"[INFO] Finishing the run....\")\n","run.finish()\n","print(\"[INFO] Run Finished....\")\n","\n","if rdir:\n","    print(f\"[INFO]: Syncing {rdir}\")\n","    subprocess.run([\"wandb\", \"sync\", rdir])\n","    print(f\"Removing {rdir}....\")\n","    shutil.rmtree(rdir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7d9916ed016542fda4d21fa043acabdb","43f2bc0c746849ab9bb5642e413ae118","508a8049bf8f4086b70cd4e024c76454","1577d1adf09840a5aeb73b9bc667800e","1fe233ef30724de8b15797fe3330fa5a","144fceffa7c442ef9e1788d897259e8d","d012749f64dd49728feb23baa63ee88d","e45abb612fca42b0a7e2c5c8f183f074","a6cb65dad5354a6f825916ca7f2ba3c0","890ab7d812074f2097d5134cb0cc257b","3f807236e9c24d92b7cc8965311e1acd","11317e2f89c047c5afc77a9e47da63a2","ab714a51d20f4963a334b511a4302bb9","da6375de2d2d4e78ada517da08db2c1e","c488b0f845394020b19351490a4626f7","4c9bd7ee8e074dcdb7b46018ab0ba881","d13e51498259426889a9b4b836768c43","268c5e42ceca454589c42b4aa101f820","8b9b464647304e958d68cd886381c43e","3804a916120040128407f6096e796e13","ff52f4781985498a9d965cbef3393f80","c4bf3257a3454b0dab5fb6d30b54132d","4d3b721f01794c8e9ee5bd09888f92c6","f54f382f3dd041a182272a4bac466060","8174a56b0ce143feb7c8b0f3ba9cf2bc","22121c6b2941450d92886f2aed745fe0","7456bae620dc44ed9c4f6b62290d1abe","047e73bd5c8d4465a1dd65e37ace7306","09ab5df035844cc1b49ebd6da7a2017d","71afcf57f8884835aaf4d0b15efeb67c","f46ea018beef4463a0c7de32376daf84","5e125fe796e14a919851f19932007589","9b42184410064c72a7a74b2f4808400c","fad0d0168b16406ebd88d3c1f9926b22","736effe10373488196e48007ec4df33e","3f3d61dcf8374639985b8ffc9ee1616d","d46db6769cfd4ec1ba7419ef2cd14302","be86ff207337453d801991b0f179f06b","1ebfdd4e288944cf9262df90984166f6","0e68f03011fa44a49a509d9531eab95a","50e9789d29174b01a7462de5c58bfedc"]},"id":"__NzIlTAWjSO","executionInfo":{"status":"ok","timestamp":1709025410118,"user_tz":-60,"elapsed":45186,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"259973b2-49c6-4461-a48c-d9fd24faced3"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Cuda support: True,devices exist : 1\n","[INFO]: Initializing wandb\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id tbb08rkz.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[INFO]: Finetuning the Oneformer model with swin backbone trained on cityscapes dataset on device 0.\n","[INFO]: Extracting Railsem19 dataset from hub on device 0\n","[INFO]: Found labels_info.json.Skipping Download...\n","[INFO]: id2label: {0: 'road', 1: 'sidewalk', 2: 'construction', 3: 'tram_track', 4: 'fence', 5: 'pole', 6: 'traffic_light', 7: 'traffic_sign', 8: 'vegetation', 9: 'terrain', 10: 'sky', 11: 'human', 12: 'rail_track', 13: 'car', 14: 'truck', 15: 'trackbed', 16: 'on_rails', 17: 'rail_raised', 18: 'rail_embedded', 19: 'background'}\n","[INFO]: Setting the seed 1\n","[INFO]: Prepare the Model and the Processor...\n","[INFO]: Model is in training mode...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized: ['model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_projector.layers.1.0.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized because the shapes did not match:\n","- model.transformer_module.decoder.class_embed.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([21, 256]) in the model instantiated\n","- model.transformer_module.decoder.class_embed.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([21]) in the model instantiated\n","- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([21]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","-----Frozen Model Trainable Params......\n","trainable params: 35.33M || all params: 236.32M || trainable%: 14.95%\n","[INFO]: Num of params in prompt textmapper group: 81\n","[INFO]: Num of params in pretrained group: 801\n","[INFO]: Run object: <wandb.sdk.wandb_run.Run object at 0x7d39cb96ed40>\n","[INFO]: Started Training the model...\n","[INFO]: Preparing the Datasets...\n","[INFO]: No splits found.. Generating splits\n","[INFO]: Total images: 8500\n","[INFO]: Total Training images: 5\n","[INFO]: Total Validation images: 3\n","[INFO]: Found labels_info.json.Skipping Download...\n","class_mapping\n","{0: 'road', 1: 'sidewalk', 2: 'construction', 3: 'tram_track', 4: 'fence', 5: 'pole', 6: 'traffic_light', 7: 'traffic_sign', 8: 'vegetation', 9: 'terrain', 10: 'sky', 11: 'human', 12: 'rail_track', 13: 'car', 14: 'truck', 15: 'trackbed', 16: 'on_rails', 17: 'rail_raised', 18: 'rail_embedded', 19: 'background'}\n","class_coding\n","{'road': 0, 'sidewalk': 1, 'construction': 2, 'tram_track': 3, 'fence': 4, 'pole': 5, 'traffic_light': 6, 'traffic_sign': 7, 'vegetation': 8, 'terrain': 9, 'sky': 10, 'human': 11, 'rail_track': 12, 'car': 13, 'truck': 14, 'trackbed': 15, 'on_rails': 16, 'rail_raised': 17, 'rail_embedded': 18, 'background': 19}\n","modified_ids\n","{0: [0], 1: [1], 2: [2, 4], 3: [17, 18], 4: [5, 6, 7], 5: [10], 6: [11], 7: [3, 12], 8: [13, 14], 9: [16], 10: [8], 11: [15], 12: [19, 9]}\n","patch queue calling\n","<class '__main__.PatchQueue'>\n","Optimizer Parameter Groups:\n","Group 0 - Learning Rate: 0.01\n","Group 1 - Learning Rate: 0.0001\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9916ed016542fda4d21fa043acabdb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["epoch\n","inside\n","val dataloder: 8\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11317e2f89c047c5afc77a9e47da63a2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Patch List is empty.\n","Excepton: 'NoneType' object is not an iterator\n","dataloader datset\n","\n","original_img shape <class 'PIL.JpegImagePlugin.JpegImageFile'>\n","original_img shape <class 'PIL.JpegImagePlugin.JpegImageFile'>\n","semantic mask shape <class 'PIL.PngImagePlugin.PngImageFile'>\n","semantic mask shape <class 'PIL.PngImagePlugin.PngImageFile'>\n","unique labels\n","unique labelstensor([ 0,  1,  2,  4,  5,  7,  8,  9, 10, 12, 15, 16, 17, 19],\n","       dtype=torch.uint8)\n","\n","tensor([ 0,  1,  2,  3,  4,  5,  7,  9, 10, 11, 12])tensor([ 0,  1,  2,  4,  5,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19],\n","       dtype=torch.uint8)\n","\n","transformed_image shape torch.Size([3, 1080, 1920])\n","dict_keys(['image', 'mask'])\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n","transformed_image shape torch.Size([3, 1080, 1920])\n","dict_keys(['image', 'mask'])\n","original_img shape <class 'PIL.JpegImagePlugin.JpegImageFile'>\n","semantic mask shape <class 'PIL.PngImagePlugin.PngImageFile'>\n","Extract patches\n","end\n","5\n","0 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","1 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","2 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","3 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","4 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","Extract patches\n","end\n","10\n","0 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","1 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","2 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","3 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","4 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","5 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","6 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","7 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","8 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","9 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","unique labels\n","tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19],\n","       dtype=torch.uint8)\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n","transformed_image shape torch.Size([3, 1080, 1920])\n","dict_keys(['image', 'mask'])\n","Val Loss : 120.943\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 116.875\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 120.721\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 118.694\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 122.439\n","\n","Patch List is empty.\n","Extract patches\n","end\n","5\n","0 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","1 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","2 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","3 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","4 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","Excepton: \n","dataloader datset\n","\n","original_img shape <class 'PIL.JpegImagePlugin.JpegImageFile'>\n","original_img shape <class 'PIL.JpegImagePlugin.JpegImageFile'>\n","semantic mask shape <class 'PIL.PngImagePlugin.PngImageFile'>\n","semantic mask shape <class 'PIL.PngImagePlugin.PngImageFile'>\n","unique labels\n","tensor([ 0,  1,  2,  4,  5,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19],\n","       dtype=torch.uint8)\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n","transformed_image shape torch.Size([3, 1080, 1920])\n","dict_keys(['image', 'mask'])\n","unique labels\n","tensor([ 0,  1,  2,  4,  5,  7,  8,  9, 10, 12, 15, 16, 17, 19],\n","       dtype=torch.uint8)\n","tensor([ 0,  1,  2,  3,  4,  5,  7,  9, 10, 11, 12])\n","transformed_image shape torch.Size([3, 1080, 1920])\n","dict_keys(['image', 'mask'])\n","original_img shape <class 'PIL.JpegImagePlugin.JpegImageFile'>\n","semantic mask shape <class 'PIL.PngImagePlugin.PngImageFile'>\n","Extract patches\n","end\n","10\n","0 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","1 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","2 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","3 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","4 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","5 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","6 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","7 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","8 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","9 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","unique labels\n","tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19],\n","       dtype=torch.uint8)\n","tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n","transformed_image shape torch.Size([3, 1080, 1920])\n","dict_keys(['image', 'mask'])\n","Val Loss : 118.360\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 113.066\n","\n","my collate\n","collate\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 512, 512])\n","Val Loss : 114.928\n","\n","[INFO]: Logging Validation Images, on device: 0\n","5\n","log_selected_indices_data before cat\n","5 torch.Size([3, 512, 512])\n","5 torch.Size([512, 512])\n","5 torch.Size([512, 512])\n","log_selected_indices_data\n","(5, 3, 512, 512)\n","(5, 512, 512)\n","(5, 512, 512)\n","wandbmasklog id2label {0: 'road', 1: 'sidewalk', 2: 'construction', 3: 'tram_track', 4: 'fence', 5: 'pole', 6: 'traffic_light', 7: 'traffic_sign', 8: 'vegetation', 9: 'terrain', 10: 'sky', 11: 'human', 12: 'rail_track', 13: 'car', 14: 'truck', 15: 'trackbed', 16: 'on_rails', 17: 'rail_raised', 18: 'rail_embedded', 19: 'background'}\n","[ 0  1  2  3  4  5  7  9 10 11 12] [16 18]\n","[ 0  1  2  3  4  5  7  9 10 11 12] [16 18]\n","[ 4  5  8  9 10] [16 18]\n","[ 0  2  3  4  7  8 10 11 12] [16]\n","[ 0  1  2  3  4  5  7  8 10 11 12] [16 18]\n","epoch\n","inside\n","val dataloder: 8\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3b721f01794c8e9ee5bd09888f92c6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 117.317\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 112.351\n","\n","Patch List is empty.\n","Extract patches\n","end\n","5\n","0 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","1 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","2 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","3 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","4 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","Extract patches\n","end\n","10\n","0 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","1 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","2 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","3 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","4 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","5 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","6 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","7 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","8 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","9 dict_keys(['image', 'mask']) torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 118.926\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 119.765\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 112.169\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 111.390\n","\n","my collate\n","collate\n","torch.Size([2, 3, 512, 512])\n","torch.Size([2, 512, 512])\n","Val Loss : 118.681\n","\n","my collate\n","collate\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 512, 512])\n","Val Loss : 122.186\n","\n","[INFO]: Logging Validation Images, on device: 0\n","5\n","log_selected_indices_data before cat\n","5 torch.Size([3, 512, 512])\n","5 torch.Size([512, 512])\n","5 torch.Size([512, 512])\n","log_selected_indices_data\n","(5, 3, 512, 512)\n","(5, 512, 512)\n","(5, 512, 512)\n","wandbmasklog id2label {0: 'road', 1: 'sidewalk', 2: 'construction', 3: 'tram_track', 4: 'fence', 5: 'pole', 6: 'traffic_light', 7: 'traffic_sign', 8: 'vegetation', 9: 'terrain', 10: 'sky', 11: 'human', 12: 'rail_track', 13: 'car', 14: 'truck', 15: 'trackbed', 16: 'on_rails', 17: 'rail_raised', 18: 'rail_embedded', 19: 'background'}\n","[ 0  1  2  3  4  5  7  9 10 11 12] [16 18]\n","[ 1  2  3  4  5  7  9 10 11 12] [16 18]\n","[ 2  4  5 10] [16 18]\n","[ 0  1  2  3  4  7  8 10 11 12] [16 18]\n","[ 0  1  2  3  4  5  7  8 10 11 12] [16]\n","[INFO] Finishing the run....\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad0d0168b16406ebd88d3c1f9926b22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val_step_loss</td><td>▁█</td></tr><tr><td>custom_step</td><td>▁█</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val_step_loss</td><td>122.18597</td></tr><tr><td>custom_step</td><td>16</td></tr><tr><td>epoch</td><td>1</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /content/wandb/offline-run-20240227_091614-tbb08rkz<code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/offline-run-20240227_091614-tbb08rkz/logs</code>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[INFO] Run Finished....\n","[INFO]: Syncing /content/wandb/offline-run-20240227_091614-tbb08rkz\n","Removing /content/wandb/offline-run-20240227_091614-tbb08rkz....\n"]}]},{"cell_type":"markdown","source":["We use the open-source CLIP (Radford et al.,\n","2021) with ViT-B/32 (Dosovitskiy et al., 2020) as\n","the teacher model. Its image encoder is a 12-layer\n","ViT with the hidden size to be 768 and 12 attention\n","heads. Its text encoder is a 12-layer Transformer\n","with hidden size to be 512 and 8 attention heads. (weughts tranferring)\n","-------------------------------\n","For the student model, we use ViT-S/16 with\n","hidden size to be 384 as the image encoder, and initialize it from the pre-trained weights on ImageNet21K (Ridnik et al., 2021).\n","For the text encoder,we experiment with 2, 4 and 6-layer Transformer,\n","of which the weights are initialized from the first corresponding layers of the teacher’s text encoder."],"metadata":{"id":"gMpNkyT2cRPH"}},{"cell_type":"code","source":["## Imports\n","from typing import Tuple\n","import torch\n","from torch import nn\n","from transformers import CLIPTextModelWithProjection, AutoModelForUniversalSegmentation\n","from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel, RobertaConfig, RobertaModel, RobertaEncoder\n","from torch.nn import CrossEntropyLoss, CosineEmbeddingLoss\n","import clip\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"cf8wBpjtWV0F","executionInfo":{"status":"aborted","timestamp":1709023732022,"user_tz":-60,"elapsed":10,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create in oneformertrainer\n","def distill_oneformertextencoder(self.model):\n","    #get the text mapper\n","    student_model = self.model.model.text_mapper\n","    #use the model config to change the text_mapper configs\n","    self.model.config.text_encoder_num_layers = 2  # Change the number of layer to 2 instead of 6\n","    self.model.config.text_encoder_width = 512 #to match the clips embedding dimension , suhc that attention heads = 512 // 64 = 8 for each layer\n","    # Return the student model\n","    return student_model\n","\n","## Distllator class\n","class TextDistillator(nn.Module):\n","\n","    \"\"\"\n","    A class to distillate a BERT-like model.\n","    \"\"\"\n","    #self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs\n","\n","    def __init__(self, teacher_model, student_model, temperature: float = 1.0, training=False) -> None:\n","        \"\"\"\n","        Initiates the TextDistillator with the (teacher_model) to distillate from.\n","        \"\"\"\n","        super(TextDistillator, self).__init__()\n","        self.student = student_model\n","        self.training = training\n","        self.temperature = temperature\n","        self.teacher = teacher_model\n","        # place teacher on same device as student\n","        self.teacher.to(self.student.device)\n","        self.teacher.eval()\n","\n","    @property\n","    def temperature(self) -> float:\n","        return self._temperature if self.training else 1 #1 for inference\n","\n","    @temperature.setter\n","    def temperature(self, value : float) -> None:\n","        if value < 1:\n","            raise(ValueError(f\"Temperature must be above 1, it cannot be {value}\"))\n","        self._temperature = value\n","\n","    def get_logits(self, input_ids: Tensor=None, attention_mask: Tensor=None, from_teacher: bool = False) -> Tensor:\n","        \"\"\"\n","        Given a couple of (input_ids) and (attention_mask), returns the logits corresponding to the prediction.\n","        The logits come from the student unless (from_teacher) is set to True, then it's from the teacher.\n","        \"\"\"\n","        if from_teacher:\n","            return self.teacher.classifier(self.roberta(input_ids, attention_mask)[0])\n","        return self.student.classifier(self.student(input_ids, attention_mask)[0])\n","\n","    def forward(self, input_ids : Tensor, attention_mask : Tensor, labels : Tensor) -> Tuple[Tensor, Tensor]:\n","        \"\"\"\n","        Given a couple of (input_ids) and (attention_mask), returns the logits corresponding to the prediction.\n","        Also takes in the (labels) associated to the inputs.\n","        Returns the student probability distibution with temperature 1 and the loss.\n","        \"\"\"\n","        student_logits = self.get_logits(input_ids, attention_mask, False)\n","        teacher_logits = self.get_logits(input_ids, attention_mask, True)\n","        return student_logits.softmax(1), self.loss(teacher_logits, student_logits, labels)\n","\n","    def compute_loss(self, teacher_logits, student_logits, return_outputs=False):\n","        student_output = self.student(**inputs)\n","\n","        with torch.no_grad():\n","          teacher_output = self.teacher(**inputs)\n","\n","        # Compute soft targets for teacher and student\n","        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n","        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n","\n","        # Compute the loss\n","        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n","\n","        # Compute the true label loss\n","        student_target_loss = student_output.loss\n","\n","        # Calculate final loss\n","        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss\n","        return (loss, student_output) if return_outputs else loss\n","\n","    # def loss(self,\n","    #     teacher_logits : Tensor,\n","    #     student_logits : Tensor,\n","    #     labels : Tensor,\n","    # ) -> Tensor:\n","    #     \"\"\"\n","    #     The distillation loss for distilating a BERT-like model.\n","    #     The loss takes the (teacher_logits), (student_logits) and (labels) for various losses.\n","    #     \"\"\"\n","    #     # Temperature and sotfmax\n","    #     student_logits, teacher_logits = (student_logits / self.temperature).softmax(1), (teacher_logits / self.temperature).softmax(1)\n","    #     # Classification loss (problem-specific loss)\n","    #     loss = CrossEntropyLoss()(student_logits, labels)\n","    #     # CrossEntropy teacher-student loss\n","    #     loss = loss + CrossEntropyLoss()(student_logits, teacher_logits)\n","    #     # Cosine loss\n","    #     loss = loss + CosineEmbeddingLoss()(teacher_logits, student_logits, torch.ones(teacher_logits.size()[0]))\n","    #     # Average the loss and return it\n","    #     loss = loss / 3\n","    #     return loss"],"metadata":{"id":"SWsOttxdWVYc","executionInfo":{"status":"aborted","timestamp":1709023732022,"user_tz":-60,"elapsed":9,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]}]}