{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzvX7tGN9zscHIzIe0O+ZO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install langchain faiss-cpu openai transformers pdfplumber pytorch torchvision"],"metadata":{"id":"rBFWV7CBFe6w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To interact with PDFs (text, images, and tables) using open-source tools. This will walk you through setting up a system where the chatbot can answer questions based on the content of PDFs, including text and images, and tables.\n","\n","\n","1. **Extract Text and Images from PDF**: Use `pdfplumber` for text and `Pillow` for images.\n","2. **Store the Data**: We’ll use `FAISS` to store text and image features.\n","3. **Use a Pre-trained Language Model (LLM)**: We will use OpenAI’s `gpt-3.5-turbo` (or an equivalent free model from HuggingFace) for answering questions.\n","4. **RAG Setup**: We will use `LangChain` to implement the RAG-based pipeline.\n"],"metadata":{"id":"KVUDkmahFpIa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjDOgFYhFXjZ"},"outputs":[],"source":["### 1. Extract Text, Images, and Tables from PDF\n","\n","import pdfplumber\n","\n","def extract_text_and_tables(pdf_path):\n","    text = \"\"\n","    tables = []\n","\n","    with pdfplumber.open(pdf_path) as pdf:\n","        for page in pdf.pages:\n","            text += page.extract_text()  # Extracting text from the page\n","            tables.extend(page.extract_tables())  # Extracting tables from the page\n","\n","    return text, tables"]},{"cell_type":"code","source":["# Test: Use it to extract content from a PDF\n","pdf_path = 'your_file.pdf'  # Replace with your PDF file path\n","text, tables = extract_text_and_tables(pdf_path)\n","\n","print(text)  # Preview the extracted text\n","print(tables)  # Preview the extracted tables"],"metadata":{"id":"e5nU6K1yGhPQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Extracting Images**:"],"metadata":{"id":"IZjSG9xCGlRj"}},{"cell_type":"code","source":["from PIL import Image\n","\n","def extract_images(pdf_path):\n","    images = []\n","    with pdfplumber.open(pdf_path) as pdf:\n","        for page in pdf.pages:\n","            for img in page.images:\n","                im = Image.open(io.BytesIO(img['stream']))\n","                images.append(im)\n","    return images"],"metadata":{"id":"9RNIlEq2GWA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example: Extract images\n","images = extract_images(pdf_path)\n","\n","# Show the first extracted image\n","images[0].show()"],"metadata":{"id":"ZsxyWZngGpdK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use `FAISS` to store text embeddings and images for efficient search. First, we’ll extract text embeddings using a pretrained language model and image embeddings using a vision model.\n","\n","**Storing Text & Image Embeddings**:"],"metadata":{"id":"x_uizrhXGfMq"}},{"cell_type":"code","source":["### 2. Storing Data Using FAISS for Efficient Search\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from transformers import CLIPProcessor, CLIPModel\n","import torch\n","\n","# Function to get text embeddings\n","def get_text_embeddings(texts):\n","    model = HuggingFaceEmbeddings()\n","    return model.embed_documents(texts)\n","\n","# Function to get image embeddings\n","def get_image_embeddings(images):\n","    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","\n","    image_inputs = processor(images=images, return_tensors=\"pt\")\n","    with torch.no_grad():\n","        image_embeddings = model.get_image_features(**image_inputs)\n","\n","    return image_embeddings\n"],"metadata":{"id":"AGSGYzorGsXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test: Create embeddings for text and images\n","texts = [text]  # Assuming 'text' is a string of extracted text from the PDF\n","images_embeddings = get_image_embeddings(images)\n","\n","# Combine text and image embeddings\n","texts_embeddings = get_text_embeddings(texts)\n","\n","# Storing in FAISS\n","text_faiss = FAISS.from_documents(texts, HuggingFaceEmbeddings())\n","image_faiss = FAISS.from_vectors(images_embeddings, dimension=512)  # CLIP output is 512-d\n","\n","# You can store both and search them separately based on the type of query."],"metadata":{"id":"LEmqqraAGYVF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let’s integrate this with a RAG approach using LangChain. This will allow the chatbot to fetch relevant information from both text and images to answer a query.\n"],"metadata":{"id":"222bij5OGPVc"}},{"cell_type":"code","source":["### 3. Implementing RAG with LangChain\n","from langchain.chains import RetrievalQA\n","from langchain.chains import RAG\n","from langchain.llms import OpenAI\n","from langchain.agents import initialize_agent\n","from langchain.agents import Tool\n","from langchain.chat_models import ChatOpenAI\n","\n","# You can use HuggingFace’s free models for this\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n","\n","# Use FAISS as a retriever for both text and image data\n","qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=text_faiss.as_retriever())\n","\n","def multimodal_rag(query):\n","    # Fetch relevant results for the query from text and images\n","    text_results = qa_chain.run(query)\n","\n","    # Optionally add image search and retrieval logic here\n","    # For example, using the image_faiss search to match query related to images\n","\n","    return text_results  # Returning text response"],"metadata":{"id":"IFx8ALlqF4ae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test with a query\n","query = \"What are the key insights in the PDF?\"\n","response = multimodal_rag(query)\n","print(response)\n"],"metadata":{"id":"XQBta2Z5GKuP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### 4. Bringing It All Together\n","# Full process to extract data from PDF, process it, and query the chatbot.\n","pdf_path = 'your_file.pdf'  # Replace with the path to your PDF\n","\n","# Step 1: Extract text, tables, and images\n","text, tables = extract_text_and_tables(pdf_path)\n","images = extract_images(pdf_path)\n","\n","# Step 2: Create embeddings for text and images\n","texts = [text]  # List of texts to pass for embedding\n","texts_embeddings = get_text_embeddings(texts)\n","images_embeddings = get_image_embeddings(images)\n","\n","# Step 3: Store embeddings in FAISS\n","text_faiss = FAISS.from_documents(texts, HuggingFaceEmbeddings())\n","image_faiss = FAISS.from_vectors(images_embeddings, dimension=512)\n","\n","# Step 4: Define and run multimodal RAG-based chatbot\n","query = \"What are the key insights in the PDF?\"\n","response = multimodal_rag(query)\n","\n","# Print the final response from the chatbot\n","print(response)"],"metadata":{"id":"ZLUzmzzLGEE0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This project setup extracts text, images, and tables from PDFs and uses `FAISS` for efficient data retrieval. You can extend the project to search and use both text and image data for answering questions in a multimodal fashion.\n","\n","Remember:\n","\n","1. **Text Embedding**: We used HuggingFace's pre-trained transformer models.\n","2. **Image Embedding**: We used CLIP (a vision model from OpenAI) to get image features.\n","3. **RAG Model**: We used LangChain’s `RetrievalQA` to implement a retrieval-augmented generation system that helps answer questions based on the available text and image data.\n"],"metadata":{"id":"9Fh1J1-CGGib"}}]}