{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOw1hOpIAhH391f0lzgkvH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Seq2Seq = Framework\n","\n","Seq2Seq (Sequence-to-Sequence) is a high-level architecture, not a specific model. It’s defined by:\n","\n","- **Encoder**: processes input sequence into a fixed representation.\n","- **Decoder**: generates the output sequence from that representation.\n","\n","**What powers the encoder/decoder in Seq2Seq?**\n","\n","They can be built using different kinds of layers:\n","\n","1. RNN-based Seq2Seq (RNN encoder and RNN decoder)\n","2. LSTM-based Seq2Seq\n","3. GRU-based Seq2Seq\n","4. Transformer-based Seq2Seq\n","\n","All these are building blocks for the encoder and decoder in Seq2Seq models."],"metadata":{"id":"fqog7i-KR184"}},{"cell_type":"markdown","source":["**Umbrella view of Seq2Seq and related architectures**\n","\n","---\n","\n","### 1. **Core Concepts**\n","\n","* **Sequence-to-Sequence (Seq2Seq) Architecture**\n","\n","  * Encoder\n","  * Decoder\n","  * Context vector (or intermediate representation)\n","  * Teacher Forcing\n","  * Auto-regressive decoding\n","\n","---\n","\n","### 2. **Recurrent-Based Models**\n","\n","Used as early Seq2Seq encoders and decoders.\n","\n","#### Basic Units\n","\n","* RNN (Recurrent Neural Network)\n","    * Vanilla RNN - with recurrent connections\n","    * Bidirectional RNN(BiRNN)\n","    * Stacked RNN\n","    * Residual RNN\n","* LSTM (Long Short-Term Memory)\n","    * Vanilla LSTM\n","        - standard LSTM cell with forget, input, and output gates and a cell state\n","    * Peephole LSTM\n","        - adds connections from the cell state to gates(peephole) for better precision\n","    * Coupled Gate LSTM\n","        - simplifies LSTM by coupling input and forget gates\n","    * BiLSTM\n","        - processes input from both directions (seq left2right and right2left) (only used in encoders)\n","* GRU (Gated Recurrent Unit)'\n","    * Vanilla GRU\n","    * BiGRU\n","\n","\n","#### Seq2Seq Encoder-Decoder Recurrent Variants\n","\n","* RNN Encoder → RNN Decoder\n","* LSTM Encoder → LSTM Decoder\n","* GRU Encoder → GRU Decoder\n","* Bi-directional RNN/LSTM/GRU Encoder\n","* Attention-enhanced RNNs\n","\n","---\n","\n","### 3. **Attention Mechanisms**\n","\n","Solves the bottleneck of fixed-size context vector in classic Seq2Seq\n","\n","* General Attention\n","    - Weights encoder hidden states based on decoder state\n","        - Bahdanau (Additive) Attention\n","        - Luong (Multiplicative) Attention\n","* Self-Attention\n","    - Compares every token with every other token in the sequence (used in Transformers)\n","\n","---\n","\n","\n"],"metadata":{"id":"wRFfgffhP3qg"}},{"cell_type":"markdown","source":["### 4. **Transformer-Based Architectures**\n","\n","Modern alternative to RNNs. Use self-attention instead of recurrence. Fully parallelizable. Backbone for BERT, GPT, T5.\n","\n","* Transformer (Encoder-Decoder)\n","* Multi-Head Self-Attention\n","* Positional Encoding\n","\n","**Encoder-Only**\n","- BERT\n","- RoBERTa\n","\n","**Decoder-Only**\n","\n","- GPT, GPT-2, GPT-3, GPT-4\n","\n","**Encoder-Decoder**\n","\n","- T5, BART, MarianMT\n","\n","---\n","\n","### 5. **Language Models & Variants**\n","Predict next word or generate text\n","\n","* Traditional LMs\n","    - N-gram Models\n","\n","* Neural LMs\n","    - RNN-LM\n","    - LSTM-LM\n","    - GRU-LM\n","    - Transformer LMs (e.g., GPT)\n","\n","* Types by Behavior\n","- Autoregressive: GPT(Generative Pretrained Transformer), RNN-LM\n","- Masked: BERT(Bidirectional Encoder Representations from Transformers)\n","- Seq2Seq-style: T5, BART\n","\n","---\n","\n","### 6. **Autoencoders**\n","\n","Encode and reconstruct input, not necessarily for sequence generation. Can be extended for sequences. Unsupervised encoders for learning representations; used in both vision and language.\n","\n","* Vanilla Autoencoders (dense)\n","* Sequence Autoencoders\n","* Variational Autoencoders (VAEs)\n","* Denoising Autoencoders\n","\n","---\n","\n","### 7. **Specialized or Extended Architectures**\n","\n","For advanced or niche tasks (e.g., ASR, memory handling).\n","\n","* Pointer Networks\n","* Copy Mechanisms\n","* Memory Networks\n","* Recursive Neural Networks (TreeRNNs)\n","* RNN-Transducer (used in ASR, speech recognition)\n","* Hybrid CNN-RNN models\n","\n","---\n","\n"],"metadata":{"id":"F-9ENXihVuly"}},{"cell_type":"markdown","source":["```\n","Seq2Seq\n","├── Encoder-Decoder\n","│   ├── RNN\n","│   ├── LSTM\n","│   │   ├── Vanilla\n","│   │   ├── Coupled Gate\n","│   │   ├── Peephole\n","│   │   └── BiLSTM\n","│   ├── GRU\n","│   └── Attention\n","│       ├── General (Bahdanau, Luong)\n","│       └── Self-Attention\n","├── Transformer (Attention-based)\n","│   ├── BERT (Encoder)\n","│   ├── GPT (Decoder)\n","│   └── T5, BART (Encoder-Decoder)\n","└── Related\n","    ├── Autoencoders\n","    └── Variational Autoencoders\n","```"],"metadata":{"id":"npFmV84gPhej"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_OzI51xNTlB"},"outputs":[],"source":[]}]}