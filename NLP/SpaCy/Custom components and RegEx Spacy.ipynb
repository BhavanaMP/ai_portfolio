{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35cc64dc-e06a-4ccc-9699-185211e4e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc9fd58-e05b-479b-93a8-a422b948ebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain GPE\n",
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Britain is a place. Mary is a Doctor.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca6aca-7878-4e9f-bc0d-b8f342f174d6",
   "metadata": {},
   "source": [
    "Like off the shelf entity_ruler, sentencizer components, we can create our custom component.\n",
    "Lets say, we want to chnage all GPEs to LOC or we want to remove all GPE's. In such case, off the shelf components are not much usable and we can use custom components using <b>@Languge.component</b> decorator in spacy and add it to spacy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb57de1d-e009-4d6a-917b-06360c1ef82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"remove_gpe\")\n",
    "def remove_gpe(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            original_ents.remove(ent)\n",
    "    doc.ents = original_ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c085bf0a-51c7-4198-8d97-de03c964d309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.remove_gpe(doc)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"remove_gpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a324996-f5ae-4acc-9453-0b62aa3659c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'remove_gpe': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'remove_gpe': []},\n",
       " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b7bd79-8f95-4e0a-8514-6a9fd5ec47fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Britain is a place. Mary is a Doctor.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d45ce85c-998a-4893-b80b-8083af0884b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.to_disk(\"data/new_en_core_web_sm\") # to save this updated model to the disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d0735-beea-427a-99ba-bd18d2624b57",
   "metadata": {},
   "source": [
    "### Regex in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c1ef0f2-5aae-4d0a-828e-02e8d6568d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2 February', '2', 'February'), ('14 August', '4', 'August')]\n"
     ]
    }
   ],
   "source": [
    "#usual regex to find patterns\n",
    "import re\n",
    "\n",
    "text = \"This is a date 2 February. Another date would be 14 August.\"\n",
    "\n",
    "pattern = r\"((\\d){1,2} (January|February|March|April|May|June|July|August|September|October|November|December))\"\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db3bc1-c730-4763-9247-9c6b0ff6d22b",
   "metadata": {},
   "source": [
    "In this bit of code, we see a real-life RegEx formula at work. While this looks quite complex, its syntax is fairly straight forward. Let’s break it down. The first ( tells RegEx that I’m looking for something within the ending ). In other words, I’m looking for a pattern that’s going to match the whole pattern, not just components.\n",
    "\n",
    "Next, we state (\\d){1,2}. This means that we are looking for any digit (0-9) that occurs either once or twice ({1,2}).\n",
    "\n",
    "Next, we have a space to indicate the space in the string that we would expect with a date.\n",
    "\n",
    "Next, we have (January|February|March|April|May|June|July|August|September|October|November|December) – this indicates another component of the pattern (because it is parentheses). The | indicates the same concept as “or” in English, so either January, or February, or March, etc.\n",
    "\n",
    "When we bring it together, this pattern will match anything that functions as a set of one or two numbers followed by a month. What happens when we try and do this with a date that is formed the opposite way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d26d781e-a4b2-46a9-9bf7-9cd0fc1a8343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('14 August', '4', 'August')]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4cefd-1c38-4920-9a22-bf2227dd7449",
   "metadata": {},
   "source": [
    "It fails. But this is no fault of RegEx. Our pattern cannot accommodate that variation. Nevertheless, we can account for it by adding it as a possible variation. Possible variations are accounted for with a *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c87235e-293f-4418-a02c-a21d745dfe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('February 2', '', '', '', '', 'February 2', 'February ', 'February', '2'), ('14 August', '14 August', '4', ' August', 'August', '', '', '', '')]\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"(((\\d){1,2}( (January|February|March|April|May|June|July|August|September|October|November|December)))|(((January|February|March|April|May|June|July|August|September|October|November|December) )(\\d){1,2}))\"\n",
    "\n",
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d0e2d2-56b7-4966-ba6f-e151f51081ce",
   "metadata": {},
   "source": [
    "There are more concise ways to write the same RegEx formula. I have opted here to be more verbose to make it a bit easier to read. You can see that we’ve allowed for two main options for our pattern matcher.\n",
    "\n",
    "Notice, however, that we have a lot of superfluous information for each match. These are the components of each match. There are several ways we can remove them. One way is to use the command finditer, rather than findall in RegEx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "147b9f12-fed4-4f43-bc3e-aa7714be6c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x000002BC55F60F70>\n",
      "<re.Match object; span=(15, 25), match='February 2'>\n",
      "<re.Match object; span=(49, 58), match='14 August'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"(((\\d){1,2}( (January|February|March|April|May|June|July|August|September|October|November|December)))|(((January|February|March|April|May|June|July|August|September|October|November|December) )(\\d){1,2}))\"\n",
    "\n",
    "text = \"This is a date February 2. Another date would be 14 August.\"\n",
    "matches = re.finditer(pattern, text) \n",
    "print(matches) # returns iterable\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebcb59-6175-4888-890f-73efaeba8260",
   "metadata": {},
   "source": [
    "Within each of these is some very salient information, such as the start and end location (inside the span) and the text itself (match). We can use the start and end location to grab the text within the string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66ec6b-13f8-4a84-bc48-9c26bd53beaf",
   "metadata": {},
   "source": [
    "### How to Use RegEx in spaCy\n",
    "Things like dates, times, IP Addresses, etc. that have either consistent or fairly consistent structures are excellent candidates for RegEx. Fortunately, spaCy has easy ways to implement RegEx in three pipes: Matcher, PhraseMatcher, and EntityRuler. One of the major drawbacks to the Matcher and PhraseMatcher, is that they do not align the matches as doc.ents. Because this notebook is about NER and our goal is to store the entities in the doc.ents, we will focus on using RegEx with the EntityRuler. In the next notebook, we will examine other methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22cb6e73-82c6-4f92-b921-03dd1d5fb47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample number 555-5555.\"\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [\n",
    "    {\"label\": \"PHONE_NUMBER\", \n",
    "     \"pattern\": [\n",
    "         {\"SHAPE\": \"ddd\"},\n",
    "         {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
    "         {\"SHAPE\": \"dddd\"}\n",
    "     ]\n",
    "    }\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af2cc66-b36d-4b8a-b413-1234a9eeafcc",
   "metadata": {},
   "source": [
    "This method worked well for grabbing the phone number. But what if we wanted to use RegEx as opposed to linguistic features, such as shape? First, let’s write some RegEx to capturee 555-5555."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2adf6109-cc4d-4829-8970-5d3687d669c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('555-5555', '5', '5')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"This is a sample number 555-5555.\"\n",
    "pattern = r\"((\\d){3}-(\\d){4})\"\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abbf74-04d7-49b5-92a3-3ff5074cb357",
   "metadata": {},
   "source": [
    "Okay. So, now we know that we have a RegEx pattern that works. Let’s try and implement it in the spaCy EntityRuler.When we execute the code below, we have no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1990c67-d432-4d46-9aa0-f0dfe0208309",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample number 555-5555.\"\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [\n",
    "    {\"label\": \"PHONE_NUMBER\", \n",
    "     \"pattern\": [\n",
    "         {\"TEXT\": {\"REGEX\": \"((\\d){3}-(\\d){4})\"}}\n",
    "     ]\n",
    "    }\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedb885-aedd-41da-b339-17ee940cc197",
   "metadata": {},
   "source": [
    "This is for one very important reason. SpaCy’s EntityRuler cannot use RegEx to pattern match across tokens. The dash in the phone number throws off the EntityRuler. So, what are we to do in this scenario? Well, we have a few different options that we will explore next. But before we get to that, let’s try and use RegEx to capture the phone number with no hyphen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eea057db-b44d-4e26-8188-33a9b2182bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5555555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample number 5555555.\"\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "    {\"label\": \"PHONE_NUMBER\", \n",
    "     \"pattern\": [\n",
    "         {\"TEXT\": {\"REGEX\": \"((\\d){5})\"}}\n",
    "     ]\n",
    "    }\n",
    "]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bf3c5e-ac21-41f9-aa0a-0b9813efc1ec",
   "metadata": {},
   "source": [
    "Notice that without the dash and a few modifications to our RegEx, we were able to capture 5555555 because this is a single token in the spaCy doc object. Let’s explore how to solve the multi-token problem in the spacy entity ruler!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6690ffe4-177a-43d3-8c19-247eca47c9b2",
   "metadata": {},
   "source": [
    "### Problems with Multi-Word Tokens in spaCy as Entities\n",
    "As we saw before from Rules-Based NER, we can use spaCy’s Matcher to grab multi-word tokens, or tokens that span multiple tokens. The main problem with this, however, is that these multi-word tokens are not placed into the doc.ents. This means that we cannot access them the same way we would like other entities. Now, we will figure out how to solve that problem with a simple workflow:\n",
    "\n",
    "- Extract Multi-Word Tokens with re.finditer()\n",
    "- Reconstruct the spans in the spaCy doc\n",
    "- Give priority to longer spans (Optional)\n",
    "- Inject the Spans into doc.ents\n",
    "\n",
    "We will cover each of these steps in turn.\n",
    "\n",
    "### Extract Multi-Word Tokens\n",
    "First, we need to grab the multi-word tokens. In this case, a person whose first name begins with Paul. In the RegEx below, we specify that we are looking for any string that starts with “Paul” and then is followed by a capitalized letter[A-Z]. We then tell it to grab the entire second word until the end of the word(\\w+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99682faa-580a-4eab-80b7-ab4ab0391c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
      "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "matches = re.finditer(pattern, text)\n",
    "for hit in matches:\n",
    "    print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106da5c-55ae-4ac6-91b4-3625b6aa2429",
   "metadata": {},
   "source": [
    "Note that we have not grabbed the final “Paul” which is not followed by a last name. In this case, we are not interested in that Paul. Now that we know how to grab the multi-word tokens, we need to have a way to parse them in spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdedeba-459a-47de-acaf-d467bfbbc812",
   "metadata": {},
   "source": [
    "### Reconstruct Spans\n",
    "This next stage is a bit more complicated, but works quite well once you understand the process. First, we need to import the libraries we will need. Note that we are also adding Span from spacy.tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16521dae-32e6-4656-a37c-7ecdd1914560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_ents: []\n",
      "Token Span: (Paul Newman, 0, 2, 'Paul Newman')\n",
      "Token Span: (Paul Hollywood, 8, 10, 'Paul Hollywood')\n",
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spacy.tokens import Span\n",
    "\n",
    "#re span are character spans but spacy nlp doc object needs token span..thats what we do now\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "matches = re.finditer(pattern, text)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(text)\n",
    "original_ents = list(doc.ents)\n",
    "print(f\"original_ents: {original_ents}\")  # empty cuz of blank spacy language model\n",
    "multi_word_ents = []\n",
    "for hit in matches:\n",
    "    start, end = hit.span()  # char span\n",
    "    # reconstruct span\n",
    "    span = doc.char_span(start, end, label=\"PERSON\") # token span, observe the start and end idx,it is token wise now\n",
    "    print(f\"Token Span: {span, span.start, span.end, span.text}\")\n",
    "    if span is not None:\n",
    "        multi_word_ents.append((span.start, span.end, span.text))\n",
    "# Inject the Spans into the doc.ents\n",
    "for mwt_ent in multi_word_ents:\n",
    "    start, end, text = mwt_ent\n",
    "    per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "    original_ents.append(per_ent)\n",
    "\n",
    "doc.ents = original_ents # we can only attach Span objects to the doc.ents\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a84393-16b1-4f25-940b-8e7cc8873f47",
   "metadata": {},
   "source": [
    "### Creating a custom component with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3933e700-0977-4651-99e1-218de01122e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"paul_ent\")\n",
    "def paul_ent(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    #mwt_ents = []\n",
    "    pattern = r\"Paul [A-Z]\\w+\"\n",
    "    matches = re.finditer(pattern, doc.text)\n",
    "    for hit in matches:\n",
    "        start, end = hit.span()\n",
    "        span = doc.char_span(start, end, label=\"PERSON\")\n",
    "        if span is not None:\n",
    "            mwt_ent = Span(doc, span.start, span.end, label=\"PERSON\")\n",
    "            original_ents.append(mwt_ent)\n",
    "    doc.ents = original_ents\n",
    "    return doc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3df3387d-7880-4e04-855e-5ddff5b71e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"paul_ent\")\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092ab61-3c78-4078-a4be-cdc58186a14b",
   "metadata": {},
   "source": [
    "### Give priority to Longer Spans\n",
    "Sometimes, the situation is not so neat. Sometimes our custom RegEx entities will overlap with spaCy’s Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3996bd6c-3812-4ba3-8346-588fb5587a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "American NORP\n",
      "Paul Hollywood PERSON\n",
      "British NORP\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host.\"\n",
    "pattern = r\"Hollywood\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc783be-e060-42c1-8412-d776b424c580",
   "metadata": {},
   "source": [
    "Let’s say that we create a new entity. Maybe words associated with Cinema. So, we want to classify Hollywood as a tag “CINEMA”. Now, in the above text, Hollywood is clearly associated with Paul Hollywood, but let’s imagine for a moment that it is not. Let’s try and run the same code as above. If we do, we notice that we get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7142a5f4-d1b5-4739-a894-87f2c5240d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(44, 53), match='Hollywood'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 9 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     per_ent \u001b[38;5;241m=\u001b[39m Span(doc, start, end, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCINEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     original_ents\u001b[38;5;241m.\u001b[39mappend(per_ent)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ments\u001b[49m \u001b[38;5;241m=\u001b[39m original_ents\n",
      "File \u001b[1;32mE:\\Learning\\ML\\Learning Practice\\Code\\NLP\\SpaCy\\NLP\\lib\\site-packages\\spacy\\tokens\\doc.pyx:798\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mE:\\Learning\\ML\\Learning Practice\\Code\\NLP\\SpaCy\\NLP\\lib\\site-packages\\spacy\\tokens\\doc.pyx:835\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 9 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host.\"\n",
    "pattern = r\"Hollywood\"\n",
    "\n",
    "mwt_ents = []\n",
    "original_ents = list(doc.ents)\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    print(match)\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start, span.end, span.text))\n",
    "for ent in mwt_ents:\n",
    "    start, end, name = ent\n",
    "    per_ent = Span(doc, start, end, label=\"CINEMA\")\n",
    "    original_ents.append(per_ent)\n",
    "\n",
    "doc.ents = original_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86457d4-b060-4813-a2b8-a088cee67678",
   "metadata": {},
   "source": [
    "This error tells us that one of our tokens from the finditer() overlapped with one that our “ner” component found. This is a problem that can be rectified with spaCy’s filter_spans. This gives priority to longer spans. Notice how we have allowed the Paul Hollywood entity to be a PERSON, rather than CINEMA. This is because Hollywood is shorter than Paul Hollywood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57d3aecf-28a2-451d-be45-f95a4fbf05e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(44, 53), match='Hollywood'>\n",
      "Paul Newman PERSON\n",
      "American NORP\n",
      "Paul Hollywood PERSON\n",
      "British NORP\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host.\"\n",
    "pattern = r\"Hollywood\"\n",
    "\n",
    "mwt_ents = []\n",
    "original_ents = list(doc.ents)\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    print(match)\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start, span.end, span.text))\n",
    "for ent in mwt_ents:\n",
    "    start, end, name = ent\n",
    "    per_ent = Span(doc, start, end, label=\"CINEMA\")\n",
    "    original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "\n",
    "doc.ents = filtered\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17cf0c-8b1f-419c-b45d-56bd193e3625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
