{"cells":[{"cell_type":"code","source":["import torch\n","torch.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"vp0POcEoXjKC","executionInfo":{"status":"ok","timestamp":1705678646482,"user_tz":-60,"elapsed":12537,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"7dbbb4b1-dd12-4ed7-ce60-12275a194a43"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.1.0+cu121'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38112,"status":"ok","timestamp":1705596386100,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"},"user_tz":-60},"id":"LHkUoP_ZOeao","outputId":"f2eac150-5403-4684-8a8d-410b4b866c27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting datasets\n","  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n","Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting timm\n","  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.16.2-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft\n","  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.9.0.80)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.39.2-py2.py3-none-any.whl (254 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.12.9)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Installing collected packages: torchinfo, smmap, setproctitle, sentry-sdk, lightning-utilities, docker-pycreds, dill, responses, multiprocess, gitdb, torchmetrics, GitPython, accelerate, wandb, timm, datasets, peft, evaluate\n","Successfully installed GitPython-3.1.41 accelerate-0.26.1 datasets-2.16.1 dill-0.3.7 docker-pycreds-0.4.0 evaluate-0.4.1 gitdb-4.0.11 lightning-utilities-0.10.0 multiprocess-0.70.15 peft-0.7.1 responses-0.18.0 sentry-sdk-1.39.2 setproctitle-1.3.3 smmap-5.0.1 timm-0.9.12 torchinfo-1.8.0 torchmetrics-1.3.0.post0 wandb-0.16.2\n"]}],"source":["!pip install transformers datasets evaluate albumentations torchinfo torchmetrics accelerate timm wandb peft"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2111135,"status":"ok","timestamp":1705524677244,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"},"user_tz":-60},"id":"GnkEUBcgQJGR","outputId":"ece0a808-1119-4c84-900a-94b3851fbb21"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m655.1 kB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.9 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m1.8/1.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for natten (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["#natten is needed for dinat checkpoints\n","!pip3 install natten -f https://shi-labs.com/natten/wheels/cu113/torch1.10.1/index.html --quiet"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5334,"status":"ok","timestamp":1705596536340,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"},"user_tz":-60},"id":"f0AW7E2yOhXy"},"outputs":[],"source":["import datasets, transformers, evaluate, accelerate#, natten\n","from datasets import load_dataset\n","from transformers import AutoProcessor, AutoModelForUniversalSegmentation, OneFormerForUniversalSegmentation\n","from transformers import Trainer, TrainingArguments\n","from huggingface_hub import hf_hub_download\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import albumentations as A\n","from torch.optim import AdamW\n","from tqdm.auto import tqdm\n","import torchinfo, torchmetrics\n","\n","from pathlib import Path\n","import os\n","import requests\n","import zipfile\n","import json\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import wandb\n","from huggingface_hub import notebook_login"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"elapsed":10516,"status":"ok","timestamp":1705524719800,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"},"user_tz":-60},"id":"F-CsRbuKLs4j","outputId":"d654cf45-4261-4264-b1a8-79fe62c7bf68"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["wandb.login()\n","#27daaa857fb52f6c2ec3a90472f3d4dadf3adba9"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["91fcbd379a6e417c970154b6e1bd2d3e","c5ef533a0632448e9df7cf4f5365b4ca","9af46f20f4b14c87987e5c51a674b5bc","4ce183ee79bc45f1b25d151fe5f016f0","eb0363c842d347abb3d1a4569c13ce10","2580c216f6004fa19682167e84da56fd","b5023b041a3b4a0aa811199f81e76bdf","c29f5bb26ef341de801404875db2337f","6a9a4c0fb3d547acaadc11ccd9dd32f9","aefbe91b8f3b46c6accc1062a6c1ed41","da4d73b3f37b47d78a5a261626db1fd6","b01c198438ea430d993fa24ba249c341","a5a2ea8dad014a018bdbb06e9f5efdb2","cfcdf24ed8054b3eb631505c15315885","36c9fe3015124f5e96c26cb909516580","932857fecc924c01a9c508d1fca13346","db2867d55c504381b70d06e505339907","ba661e51c1b14efbaaafce8151f6d6f0","74ee7c50cd55495db07510ae70d927aa","95343f74a7c14d8e82e199c33f941520","1559eb2163974aa68f5403b7380b5747","fa1d7006f6f9463aba46e07829a7026b","080d6ad2a5f54e2eb442eba49f916dd4","72fbe645c1594252b9cfc06245dfb0cb","69036b9161564aff85ded31225ae879d","e16b58dc0ba945e980b4176e1d6d84f7","89cb6e3e172247f0a095aed16ee35eae","1f1ad0d18f554d799a17cc0c1160458e","0f70caad1fee4c2db91c93ff00634de3","2aee003dfc1f445b8fecb5e3e320b5f6","0d166fcbb93c4319b8d45811f2a96c46","1712bcf4e9a74a01b03dd2e5b3442ae3"]},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1705596541463,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"},"user_tz":-60},"id":"RjOzew5SLw0g","outputId":"facdbdf1-911b-403b-cb2e-da7417c4ac53"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91fcbd379a6e417c970154b6e1bd2d3e"}},"metadata":{}}],"source":["notebook_login()\n","#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"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1705530006714,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"},"user_tz":-60},"id":"D0GrAhZJTFXk"},"outputs":[],"source":["swin_checkpoint = \"shi-labs/oneformer_cityscapes_swin_large\"\n","dinat_checkpoint = \"shi-labs/oneformer_cityscapes_dinat_large\"\n","\n","checkpoint = swin_checkpoint\n","ckpt_name = \"Swin\"\n","\n","# checkpoint = dinat_checkpoint\n","# ckpt_name = \"Dinat\"\n","\n","dataset_name = \"Cityscapes\""]},{"cell_type":"code","source":["#Note: to do domain shift check, we first have to map the labels of railsem19 as per city scapes labeling\n","#railsem actual before\n","# ['buffer-stop','car','crossing','fence','guard-rail','person','person-group','platform','pole','rail','rail-occluder','switch-indicator',\n","#  'switch-left','switch-right','switch-static','switch-unknown','track-sign-front', 'track-signal-back','track-signal-front','train-car','truck'] - original railsem19 labels\n","\n","#railsem labels original after refining\n","#{0: 'road', 1: 'sidewalk', 2: 'construction', 3: 'tram-track', 4: 'fence',\n","# 5: 'pole', 6: 'traffic-light', 7: 'traffic-sign', 8: 'vegetation', 9: 'terrain',\n","# 10: 'sky', 11: 'human', 12: 'rail-track', 13: 'car', 14: 'truck', 15: 'trackbed',\n","# 16: 'on-rails', 17: 'rail-raised', 18: 'rail-embedded'}\n","\n","# city scapes oneformer_cityscapes_model.config.id2label\n","#{0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence', 5: 'pole', 6: 'traffic light', 7: 'traffic sign',\n","#8: 'vegetation', 9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car', 14: 'truck', 15: 'bus',\n","#16: 'train', 17: 'motorcycle', 18: 'bicycle'}\n","\n","#cityscapes labels in onformer processor\n","# {\"0\": \"road\",\"1\": \"building\", \"2\": \"sidewalk\", \"3\": \"wall\", \"4\": \"fence\", \"5\": \"pole\",\n","# \"6\": \"traffic light\", \"7\": \"traffic sign\", \"8\": \"vegetation\", \"9\": \"terrain\", \"10\": \"sky\",\n","# \"11\": \"person\", \"12\": \"rider\", \"13\": \"car\", \"14\": \"truck\", \"15\": \"bus\", \"16\": \"train\",\n","#  \"17\": \"motorcycle\",\"18\": \"bicycle\"}\n","\n","\n","#mapped rail labels according to\n","#Label RS19\n","#road sidewalk construction tramtrack fence pole trafficlight trafficsign vegetation terrain sky     human   railtrack car truck trackbed on-rails railraised railembedded void\n","#Label CS\n","#road sidewalk building+wall   -      fence pole trafficlight trafficsign vegetation terrain sky person+rider    *     car truck+bus -    on-rails     *        -       bi/motorcy.+void"],"metadata":{"id":"Te18AlG8qcAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["350f3749ea1c4efabc5328d6598dfd31","be59cd0267ee4b189bf6ca00dae062ca","aeccb86c624947eeb9b75a6d4badb337","0d23b07174434ce08dc5a5c1b23fa5cf","86870893cb1e4a78961f446ad7ad2789","5e6d053044d14033bd86a54a520574be","dd02acc3c8df496fb9102d622f6d817c","dc7a6b53f65a4215bfebfaf26a6f1aef","5a34b46de23f473ca6f45d86328ece10","2542210e2f5e4eaaab495d7b3975d66b","a3163cc54eb54d1f8d24b8e876e67a79","2a93c9e7ca5448ddb37341fb3dec398a","3865263e6e8f4c479cfbbaf0c1f066d2","adb1f1f39a3a47e3b52a66731682fa15","7b2c556cfe524baa86ab3f7ab48d0f49","d0e978d20f564e9c9cbaa2fbf03bd613","0ef722daea0947d19722945d6abbcad5","9efe717e9bce4f71805dbf20722f26d3","197f05bb8d6f40cea64ccadf86a6a124","f6318dfcb9b6467bb5a7595738999543","341943a43d4946098912afac5d326f40","c66d177ae36942b4862ad7df49c3d033","81ea210c029c4f0695346ab719837a65","b37e35208da44f3c9963bf48ab762bbe","b0f3e52bbfb74245b886c4cb0835dbb5","9225137e684f4394a1c1b0341814ced9","673ac3cdad9c4865be1c2ec7b3a180d1","e196113086a742a1ac1ae502e69800d3","3c648fc93cd84f26ae93cbe6ad987cf0","abe5b15540b2409aa4107d06444936e5","1cfc842794af4cf8a2d246bab56de11a","8c181e28392243dc856d543d1037a083","79d4a4b35042499ab260d8ff93972961","7f847256031349b9adf66ba7a9d80631","d7bcc109866244b2a578ef01ff8f43f9","a1bd83bd6d7f47c8ad8590b773c99af6","e013283446a645659244ba667c0b46be","f547d118c723490db017b38af2ba84bd","b9f23447f3c84e5ca45ae5b4db393f23","eec98d54979848b0a27464865960716a","485a88e24ae4442cb2de8831323fca28","bf4e74547f104ee88aaf9695065d1431","fe79b59de5894978a6e1ae60c69ad06d","995afb8ce9a24be28681578581418fe8","691b0deea63d473a98a93cd9622c61d6","5beb990d358b458abdc2a49fe49c9763","205a97ce351c4a40bf8162d5c96ab1c2","b4f41b6a9aee4233bd511e14809793fa","9cbb70d40c514441a1c7dda82341307b","a8a7ab4ca7e5477d9d60ef05975687ec","e3e623233e0e49e2bf43ecf4c32cc93c","165eb3ae8dd14a5fa5660b62344251b2","6a3c42870af44bc096ed971cd8775724","2d7a945a970544418012df0a126c54a3","eb7f1c36bbb24b1abf8ec06b40e4abdb","611162d783994462b1fb75f4ca74dd8d","561de7ced93d4e7f8ba3ac5ae569fd68","4ea711e4efca4db19bf7a98b1d503213","8e88b0e798ae4cd0909bb36cd3b6a445","1ef8f7752c234566887ef106026ad4d7","67aed2a5fdc64786ac8b24dbf9e7ae1a","6bac6f2be83a4961b120592d283e2897","60d29a12f0fe403886c9d1e3fe35521d","a137e5880be34a24b28508b53b2a4977","841a05e6b81147189e58af1ecfe542e4","2db28ca705b14ceda46bbdc7907ab80a","310811ac282d4fb5a2cd57c4517ac8cc","e3859acaac69496a9bc48c21e250475e","2bea0a2f24d742b79cdd13734eb1a152","fa9810376469423c81ae792283198cca","c30c19ef23f64395b00f97ca75e410c3","0177e74a46d34d878042123e9efb8a61","818cea53d53c40d8bd2ab23cb4ef2ae2","0fb9b80b14b54c5eb355af05273b6dea","a17e4ec8f22d4115adc239e190f279c1","dd36b53c664e4f59b89e6009f70d3519","d767c40dab1041cbbd81ca338cb1cc77","8022094846a44f50b0e41ebd728c84a0","041d8ddb004e4bd989065e9bd191f4e9","c1d9efd672144e01a10cfefbb07168f3","07e4defe9a7f4e188381c10b279ddb7c","4c46bfeabc97424d91821df6c7233d08","76a27bc65c504d7689581354a1414e89","58810df1c5f1418694bfd55a081b32e6","5a5809367a304ba9a271a6aff7a33c0e","2aeaa6c6a5df4ccf830e9d06305b8a66","81755e9266064bb6b17bb5314c0827a6","8ed8cf93f15649f68aab21684ee98d82","6f59cf3a55624579bf9623d111d1c607","2543a03701da400ca56589fbd08750a4","c6eea20e76a7410b8a34714e52ed3191","07327916e56c47cab7059da0d5e1f449","4ca32d8cac484412bb3f743c7f29a514","860a03f0983649a9809875fb66e9c585","66a434f08824456abef1a0aec6efbf0e","9d9b4af892204fffbe49d7bc50e8179b"]},"id":"zEx7Sc67i4UE","executionInfo":{"status":"ok","timestamp":1705529899529,"user_tz":-60,"elapsed":2121691,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"eee4fd0c-310a-4631-cf94-ec23f2bc50d0"},"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/78.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"350f3749ea1c4efabc5328d6598dfd31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a93c9e7ca5448ddb37341fb3dec398a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ea210c029c4f0695346ab719837a65"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:426: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/813 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f847256031349b9adf66ba7a9d80631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691b0deea63d473a98a93cd9622c61d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"611162d783994462b1fb75f4ca74dd8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"310811ac282d4fb5a2cd57c4517ac8cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:drc923f0) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8022094846a44f50b0e41ebd728c84a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>mean_iou</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>mean_iou</td><td>0.33272</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">sleek-vortex-2</strong> at: <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_Cityscapes_Swin_rail19ValEval_subset/runs/drc923f0' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_Cityscapes_Swin_rail19ValEval_subset/runs/drc923f0</a><br/>Synced 5 W&B file(s), 4 media file(s), 2 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240117_205522-drc923f0/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:drc923f0). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240117_214313-smi0n93o</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset/runs/smi0n93o' target=\"_blank\">lilac-valley-7</a></strong> to <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset/runs/smi0n93o' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset/runs/smi0n93o</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'epochs': 1, 'classes': 19, 'batch_size': 2, 'dataset': 'Cityscapes_RailSem19', 'architecture': 'OF_Dinat_L'}\n","{0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence', 5: 'pole', 6: 'traffic light', 7: 'traffic sign', 8: 'vegetation', 9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car', 14: 'truck', 15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle'}\n","OneFormerImageProcessor {\n","  \"_max_size\": 2048,\n","  \"class_info_file\": \"cityscapes_panoptic.json\",\n","  \"do_normalize\": true,\n","  \"do_reduce_labels\": false,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"ignore_index\": 255,\n","  \"image_mean\": [\n","    0.48500001430511475,\n","    0.4560000002384186,\n","    0.4059999883174896\n","  ],\n","  \"image_processor_type\": \"OneFormerImageProcessor\",\n","  \"image_std\": [\n","    0.2290000021457672,\n","    0.2239999920129776,\n","    0.22499999403953552\n","  ],\n","  \"metadata\": {\n","    \"0\": \"road\",\n","    \"1\": \"building\",\n","    \"10\": \"sky\",\n","    \"11\": \"person\",\n","    \"12\": \"rider\",\n","    \"13\": \"car\",\n","    \"14\": \"truck\",\n","    \"15\": \"bus\",\n","    \"16\": \"train\",\n","    \"17\": \"motorcycle\",\n","    \"18\": \"bicycle\",\n","    \"2\": \"sidewalk\",\n","    \"3\": \"wall\",\n","    \"4\": \"fence\",\n","    \"5\": \"pole\",\n","    \"6\": \"traffic light\",\n","    \"7\": \"traffic sign\",\n","    \"8\": \"vegetation\",\n","    \"9\": \"terrain\",\n","    \"class_names\": [\n","      \"road\",\n","      \"building\",\n","      \"sidewalk\",\n","      \"wall\",\n","      \"fence\",\n","      \"pole\",\n","      \"traffic light\",\n","      \"traffic sign\",\n","      \"vegetation\",\n","      \"terrain\",\n","      \"sky\",\n","      \"person\",\n","      \"rider\",\n","      \"car\",\n","      \"truck\",\n","      \"bus\",\n","      \"train\",\n","      \"motorcycle\",\n","      \"bicycle\"\n","    ],\n","    \"thing_ids\": [\n","      11,\n","      12,\n","      13,\n","      14,\n","      15,\n","      16,\n","      17,\n","      18\n","    ]\n","  },\n","  \"num_labels\": 19,\n","  \"num_text\": 234,\n","  \"processor_class\": \"OneFormerProcessor\",\n","  \"repo_path\": \"shi-labs/oneformer_demo\",\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"longest_edge\": 2048,\n","    \"shortest_edge\": 1024\n","  }\n","}\n","\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/75 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aeaa6c6a5df4ccf830e9d06305b8a66"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Mean IoU: 0.3606062905979246\n"]}],"source":["####\n","#@title Domain shift - Pytorch inference and evaluation with cityscapes pretrained model on railsem19 validation subset without changing the cityscapes labels\n","####\n","\n","#Dataset\n","railsem_ds_val = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\", split=\"validation[:150]\")\n","\n","#Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","\n","#instantiate the validation dataset\n","val_dataset = CustomDataset(railsem_ds_val)\n","\n","#model and processor config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint)\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_domain_shift_{dataset_name}_{ckpt_name}_rail19ValEval_subset\", config=config)\n","print(wandb.config)\n","\n","#collate function\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#dataloaders\n","val_dataloader = DataLoader(dataset=val_dataset,\n","                            batch_size=wandb.config.batch_size,\n","                            shuffle=False,\n","                            collate_fn=collate_fn)\n","\n","#Evaluation set up\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","oneformer_cityscapes_model.to(device)\n","print(oneformer_cityscapes_model.config.id2label)\n","print(cityscapes_processor.image_processor)\n","#metric\n","metric = evaluate.load(\"mean_iou\")\n","#put the model in eval mode\n","oneformer_cityscapes_model.model.is_training = False\n","oneformer_cityscapes_model.eval()\n","#Eval Loop\n","running_loss = 0.0\n","num_samples = 0.0\n","batch_cnt = 0\n","for epoch in range(wandb.config.epochs):\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"],\n","            \"pixel_mask\" : batch[\"pixel_mask\"],\n","            \"task_inputs\" : batch[\"task_inputs\"],\n","        }\n","        batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = oneformer_cityscapes_model(**batch_dict)\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                             target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size #num of examples seen\n","        batch_cnt += 1\n","\n","    mean_iou_per_epoch = metric.compute(num_labels=wandb.config.classes, ignore_index=255)\n","    wandb.log({\"epoch\": epoch, \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_accuracy\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_accuracy\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category accuracy\")})\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_iou\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_iou\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category iou\")})\n","\n"]},{"cell_type":"code","source":["mean_iou_per_epoch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMxW1-4Gvgyx","executionInfo":{"status":"ok","timestamp":1705526983348,"user_tz":-60,"elapsed":348,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"b16df3ca-9552-40e5-e20c-9d5b82b98b38"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'mean_iou': 0.33271526182604944,\n"," 'mean_accuracy': 0.4930208611233813,\n"," 'overall_accuracy': 0.625256311524518,\n"," 'per_category_iou': array([0.13545091, 0.30234026, 0.54456238, 0.        , 0.38341989,\n","        0.47895285, 0.31448542, 0.16680361, 0.76703061, 0.4371604 ,\n","        0.92615597, 0.54875225, 0.        , 0.64324405, 0.06480213,\n","        0.        , 0.60842924, 0.        , 0.        ]),\n"," 'per_category_accuracy': array([0.90018463, 0.57253129, 0.61546658, 0.        , 0.51690723,\n","        0.57981367, 0.41142182, 0.70821619, 0.80572539, 0.64490049,\n","        0.96610556, 0.83732587, 0.        , 0.94166837, 0.17625263,\n","        0.        , 0.69087666, 0.        , 0.        ])}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0003466ea4e7484c87550337a8239879","0cad075fb9ba4abead00dedd8af5174f","87b969e45ee94944bc92364263e6e098","2b14993f1b8b4c0aaf19fc9bfa72cc99","d19cfe36032e4d939ddf63f147b6ee3d","c38d02c6337045bba678bd22e001088a","c47150146a244f21a7365ee71484d360","e92583e296864e02bd42a80962535d59","55267da5821f4647b59973294f9abfb4","d675360392c245b0b8854b7eb6873f9a","cd03aab41657492bac03c36f01953b62","e320ad39cd11465aad9c6d0a1f1e1d83","6aede4df15294349a3749589c9f3ebe6","16f34813d50b4083a9dd0be474eb8a20","8e5a669c2a6143809962f2546d11c03d","eeb23828635041daa3ca85244be8cd3c","cc04d377b60a482c9dd6288419107749","984df39d9580428a888a40fb012e4f9c","5589638401a04a42a133be98a619702c"]},"id":"63GyqPCpOySL","outputId":"78df5ac1-5fce-4bcb-a5f3-172aed6e3b28","executionInfo":{"status":"error","timestamp":1705355505063,"user_tz":-60,"elapsed":4714001,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:426: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:90c4bpzu) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0003466ea4e7484c87550337a8239879"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">mild-monkey-6</strong> at: <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset/runs/90c4bpzu' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset/runs/90c4bpzu</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240115_202632-90c4bpzu/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:90c4bpzu). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240115_203315-87a0ktoz</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval/runs/87a0ktoz' target=\"_blank\">charmed-glade-2</a></strong> to <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval/runs/87a0ktoz' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval/runs/87a0ktoz</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'epochs': 1, 'classes': 19, 'batch_size': 2, 'dataset': 'Cityscapes_RailSem19', 'architecture': 'OF_Dinat_L'}\n","{0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence', 5: 'pole', 6: 'traffic light', 7: 'traffic sign', 8: 'vegetation', 9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car', 14: 'truck', 15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle'}\n","OneFormerImageProcessor {\n","  \"_max_size\": 2048,\n","  \"class_info_file\": \"cityscapes_panoptic.json\",\n","  \"do_normalize\": true,\n","  \"do_reduce_labels\": false,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"ignore_index\": 255,\n","  \"image_mean\": [\n","    0.48500001430511475,\n","    0.4560000002384186,\n","    0.4059999883174896\n","  ],\n","  \"image_processor_type\": \"OneFormerImageProcessor\",\n","  \"image_std\": [\n","    0.2290000021457672,\n","    0.2239999920129776,\n","    0.22499999403953552\n","  ],\n","  \"metadata\": {\n","    \"0\": \"road\",\n","    \"1\": \"building\",\n","    \"10\": \"sky\",\n","    \"11\": \"person\",\n","    \"12\": \"rider\",\n","    \"13\": \"car\",\n","    \"14\": \"truck\",\n","    \"15\": \"bus\",\n","    \"16\": \"train\",\n","    \"17\": \"motorcycle\",\n","    \"18\": \"bicycle\",\n","    \"2\": \"sidewalk\",\n","    \"3\": \"wall\",\n","    \"4\": \"fence\",\n","    \"5\": \"pole\",\n","    \"6\": \"traffic light\",\n","    \"7\": \"traffic sign\",\n","    \"8\": \"vegetation\",\n","    \"9\": \"terrain\",\n","    \"class_names\": [\n","      \"road\",\n","      \"building\",\n","      \"sidewalk\",\n","      \"wall\",\n","      \"fence\",\n","      \"pole\",\n","      \"traffic light\",\n","      \"traffic sign\",\n","      \"vegetation\",\n","      \"terrain\",\n","      \"sky\",\n","      \"person\",\n","      \"rider\",\n","      \"car\",\n","      \"truck\",\n","      \"bus\",\n","      \"train\",\n","      \"motorcycle\",\n","      \"bicycle\"\n","    ],\n","    \"thing_ids\": [\n","      11,\n","      12,\n","      13,\n","      14,\n","      15,\n","      16,\n","      17,\n","      18\n","    ]\n","  },\n","  \"num_labels\": 19,\n","  \"num_text\": 234,\n","  \"processor_class\": \"OneFormerProcessor\",\n","  \"repo_path\": \"shi-labs/oneformer_demo\",\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"longest_edge\": 2048,\n","    \"shortest_edge\": 1024\n","  }\n","}\n","\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/250 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55267da5821f4647b59973294f9abfb4"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-b30043c1e16c>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# get ground truth segmentation maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mground_truth_segmentation_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"original_segmentation_maps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mground_truth_segmentation_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_segmentation_maps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enforce_nested_string_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_feature_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowInvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1912\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1913\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1912\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1913\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                     \u001b[0;32mor\u001b[0m \u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_elmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfirst_elmt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 ):\n\u001b[0;32m-> 1293\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Object with special encoding:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                     \u001b[0;32mor\u001b[0m \u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_elmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfirst_elmt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 ):\n\u001b[0;32m-> 1293\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Object with special encoding:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfirst_elmt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0m_check_non_null_non_empty_recursive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_elmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m                 \u001b[0;31m# be careful when comparing tensors here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m_check_non_null_non_empty_recursive\u001b[0;34m(obj, schema)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_check_non_null_non_empty_recursive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatureType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m     \"\"\"\n\u001b[1;32m   1178\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["####\n","#@title Domain shift - Pytorch inference and evaluation with cityscapes pretrained model on railsem19 validation set without changing the cityscapes labels\n","####\n","\n","#Dataset\n","#railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\")\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\", split=\"validation\")\n","railsem_ds_subset = railsem_ds.train_test_split(test_size=0.3)\n","\n","#Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","\n","#instantiate the validation dataset\n","#val_dataset = CustomDataset(railsem_ds[\"validation\"])\n","val_dataset = CustomDataset(railsem_ds_subset[\"train\"])\n","\n","#model and processor config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint)\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_domain_shift_{dataset_name}_{ckpt_name}_rail19ValEval\", config=config)\n","print(wandb.config)\n","\n","#collate function\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#dataloaders\n","val_dataloader = DataLoader(dataset=val_dataset,\n","                            batch_size=wandb.config.batch_size, shuffle=False,\n","                            collate_fn=collate_fn)\n","\n","#Evaluation set up\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","oneformer_cityscapes_model.to(device)\n","print(oneformer_cityscapes_model.config.id2label)\n","print(cityscapes_processor.image_processor)\n","#metric\n","metric = evaluate.load(\"mean_iou\")\n","#put the model in eval mode\n","oneformer_cityscapes_model.model.is_training = False\n","oneformer_cityscapes_model.eval()\n","#Eval Loop\n","running_loss = 0.0\n","num_samples = 0.0\n","batch_cnt = 0\n","for epoch in range(wandb.config.epochs):\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","        batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = oneformer_cityscapes_model(**batch_dict)\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                             target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size #num of examples seen\n","        batch_cnt += 1\n","\n","    mean_iou_per_epoch = metric.compute(num_labels=wandb.config.classes, ignore_index=255)\n","    wandb.log({\"epoch\": epoch,\n","            \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"],\n","            \"mean_accuracy\": mean_iou_per_epoch[\"mean_accuracy\"],\n","            \"overall_accuracy\": mean_iou_per_epoch[\"overall_accuracy\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_accuracy\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_accuracy\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category accuracy\")})\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_iou\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_iou\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category iou\")})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NQ1IMTgS6Xe","colab":{"base_uri":"https://localhost:8080/","height":451,"referenced_widgets":["86499d904f7c47f1bc17f859e733c185","d581202569ac4bf6ba4a3ea777a0d0a2","c176f999a31a4df0b1b756e2a8aa1227","5862d6f1dc7644c7b442adb82ae1d555","48d5e1354f3c44e99e852b343ecf099a","2da0581c49784688a2516beef7bfc6c5","20514f4be69c4a8d9dbcff4f30efa44e","cac69ddf627a4252964faa9c86c92d2e","327a9a95704e4e69bee0b63afa5f23bb","8e6777c6380f4b9d96fe23245480e501","c5a63d920e2c42db9760d776ab1e7350","c5bb26a3232b485bb25006250fca10a7","6800bebebec04e5aad57840a77a04b89","b6af034d41e64cb5abc91ae3f285d120","88fef41184384ea6812d55f5a3eb9ce2","90779e97ac19421a8acb9fe5e7bd1309","889c3d80677145e9a12c742881f2c06d","66058fb9b35743538ff43f2ecb787b49","e741c0ec88a4483682062fd11d4d366e"]},"outputId":"8ce688c9-b988-4800-9cb2-dd20b60e2d09"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:426: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:smi0n93o) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86499d904f7c47f1bc17f859e733c185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>mean_iou</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>mean_iou</td><td>0.36061</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">lilac-valley-7</strong> at: <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset/runs/smi0n93o' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_cityscapes_dinat_rail19ValEval_subset/runs/smi0n93o</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240117_214313-smi0n93o/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:smi0n93o). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240117_222138-fes8zyp4</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/bhavanathesis/OF_domain_shift_transforms_Cityscapes_Swin_rail19ValEval/runs/fes8zyp4' target=\"_blank\">lilac-plasma-1</a></strong> to <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_transforms_Cityscapes_Swin_rail19ValEval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_transforms_Cityscapes_Swin_rail19ValEval' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_transforms_Cityscapes_Swin_rail19ValEval</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/bhavanathesis/OF_domain_shift_transforms_Cityscapes_Swin_rail19ValEval/runs/fes8zyp4' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_domain_shift_transforms_Cityscapes_Swin_rail19ValEval/runs/fes8zyp4</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'epochs': 1, 'classes': 19, 'batch_size': 2, 'dataset': 'Cityscapes_RailSem19', 'architecture': 'OF_Swin_L'}\n","{0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence', 5: 'pole', 6: 'traffic light', 7: 'traffic sign', 8: 'vegetation', 9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car', 14: 'truck', 15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle'}\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/250 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"327a9a95704e4e69bee0b63afa5f23bb"}},"metadata":{}}],"source":["####\n","#@title Domain shift with transforms - Pytorch inference and evaluation with cityscapes pretrained model on railsem19 validation set without changing the cityscapes labels\n","####\n","\n","#Dataset\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\")\n","\n","#Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","#Tranforms\n","#IMAGENET Stats\n","MEAN = np.array([0.485, 0.456, 0.406])\n","STD = np.array([0.229, 0.224, 0.225])\n","\n","val_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","#instantiate the datasets\n","val_dataset = CustomDataset(railsem_ds[\"validation\"], val_transform)\n","\n","#model and processor config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint)\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint,\n","                                                       do_rescale=False,\n","                                                       do_resize=False,\n","                                                       do_normalize=False)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_domain_shift_transforms_{dataset_name}_{ckpt_name}_rail19ValEval\", config=config)\n","print(wandb.config)\n","\n","#collate function\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#dataloaders\n","val_dataloader = DataLoader(dataset=val_dataset, batch_size=wandb.config.batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","#Evaluation set up\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","oneformer_cityscapes_model.to(device)\n","print(oneformer_cityscapes_model.config.id2label)\n","#metric\n","metric = evaluate.load(\"mean_iou\")\n","#put the model in eval mode\n","oneformer_cityscapes_model.model.is_training = False\n","oneformer_cityscapes_model.eval()\n","#Eval Loop\n","running_loss = 0.0\n","num_samples = 0.0\n","for epoch in range(wandb.config.epochs):\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","        batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = oneformer_cityscapes_model(**batch_dict)\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                             target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size\n","\n","    mean_iou_per_epoch = metric.compute(num_labels=wandb.config.classes, ignore_index=255)\n","    wandb.log({\"epoch\": epoch,\n","            \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"],\n","            \"mean_accuracy\": mean_iou_per_epoch[\"mean_accuracy\"],\n","            \"overall_accuracy\": mean_iou_per_epoch[\"overall_accuracy\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_accuracy\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_accuracy\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category accuracy\")})\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_iou\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_iou\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category iou\")})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rujKgSIlPD1E"},"outputs":[],"source":["####\n","#@title Zero shot performance - Pytorch evaluation/inference with pretrained model on railsem19 labels(id2label) and with mean iou metric\n","####\n","\n","#Dataset\n","#railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\")\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic\") #needs to be the original labels of railsem19\n","railsem_ds_subset = railsem_ds[\"data\"].train_test_split(test_size=0.04, shuffle=True, seed=42) #just 4 percent data = 340 images.. otherwise crashing\n","\n","#Get the id2label, label2id, labels from hf repo for rail19sem dataset with 19 classes\n","json_file = hf_hub_download(repo_id=\"BhavanaMalla/railsem19-semantic\",\n","                filename=\"labels_info.json\",\n","                repo_type=\"dataset\",\n","                local_dir=\"/content\")\n","\n","with open(json_file, \"r\") as f:\n","    labels_info = json.load(f)\n","\n","id2label, label2id, labels, color_palette, readable_labels = labels_info.values()\n","\n","#Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","\n","#instantiate the railsem19 val dataset\n","#val_dataset = CustomDataset(railsem_ds[\"validation\"])\n","val_dataset = CustomDataset(railsem_ds_subset[\"test\"])\n","\n","#model and processor config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint,\n","                                                                           id2label=id2label,\n","                                                                           ignore_mismatched_sizes=True)\n","\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_Zero_Shot_{dataset_name}_{ckpt_name}_rail19ValEval\", config=config)\n","print(wandb.config)\n","\n","#collate function\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#dataloader\n","val_dataloader = DataLoader(\n","    dataset=val_dataset,\n","    batch_size=wandb.config.batch_size,\n","    shuffle=False,\n","    collate_fn=collate_fn\n",")\n","\n","#Eval set up\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","oneformer_cityscapes_model.to(device)\n","print(oneformer_cityscapes_model.config.id2label)\n","metric = evaluate.load(\"mean_iou\")\n","model_results = {\"miou\": []}\n","\n","oneformer_cityscapes_model.model.is_training = False\n","\n","oneformer_cityscapes_model.eval()\n","running_loss = 0.0\n","num_samples = 0.0\n","for epoch in range(wandb.config.epochs):\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","        batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = oneformer_cityscapes_model(pixel_values=batch_dict[\"pixel_values\"],\n","                                               pixel_mask=batch_dict[\"pixel_mask\"],\n","                                               task_inputs=batch_dict[\"task_inputs\"])\n","\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                             target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size\n","\n","    mean_iou_per_epoch = metric.compute(num_labels=wandb.config.classes, ignore_index=255)\n","    model_results[\"miou\"].append(mean_iou_per_epoch)\n","    wandb.log({\"epoch\": epoch,\n","            \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"],\n","            \"mean_accuracy\": mean_iou_per_epoch[\"mean_accuracy\"],\n","            \"overall_accuracy\": mean_iou_per_epoch[\"overall_accuracy\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_accuracy\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_accuracy\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category accuracy\")})\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_iou\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_iou\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category iou\")})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwhU0gcAQpsQ"},"outputs":[],"source":["####\n","#@title Zero shot performance with transforms - Pytorch evaluation/inference with pretrained model on railsem19 labels(id2label) and with mean iou metric\n","####\n","\n","#Dataset\n","#railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\")\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic\") #needs to be the original labels of railsem19\n","railsem_ds_subset = railsem_ds[\"data\"].train_test_split(test_size=0.04, shuffle=True, seed=42) #just 4 percent data = 340 images.. otherwise crashing\n","\n","#Get the id2label, label2id, labels from hf repo for rail19sem dataset with 19 classes\n","json_file = hf_hub_download(repo_id=\"BhavanaMalla/railsem19-semantic\",\n","                filename=\"labels_info.json\",\n","                repo_type=\"dataset\",\n","                local_dir=\"/content\")\n","\n","with open(json_file, \"r\") as f:\n","    labels_info = json.load(f)\n","\n","id2label, label2id, labels, color_palette, readable_labels = labels_info.values()\n","\n","#Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","#Tranforms\n","#IMAGENET STATS\n","MEAN = np.array([0.485, 0.456, 0.406])\n","STD = np.array([0.229, 0.224, 0.225])\n","\n","val_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","#instantiate the railsem19 val dataset\n","#val_dataset = CustomDataset(railsem_ds[\"validation\"], val_transform)\n","val_dataset = CustomDataset(railsem_ds_subset[\"test\"], val_transform)\n","\n","#model and processor config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint,\n","                                                                           id2label=id2label,\n","                                                                           ignore_mismatched_sizes=True)\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint,\n","                                                       do_rescale=False,\n","                                                       do_resize=False,\n","                                                       do_normalize=False)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_Zero_Shot_transforms_{dataset_name}_{ckpt_name}_rail19ValEval\", config=config)\n","print(wandb.config)\n","\n","#collate function\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#dataloader\n","val_dataloader = DataLoader(\n","    dataset=val_dataset,\n","    batch_size=wandb.config.batch_size,\n","    shuffle=False,\n","    collate_fn=collate_fn\n",")\n","\n","#Eval set up\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","oneformer_cityscapes_model.to(device)\n","print(oneformer_cityscapes_model.config.id2label)\n","metric = evaluate.load(\"mean_iou\")\n","model_results = {\"eval_loss\": [],\n","                 \"miou\": []}\n","\n","oneformer_cityscapes_model.model.is_training = False\n","\n","oneformer_cityscapes_model.eval()\n","running_loss = 0.0\n","num_samples = 0.0\n","for epoch in range(wandb.config.epochs):\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","        batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = oneformer_cityscapes_model(pixel_values=batch_dict[\"pixel_values\"],\n","                                               pixel_mask=batch_dict[\"pixel_mask\"],\n","                                               task_inputs=batch_dict[\"task_inputs\"])\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                             target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size\n","\n","    mean_iou_per_epoch = metric.compute(num_labels=wandb.config.classes, ignore_index=255)\n","    model_results[\"miou\"].append(mean_iou_per_epoch)\n","    wandb.log({\"epoch\": epoch,\n","            \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"],\n","            \"mean_accuracy\": mean_iou_per_epoch[\"mean_accuracy\"],\n","            \"overall_accuracy\": mean_iou_per_epoch[\"overall_accuracy\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_accuracy\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_accuracy\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category accuracy\")})\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_iou\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_iou\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category iou\")})"]},{"cell_type":"code","source":["####\n","#@title Few shot performance - create sub dataset for each class : Pytorch Finetuning pretrained model with railsem19 both train data with few wxamples, val data, transforms and metrics\n","####\n","\n","#Dataset Loading\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\")\n","\n","#Get the id2label, label2id, labels from hf repo for rail19sem dataset with 19 classes\n","json_file = hf_hub_download(repo_id=\"BhavanaMalla/railsem19-semantic\",\n","                filename=\"labels_info.json\",\n","                repo_type=\"dataset\",\n","                local_dir=\"/content\")\n","\n","with open(json_file, \"r\") as f:\n","    labels_info = json.load(f)\n","\n","id2label, label2id, labels, color_palette, readable_labels = labels_info.values()\n","\n","#custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","#model config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint,\n","                                                                        is_training=True,\n","                                                                        id2label=id2label,\n","                                                                        ignore_mismatched_sizes=True)\n","#processor\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint,\n","                                                    do_reduce_labels=False,\n","                                                    do_normalize=False,\n","                                                    do_rescale=False,\n","                                                    do_resize=False,)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_Fewshot_Finetuning_{dataset_name}_{ckpt_name}_rail19_TrainVal\", config=config)\n","print(f\"Wandb Config: {wandb.config}\")\n","\n","#Tranforms\n","#IMAGENET\n","MEAN = np.array([0.485, 0.456, 0.406])\n","STD = np.array([0.229, 0.224, 0.225])\n","\n","train_transform = A.Compose([\n","    A.LongestMaxSize(max_size=1333),\n","    A.RandomCrop(width=512, height=512),\n","    A.HorizontalFlip(p=0.5),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","val_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","test_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","#datasets\n","train_dataset = CustomDataset(railsem_ds[\"train\"], train_transform)\n","val_dataset = CustomDataset(railsem_ds[\"validation\"], val_transform)\n","test_dataset = CustomDataset(railsem_ds[\"test\"], test_transform)\n","\n","#collate_fn\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#Dataloaders\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","val_dataloader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n","\n","#Training\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","oneformer_cityscapes_model.to(device)\n","\n","optimizer = AdamW(params=oneformer_cityscapes_model.parameters(), lr=5e-5)\n","\n","metric = evaluate.load(\"mean_iou\")\n","\n","model_results = {\n","    \"train_loss\": [],\n","    \"eval_loss\": [],\n","    \"miou\": []}\n","\n","#print trainable params\n","def print_trainable_parameters(model, print_msg):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for name, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(f\"\\n-----{print_msg}......\")\n","    print(\n","        f\"trainable params: {trainable_params / 1e+6:.2f}M || all params: {all_param / 1e+6:.2f}M || trainable%: {100 * trainable_params / all_param:.2f}%\"\n","    )\n","\n","print_trainable_parameters(oneformer_cityscapes_model, print_msg=\"Base Model Trainable Params\")\n","\n","#Freeze the base model pixel decoder, task encoder, transformer decode excpt the last embedding layers\n","for name, params in oneformer_cityscapes_model.named_parameters():\n","    if name.startswith(\"model.pixel_level_module\") or name.startswith(\"model.task_encoder\"):\n","        params.requires_grad = False\n","    if name.startswith(\"model.transformer_module\"):\n","        if not any(keyword in name for keyword in [\"query_input_projection\", \"class_embed\", \"mask_embed\", \"level_embed\"]):\n","            params.requires_grad = False\n","\n","print_trainable_parameters(oneformer_cityscapes_model, print_msg=\"Frozen Model Trainable Params\")\n","\n","\n","#Training Loop\n","running_loss = 0.0\n","running_eval_loss = 0.0\n","num_samples = 0.0\n","num_samples_eval = 0.0\n","batch_cnt = 0\n","for epoch in range(wandb.config.epochs):\n","    oneformer_cityscapes_model.train()\n","    for idx, batch in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"mask_labels\":  [labels.to(device) for labels in batch[\"mask_labels\"]],\n","            \"class_labels\" : [labels.to(device) for labels in batch[\"class_labels\"]],\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"text_inputs\" : batch[\"text_inputs\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","        batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #train forward pass\n","        outputs = oneformer_cityscapes_model(**batch_dict)\n","        train_loss = outputs.loss\n","        running_loss += train_loss.item()\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size\n","        batch_cnt += 1\n","\n","        # Report metrics every 25th batch\n","        if ((batch_cnt + 1) % 2) == 0:\n","            print(\"\\n------------------------------------------------------\\n\")\n","            wandb.log({\"epoch\": epoch, \"loss\": train_loss}, step=num_samples)\n","            wandb.log({\"epoch\": epoch, \"loss\": train_loss}, step=num_samples)\n","            print(f\"Loss after {str(num_samples).zfill(5)} examples: {train_loss:.3f}\")\n","            print(\"\\n------------------------------------------------------\\n\")\n","\n","        print(f\"Train Loss: {running_loss / num_samples}\")\n","        train_loss.backward()\n","        optimizer.step()\n","\n","    train_loss_per_epoch = running_loss / num_samples\n","    wandb.log({\"epoch\": epoch, \"train_epoch_loss\": train_loss_per_epoch})\n","    model_results[\"train_loss\"].append(train_loss_per_epoch)\n","    del batch\n","\n","    #Eval Loop\n","    oneformer_cityscapes_model.eval()\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = oneformer_cityscapes_model(pixel_values=batch_dict[\"pixel_values\"],\n","                                               pixel_mask=batch_dict[\"pixel_mask\"],\n","                                               task_inputs=batch_dict[\"task_inputs\"]\n","                                               )\n","        eval_loss = eval_outputs.loss\n","        running_eval_loss += eval_loss.item()\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                             target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples_eval += batch_size\n","        print(f\"Eval Loss: {running_eval_loss / num_samples_eval}\")\n","\n","    eval_loss_per_epoch = running_eval_loss / num_samples_eval\n","    model_results[\"eval_loss\"].append(eval_loss_per_epoch)\n","    wandb.log({\"epoch\": epoch, \"eval_loss_per_epoch\": eval_loss_per_epoch})\n","\n","    mean_iou_per_epoch = metric.compute(num_labels=len(id2label), ignore_index=255)\n","    model_results[\"miou\"].append(mean_iou_per_epoch)\n","    wandb.log({\"epoch\": epoch,\n","               \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"],\n","               \"mean_accuracy\": mean_iou_per_epoch[\"mean_accuracy\"],\n","               \"overall_accuracy\": mean_iou_per_epoch[\"overall_accuracy\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","    del batch\n","\n","data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_accuracy\"])]\n","table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","wandb.log({\"per_category_accuracy\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category accuracy\")})\n","data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_iou\"])]\n","table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","wandb.log({\"per_category_iou\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category iou\")})\n","\n","# Save the model in the exchangeable ONNX format\n","torch.onnx.export(oneformer_cityscapes_model, f\"oneformer_{dataset_name}_{ckpt_name}_finetune_fewshot.onnx\")\n","wandb.save(f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune_fewshot.onnx\")"],"metadata":{"id":"wPImQiaV8z3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"6Mj4kQDmYV-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####\n","#@title Pytorch Finetuning pretrained model with railsem19 both train data, val data subsets transforms and metrics\n","####\n","\n","#Dataset Loading\n","#railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\") #we need to do on original dataset when you knpw info on id2label\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\", split=\"train[:350]\")\n","railsem_ds_subset = railsem_ds.train_test_split(test_size=0.2, shuffle=True)\n","\n","#Get the id2label, label2id, labels from hf repo for rail19sem dataset with 19 classes\n","json_file = hf_hub_download(repo_id=\"BhavanaMalla/railsem19-semantic\",\n","                filename=\"labels_info.json\",\n","                repo_type=\"dataset\",\n","                local_dir=\"/content\")\n","\n","with open(json_file, \"r\") as f:\n","    labels_info = json.load(f)\n","\n","id2label, label2id, labels, color_palette, readable_labels = labels_info.values()\n","\n","#custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","#model config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint,\n","                                                                        is_training=True,\n","                                                                        id2label=id2label,\n","                                                                        ignore_mismatched_sizes=True)\n","#processor\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint,\n","                                                    do_reduce_labels=False,\n","                                                    do_normalize=False,\n","                                                    do_rescale=False,\n","                                                    do_resize=False,)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_Finetuning_{dataset_name}_{ckpt_name}_rail19_TrainVal_subset\", config=config)\n","print(f\"Wandb Config: {wandb.config}\")\n","\n","#Tranforms\n","#IMAGENET\n","MEAN = np.array([0.485, 0.456, 0.406])\n","STD = np.array([0.229, 0.224, 0.225])\n","\n","train_transform = A.Compose([\n","    A.LongestMaxSize(max_size=1333),\n","    A.RandomCrop(width=512, height=512),\n","    A.HorizontalFlip(p=0.5),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","val_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","test_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","##datasets\n","# train_dataset = CustomDataset(railsem_ds[\"train\"], train_transform)\n","# val_dataset = CustomDataset(railsem_ds[\"validation\"], val_transform)\n","# test_dataset = CustomDataset(railsem_ds[\"test\"], test_transform)\n","train_dataset = CustomDataset(railsem_ds_subset[\"train\"], train_transform)\n","val_dataset = CustomDataset(railsem_ds_subset[\"test\"], val_transform)\n","\n","\n","#collate_fn\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#Dataloaders\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","val_dataloader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","# test_dataloader = DataLoader(dataset=test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n","\n","#print trainable params\n","def print_trainable_parameters(model, print_msg):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for name, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(f\"\\n-----{print_msg}......\")\n","    print(\n","        f\"trainable params: {trainable_params / 1e+6:.2f}M || all params: {all_param / 1e+6:.2f}M || trainable%: {100 * trainable_params / all_param:.2f}%\"\n","    )\n","\n","print_trainable_parameters(oneformer_cityscapes_model, print_msg=\"Base Model Trainable Params\")\n","\n","#Training\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","optimizer = AdamW(params=oneformer_cityscapes_model.parameters(), lr=5e-5)\n","\n","metric = evaluate.load(\"mean_iou\")\n","\n","model_results = {\n","    \"train_loss\": [],\n","    \"eval_loss\": [],\n","    \"miou\": []}\n","\n","#Freeze the base model pixel decoder, task encoder, transformer decode excpt the last embedding layers\n","for name, params in oneformer_cityscapes_model.named_parameters():\n","    if name.startswith(\"model.pixel_level_module\") or name.startswith(\"model.task_encoder\"):\n","        params.requires_grad = False\n","    if name.startswith(\"model.transformer_module\"):\n","        if not any(keyword in name for keyword in [\"query_input_projection\", \"class_embed\", \"mask_embed\", \"level_embed\"]):\n","            params.requires_grad = False\n","\n","print_trainable_parameters(oneformer_cityscapes_model, print_msg=\"Frozen Model Trainable Params\")\n","\n","\n","#Training Loop\n","running_loss = 0.0\n","running_eval_loss = 0.0\n","num_samples = 0\n","num_samples_eval = 0\n","batch_cnt = 0\n","best_loss = 1e+5\n","start_epoch = 0\n","\n","#Checkpoint checking\n","checkpoint_save_path = Path(\"/content\")\n","checkpoint_name = f\"checkpoint_{dataset_name}_{ckpt_name}_finetune_subset.pt\"\n","name_path = checkpoint_save_path / checkpoint_name\n","if os.path.exists(name_path):\n","    print(f\"Checkpoint found..Loading the model from: {name_path}\")\n","    #Loading checkpoint\n","    checkpoint = torch.load(f=name_path)\n","    oneformer_cityscapes_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","    best_loss = checkpoint[\"best_loss\"]\n","    start_epoch = checkpoint[\"epoch\"] + 1\n","\n","#Training Loop\n","oneformer_cityscapes_model.to(device)\n","for epoch in range(start_epoch, wandb.config.epochs):\n","    oneformer_cityscapes_model.train()\n","    for idx, batch in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"mask_labels\":  [labels.to(device) for labels in batch[\"mask_labels\"]],\n","            \"class_labels\" : [labels.to(device) for labels in batch[\"class_labels\"]],\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"text_inputs\" : batch[\"text_inputs\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","        #batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #train forward pass\n","        outputs = oneformer_cityscapes_model(**batch_dict)\n","        train_loss = outputs.loss\n","        running_loss += train_loss.item()\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size\n","        batch_cnt += 1\n","\n","        print(f\"Train Loss: {train_loss.item()}\")\n","\n","        # Report metrics every 25th batch\n","        if ((batch_cnt + 1) % 2) == 0:\n","            print(\"\\n------------------------------------------------------\\n\")\n","            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss.item()}, step=num_samples)\n","            print(f\"Loss after {str(num_samples).zfill(5)} examples: {train_loss.item():.3f}\")\n","            print(\"\\n------------------------------------------------------\\n\")\n","\n","        #backward pass\n","        train_loss.backward()\n","        optimizer.step()\n","        del batch\n","\n","    train_loss_per_epoch = running_loss / num_samples\n","    if train_loss_per_epoch < best_loss:\n","        best_loss = train_loss_per_epoch\n","        #Saving checkpoint\n","        print(f\"Epoch {epoch} | Training checkpoint saved at {name_path}\")\n","        checkpoint = {\"model_state_dict\": oneformer_cityscapes_model.state_dict(),\n","                      \"optimizer_state_dict\": optimizer.state_dict(),\n","                      \"best_loss\": best_loss,\n","                      \"epoch\": start_epoch}\n","        torch.save(obj=checkpoint, f=name_path)\n","    wandb.log({\"epoch\": epoch, \"train_epoch_loss\": train_loss_per_epoch})\n","    model_results[\"train_loss\"].append(train_loss_per_epoch)\n","    start_epoch += 1\n","    #Eval Loop - just fr this run - changed the indentadtion as start epochus already updated to 1\n","    oneformer_cityscapes_model.eval()\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","        batch_dict = {\n","                \"pixel_values\": batch[\"pixel_values\"].to(device),\n","                \"mask_labels\":  [labels.to(device) for labels in batch[\"mask_labels\"]],\n","                \"class_labels\" : [labels.to(device) for labels in batch[\"class_labels\"]],\n","                \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","                \"text_inputs\" : batch[\"text_inputs\"].to(device),\n","                \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = oneformer_cityscapes_model(**batch_dict)\n","\n","        eval_loss = eval_outputs.loss\n","        running_eval_loss += eval_loss.item()\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                                target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples_eval += batch_size\n","        wandb.log({\"epoch\": epoch, \"eval_loss\": eval_loss.item()}, step=num_samples_eval)\n","        print(f\"Eval Loss: {eval_loss.item()}\")\n","\n","    eval_loss_per_epoch = running_eval_loss / num_samples_eval\n","    model_results[\"eval_loss\"].append(eval_loss_per_epoch)\n","    wandb.log({\"epoch\": epoch, \"eval_loss_per_epoch\": eval_loss_per_epoch})\n","    mean_iou_per_epoch = metric.compute(num_labels=len(id2label), ignore_index=255)\n","    model_results[\"miou\"].append(mean_iou_per_epoch)\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","    wandb.log({\"epoch\": epoch,\n","                \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"],\n","                \"mean_accuracy\": mean_iou_per_epoch[\"mean_accuracy\"],\n","                \"overall_accuracy\": mean_iou_per_epoch[\"overall_accuracy\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","    del batch\n","\n","    data = [[x, y] for (x, y) in zip(\n","        cityscapes_processor.image_processor.metadata[\"class_names\"],\n","        mean_iou_per_epoch[\"per_category_accuracy\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_accuracy\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category accuracy\")})\n","    data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_iou\"])]\n","    table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","    wandb.log({\"per_category_iou\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category iou\")})\n","\n","# Save the model in the exchangeable ONNX format\n","# torch.onnx.export(oneformer_cityscapes_model, f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune_subset.onnx\")\n","# wandb.save(f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune_subset.onnx\")\n","\n","#Save the model to hugging face\n","model_id = f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune_subset\"\n","oneformer_cityscapes_model.save_pretrained(model_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":782,"referenced_widgets":["8e91969c41674638a868e42ec9e2cfe0","23cbd7dddd7345fba7f77b37af227ffe","e2482a81d2104575b1b605c965782f5c","7c5ac8e5fa4548f69aa2b76422cc916b","02fc74850b1a478484e3ecf992652133","4866279cb64141e4815c261744a29a5b","f60b315d7c3a4c509cf041f15c8a8e93","8e64c6eece2b4096a58bf80d4ea3ee74","c10db7aa31b1484b8368b4138d879b5f","175d8d1620a749deb5a2a6a729e93cb8","9586ae04136d43a6be94ad6c23753492","2d9af20799ca4ba2948289032b4ded62","2646c4ba4b7249c4aaedbaf8352fac70","77e0e3e632b442e3b8166c0ccf242bb3","c99a46cdb16947b39b17a7e7d7d6a3b2","30ffb8b77bfa437f8a9e25ef64a592e3","3326743d2bfd4965ac061977556ee489","0f35b82393e6460ab23fbde15c1e810d","976e23fca8d94c68bbe4ec57e31c0923"]},"id":"gZUSqS3qRjHg","executionInfo":{"status":"error","timestamp":1705361339760,"user_tz":-60,"elapsed":418698,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"f21a17e8-88b0-4fb4-c90a-8797bb1493af"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_dinat_large and are newly initialized: ['model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_projector.layers.1.0.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:f00krwgr) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e91969c41674638a868e42ec9e2cfe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">solar-breeze-19</strong> at: <a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal/runs/f00krwgr' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal/runs/f00krwgr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240115_231339-f00krwgr/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:f00krwgr). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240115_232216-67pamt7l</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal/runs/67pamt7l' target=\"_blank\">rich-shape-20</a></strong> to <a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal/runs/67pamt7l' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal/runs/67pamt7l</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Wandb Config: {'epochs': 1, 'classes': 19, 'batch_size': 2, 'dataset': 'Cityscapes_RailSem19', 'architecture': 'OF_Dinat_L'}\n","\n","-----Base Model Trainable Params......\n","trainable params: 240.53M || all params: 240.53M || trainable%: 100.00%\n","\n","-----Frozen Model Trainable Params......\n","trainable params: 17.81M || all params: 240.53M || trainable%: 7.41%\n","Checkpoint found..Loading the model from: /content/checkpoint_cityscape_dinat.pt\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10db7aa31b1484b8368b4138d879b5f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Eval Loss: 84.25753784179688\n","Eval Loss: 89.3807144165039\n"]},{"output_type":"stream","name":"stderr","text":["/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n","  iou = total_area_intersect / total_area_union\n","/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n","  acc = total_area_intersect / total_area_label\n"]},{"output_type":"stream","name":"stdout","text":["Mean IoU: 0.20556262819398163\n","Mean IoU: 0.20556262819398163\n"]},{"output_type":"error","ename":"TypeError","evalue":"export() missing 1 required positional argument: 'f'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-1f1fc2177b0e>\u001b[0m in \u001b[0;36m<cell line: 303>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;31m# Save the model in the exchangeable ONNX format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moneformer_cityscapes_dinat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"oneformer_cityscapes_dinat_finetune.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oneformer_cityscapes_dinat_finetune.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: export() missing 1 required positional argument: 'f'"]}]},{"cell_type":"code","source":["# Save the model in the exchangeable ONNX format\n","# torch.onnx.export(model=oneformer_cityscapes_model,\n","#                   args=list(np.array(railsem_ds_subset[\"train\"][\"image\"])),                         # model input (or a tuple for multiple inputs)\n","#                   f=f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune_subset.onnx\",\n","#                   export_params=True,)                                                              # store the trained parameter weights inside the model file\n","# wandb.save(f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune_subset.onnx\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"MIs_StK_dxU5","executionInfo":{"status":"error","timestamp":1705362055113,"user_tz":-60,"elapsed":739,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"72f30d31-325b-42f5-ff39-e4f263d1eeb0"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-32-b9ef254942e4>:4: FutureWarning: The input object of type 'JpegImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'JpegImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  args=list(np.array(railsem_ds_subset[\"train\"][\"image\"])),                         # model input (or a tuple for multiple inputs)\n","<ipython-input-32-b9ef254942e4>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  args=list(np.array(railsem_ds_subset[\"train\"][\"image\"])),                         # model input (or a tuple for multiple inputs)\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: JpegImageFile","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-b9ef254942e4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the model in the exchangeable ONNX format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m torch.onnx.export(model=oneformer_cityscapes_dinat,              # model being run\n\u001b[0m\u001b[1;32m      4\u001b[0m                   \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrailsem_ds_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# model input (or a tuple for multiple inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"oneformer_cityscapes_dinat_finetune.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# where to save the model (can be a file or file-like object)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \"\"\"\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0m_validate_dynamic_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m             graph, params_dict, torch_out = _model_to_graph(\n\u001b[0m\u001b[1;32m   1597\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pre_trace_quant_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_jit_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0mparams_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_named_param_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_and_get_graph_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m     \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_onnx_lint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0mprev_autocast_cache_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_autocast_cache_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_autocast_cache_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m     trace_graph, torch_out, inputs_states = torch.jit._get_trace_graph(\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m     outs = ONNXTracedModule(\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     )(*args, **kwargs)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0min_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;31m# NOTE: use full state, because we need it for BatchNorm export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# This differs from the compiler path, which doesn't support it at the moment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: JpegImageFile"]}]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1350117,"status":"error","timestamp":1705322454685,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"},"user_tz":-60},"id":"MCxXLW1NUBW-","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2e2e40038cee4e71bd9fcc1af6c6bc43","587b981ace3c42eda18c45a236e22083","d751cd419ad7403b9bf888441ae815fc","8993b4eb179b4efc9546f8e059a516fe","b512a15ec9784b8f99c888062736e92e","fff5b1e76fdf48938cb3dfb5c31f6f3f","33d6c9a5417141ddae2ab84116047dd1","57eef240334f45bbb37d76123be39f59","a29d655476324102aba9ce15152a0bb5","954a91f37f1047988b6c545297ba2ffb","bc28da5cac374cefaa61490810402430"]},"outputId":"7c1ee7c7-8efa-4f96-90f0-18d48bf36461"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_dinat_large and are newly initialized: ['model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_projector.layers.1.0.weight', 'model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:426: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240115_121835-sll22ybw</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal/runs/sll22ybw' target=\"_blank\">bright-dragon-14</a></strong> to <a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal/runs/sll22ybw' target=\"_blank\">https://wandb.ai/bhavanathesis/OF_Finetuning_cityscapes_dinat_rail19_TrainVal/runs/sll22ybw</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Wandb Config: {'epochs': 1, 'classes': 19, 'batch_size': 2, 'dataset': 'Cityscapes_RailSem19', 'architecture': 'OF_Dinat_L'}\n","\n","-----Base Model Trainable Params......\n","trainable params: 240.53M || all params: 240.53M || trainable%: 100.00%\n","\n","-----Frozen Model Trainable Params......\n","trainable params: 17.81M || all params: 240.53M || trainable%: 7.41%\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2e40038cee4e71bd9fcc1af6c6bc43"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 58.13840103149414\n","\n","------------------------------------------------------\n","\n","Loss after 00001 examples: 58.138\n","\n","------------------------------------------------------\n","\n","Train Loss: 71.90904998779297\n","Train Loss: 71.19502258300781\n","\n","------------------------------------------------------\n","\n","Loss after 00003 examples: 71.195\n","\n","------------------------------------------------------\n","\n","Train Loss: 56.05479049682617\n","Train Loss: 34.41499710083008\n","\n","------------------------------------------------------\n","\n","Loss after 00005 examples: 34.415\n","\n","------------------------------------------------------\n","\n","Train Loss: 66.79296112060547\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-87c015d5a6eb>\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m#backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["####\n","#@title Pytorch Finetuning pretrained model with railsem19 both train data, val data, transforms and metrics\n","####\n","\n","#Dataset Loading\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\") #we need to do on original dataset when you knpw info on id2label\n","\n","#Get the id2label, label2id, labels from hf repo for rail19sem dataset with 19 classes\n","json_file = hf_hub_download(repo_id=\"BhavanaMalla/railsem19-semantic\",\n","                filename=\"labels_info.json\",\n","                repo_type=\"dataset\",\n","                local_dir=\"/content\")\n","\n","with open(json_file, \"r\") as f:\n","    labels_info = json.load(f)\n","\n","id2label, label2id, labels, color_palette, readable_labels = labels_info.values()\n","\n","#custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","#model config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint,\n","                                                                        is_training=True,\n","                                                                        id2label=id2label,\n","                                                                        ignore_mismatched_sizes=True)\n","#processor\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint,\n","                                                    do_reduce_labels=False,\n","                                                    do_normalize=False,\n","                                                    do_rescale=False,\n","                                                    do_resize=False,)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_Finetuning_{dataset_name}_{ckpt_name}_rail19_TrainVal\", config=config)\n","print(f\"Wandb Config: {wandb.config}\")\n","\n","#Tranforms\n","#IMAGENET\n","MEAN = np.array([0.485, 0.456, 0.406])\n","STD = np.array([0.229, 0.224, 0.225])\n","\n","train_transform = A.Compose([\n","    A.LongestMaxSize(max_size=1333),\n","    A.RandomCrop(width=512, height=512),\n","    A.HorizontalFlip(p=0.5),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","val_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","test_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","##datasets\n","train_dataset = CustomDataset(railsem_ds[\"train\"], train_transform)\n","val_dataset = CustomDataset(railsem_ds[\"validation\"], val_transform)\n","test_dataset = CustomDataset(railsem_ds[\"test\"], test_transform)\n","\n","#collate_fn\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#Dataloaders\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","val_dataloader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n","\n","#print trainable params\n","def print_trainable_parameters(model, print_msg):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for name, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(f\"\\n-----{print_msg}......\")\n","    print(\n","        f\"trainable params: {trainable_params / 1e+6:.2f}M || all params: {all_param / 1e+6:.2f}M || trainable%: {100 * trainable_params / all_param:.2f}%\"\n","    )\n","\n","print_trainable_parameters(oneformer_cityscapes_model, print_msg=\"Base Model Trainable Params\")\n","\n","#Training\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","optimizer = AdamW(params=oneformer_cityscapes_model.parameters(), lr=5e-5)\n","\n","metric = evaluate.load(\"mean_iou\")\n","\n","model_results = {\n","    \"train_loss\": [],\n","    \"eval_loss\": [],\n","    \"miou\": []}\n","\n","#Freeze the base model pixel decoder, task encoder, transformer decode excpt the last embedding layers\n","for name, params in oneformer_cityscapes_model.named_parameters():\n","    if name.startswith(\"model.pixel_level_module\") or name.startswith(\"model.task_encoder\"):\n","        params.requires_grad = False\n","    if name.startswith(\"model.transformer_module\"):\n","        if not any(keyword in name for keyword in [\"query_input_projection\", \"class_embed\", \"mask_embed\", \"level_embed\"]):\n","            params.requires_grad = False\n","\n","print_trainable_parameters(oneformer_cityscapes_model, print_msg=\"Frozen Model Trainable Params\")\n","\n","#Training Loop\n","running_loss = 0.0\n","running_eval_loss = 0.0\n","num_samples = 0\n","num_samples_eval = 0\n","batch_cnt = 0\n","best_loss = 1e+5\n","start_epoch = 0\n","\n","#Checkpoint checking\n","#checkpoint_save_path = Path(\"/content/drive/MyDrive/Colab\\ Notebooks/\")\n","checkpoint_save_path = Path(\"/content\")\n","checkpoint_name = f\"checkpoint_{dataset_name}_{ckpt_name}.pt\"\n","name_path = checkpoint_save_path / checkpoint_name\n","if os.path.exists(name_path):\n","    print(f\"Checkpoint found..Loading the model from: {name_path}\")\n","    #Loading checkpoint\n","    checkpoint = torch.load(f=name_path)\n","    oneformer_cityscapes_model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","    best_loss = checkpoint[\"best_loss\"]\n","    start_epoch = checkpoint[\"epoch\"] + 1\n","\n","#Training Loop\n","oneformer_cityscapes_model.to(device)\n","for epoch in range(start_epoch, wandb.config.epochs):\n","    oneformer_cityscapes_model.train()\n","    for idx, batch in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"mask_labels\":  [labels.to(device) for labels in batch[\"mask_labels\"]],\n","            \"class_labels\" : [labels.to(device) for labels in batch[\"class_labels\"]],\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"text_inputs\" : batch[\"text_inputs\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","        #batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #train forward pass\n","        outputs = oneformer_cityscapes_model(**batch_dict)\n","        train_loss = outputs.loss\n","        running_loss += train_loss.item()\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size\n","        batch_cnt += 1\n","\n","        print(f\"Train Loss: {train_loss.item()}\")\n","\n","        # Report metrics every 25th batch\n","        if ((batch_cnt + 1) % 2) == 0:\n","            print(\"\\n------------------------------------------------------\\n\")\n","            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss.item()}, step=num_samples)\n","            print(f\"Loss after {str(num_samples).zfill(5)} examples: {train_loss.item():.3f}\")\n","            print(\"\\n------------------------------------------------------\\n\")\n","\n","        #backward pass\n","        train_loss.backward()\n","        optimizer.step()\n","        del batch\n","\n","    train_loss_per_epoch = running_loss / num_samples\n","    if train_loss_per_epoch < best_loss:\n","        best_loss = train_loss_per_epoch\n","        #Saving checkpoint\n","        print(f\"Epoch {epoch} | Training checkpoint saved at {name_path}\")\n","        checkpoint = {\"model_state_dict\": oneformer_cityscapes_model.state_dict(),\n","                      \"optimizer_state_dict\": optimizer.state_dict(),\n","                      \"best_loss\": best_loss,\n","                      \"epoch\": start_epoch}\n","        torch.save(obj=checkpoint, f=name_path)\n","    wandb.log({\"epoch\": epoch, \"train_epoch_loss\": train_loss_per_epoch})\n","    model_results[\"train_loss\"].append(train_loss_per_epoch)\n","    start_epoch += 1\n","    #Eval Loop\n","    oneformer_cityscapes_model.eval()\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"mask_labels\":  [labels.to(device) for labels in batch[\"mask_labels\"]],\n","            \"class_labels\" : [labels.to(device) for labels in batch[\"class_labels\"]],\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"text_inputs\" : batch[\"text_inputs\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),}\n","\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = oneformer_cityscapes_model(**batch_dict)\n","\n","        eval_loss = eval_outputs.loss\n","        running_eval_loss += eval_loss.item()\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                             target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples_eval += batch_size\n","        wandb.log({\"epoch\": epoch, \"eval_loss\": eval_loss.item()}, step=num_samples_eval)\n","        print(f\"Eval Loss: {eval_loss.item()}\")\n","\n","    eval_loss_per_epoch = running_eval_loss / num_samples_eval\n","    model_results[\"eval_loss\"].append(eval_loss_per_epoch)\n","    wandb.log({\"epoch\": epoch, \"eval_loss_per_epoch\": eval_loss_per_epoch})\n","    mean_iou_per_epoch = metric.compute(num_labels=len(id2label), ignore_index=255)\n","    model_results[\"miou\"].append(mean_iou_per_epoch)\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","    wandb.log({\"epoch\": epoch,\n","               \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"],\n","               \"mean_accuracy\": mean_iou_per_epoch[\"mean_accuracy\"],\n","               \"overall_accuracy\": mean_iou_per_epoch[\"overall_accuracy\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","    del batch\n","\n","data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_accuracy\"])]\n","table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","wandb.log({\"per_category_accuracy\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category accuracy\")})\n","data = [[x, y] for (x, y) in zip(cityscapes_processor.image_processor.metadata[\"class_names\"], mean_iou_per_epoch[\"per_category_iou\"])]\n","table = wandb.Table(data=data, columns=[\"x\", \"y\"])\n","wandb.log({\"per_category_iou\": wandb.plot.line(table, \"x\", \"y\", title=\"Per category iou\")})\n","\n","# Save the model in the exchangeable ONNX format\n","torch.onnx.export(oneformer_cityscapes_model, f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune.onnx\")\n","wandb.save(f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune.onnx\")\n","\n","#Save the model to hugging face\n","model_id = f\"oneformer_{dataset_name}_{ckpt_name}_model_finetune\"\n","oneformer_cityscapes_model.save_pretrained(model_id)"]},{"cell_type":"code","source":["oneformer_cityscapes_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evEypbEF5Hsv","executionInfo":{"status":"ok","timestamp":1705318319253,"user_tz":-60,"elapsed":367,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"80d9ee1c-0459-4520-9d0a-4a022c81e396"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerForUniversalSegmentation(\n","  (model): OneFormerModel(\n","    (pixel_level_module): OneFormerPixelLevelModule(\n","      (encoder): DinatBackbone(\n","        (embeddings): DinatEmbeddings(\n","          (patch_embeddings): DinatPatchEmbeddings(\n","            (projection): Sequential(\n","              (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","              (1): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","            )\n","          )\n","          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (encoder): DinatEncoder(\n","          (levels): ModuleList(\n","            (0): DinatStage(\n","              (layers): ModuleList(\n","                (0): DinatLayer(\n","                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=192, out_features=192, bias=True)\n","                      (key): Linear(in_features=192, out_features=192, bias=True)\n","                      (value): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): Identity()\n","                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=192, out_features=384, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=384, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (1): DinatLayer(\n","                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=192, out_features=192, bias=True)\n","                      (key): Linear(in_features=192, out_features=192, bias=True)\n","                      (value): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.01206896547228098)\n","                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=192, out_features=384, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=384, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (2): DinatLayer(\n","                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=192, out_features=192, bias=True)\n","                      (key): Linear(in_features=192, out_features=192, bias=True)\n","                      (value): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=192, out_features=192, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.02413793094456196)\n","                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=192, out_features=384, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=384, out_features=192, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): DinatDownsampler(\n","                (reduction): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (1): DinatStage(\n","              (layers): ModuleList(\n","                (0): DinatLayer(\n","                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=384, out_features=384, bias=True)\n","                      (key): Linear(in_features=384, out_features=384, bias=True)\n","                      (value): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.03620689734816551)\n","                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=384, out_features=768, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=768, out_features=384, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (1): DinatLayer(\n","                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=384, out_features=384, bias=True)\n","                      (key): Linear(in_features=384, out_features=384, bias=True)\n","                      (value): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.04827586188912392)\n","                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=384, out_features=768, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=768, out_features=384, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (2): DinatLayer(\n","                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=384, out_features=384, bias=True)\n","                      (key): Linear(in_features=384, out_features=384, bias=True)\n","                      (value): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.06034482643008232)\n","                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=384, out_features=768, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=768, out_features=384, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (3): DinatLayer(\n","                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=384, out_features=384, bias=True)\n","                      (key): Linear(in_features=384, out_features=384, bias=True)\n","                      (value): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=384, out_features=384, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.07241379469633102)\n","                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=384, out_features=768, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=768, out_features=384, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): DinatDownsampler(\n","                (reduction): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (2): DinatStage(\n","              (layers): ModuleList(\n","                (0): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.08448275923728943)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (1): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.09655172377824783)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (2): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.10862068831920624)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (3): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.12068965286016464)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (4): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.13275861740112305)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (5): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.14482758939266205)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (6): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.15689654648303986)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (7): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.16896551847457886)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (8): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.18103447556495667)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (9): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.19310344755649567)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (10): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.20517240464687347)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (11): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.21724137663841248)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (12): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.22931033372879028)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (13): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.24137930572032928)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (14): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.2534482777118683)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (15): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.2655172348022461)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (16): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.2775861918926239)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (17): DinatLayer(\n","                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=768, out_features=768, bias=True)\n","                      (key): Linear(in_features=768, out_features=768, bias=True)\n","                      (value): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=768, out_features=768, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.2896551787853241)\n","                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=768, out_features=1536, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=1536, out_features=768, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","              (downsample): DinatDownsampler(\n","                (reduction): Conv2d(768, 1536, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","              )\n","            )\n","            (3): DinatStage(\n","              (layers): ModuleList(\n","                (0): DinatLayer(\n","                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.3017241358757019)\n","                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=1536, out_features=3072, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=3072, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (1): DinatLayer(\n","                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.3137930929660797)\n","                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=1536, out_features=3072, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=3072, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (2): DinatLayer(\n","                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.3258620500564575)\n","                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=1536, out_features=3072, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=3072, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (3): DinatLayer(\n","                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.3379310369491577)\n","                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=1536, out_features=3072, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=3072, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","                (4): DinatLayer(\n","                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (attention): NeighborhoodAttentionModule(\n","                    (self): NeighborhoodAttention(\n","                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                    (output): NeighborhoodAttentionOutput(\n","                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n","                      (dropout): Dropout(p=0.0, inplace=False)\n","                    )\n","                  )\n","                  (drop_path): DinatDropPath(p=0.3499999940395355)\n","                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","                  (intermediate): DinatIntermediate(\n","                    (dense): Linear(in_features=1536, out_features=3072, bias=True)\n","                    (intermediate_act_fn): GELUActivation()\n","                  )\n","                  (output): DinatOutput(\n","                    (dense): Linear(in_features=3072, out_features=1536, bias=True)\n","                    (dropout): Dropout(p=0.0, inplace=False)\n","                  )\n","                )\n","              )\n","            )\n","          )\n","        )\n","        (hidden_states_norms): ModuleDict(\n","          (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n","          (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","          (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (decoder): OneFormerPixelDecoder(\n","        (position_embedding): OneFormerSinePositionEmbedding()\n","        (input_projections): ModuleList(\n","          (0): Sequential(\n","            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","          (1): Sequential(\n","            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","          (2): Sequential(\n","            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          )\n","        )\n","        (encoder): OneFormerPixelDecoderEncoderOnly(\n","          (layers): ModuleList(\n","            (0-5): 6 x OneFormerPixelDecoderEncoderLayer(\n","              (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n","                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n","                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n","                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n","              )\n","              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","        )\n","        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (adapter_1): Sequential(\n","          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","        )\n","        (layer_1): Sequential(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n","          (2): ReLU()\n","        )\n","      )\n","    )\n","    (transformer_module): OneFormerTransformerModule(\n","      (position_embedder): OneFormerSinePositionEmbedding()\n","      (queries_embedder): Embedding(250, 256)\n","      (decoder): OneFormerTransformerDecoder(\n","        (query_transformer): OneFormerTransformerDecoderQueryTransformer(\n","          (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(\n","            (layers): ModuleList(\n","              (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(\n","                (self_attn): MultiheadAttention(\n","                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","                )\n","                (multihead_attn): MultiheadAttention(\n","                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","                )\n","                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","                (dropout1): Dropout(p=0.1, inplace=False)\n","                (dropout2): Dropout(p=0.1, inplace=False)\n","                (dropout3): Dropout(p=0.1, inplace=False)\n","                (activation): ReLU()\n","              )\n","            )\n","            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (layers): ModuleList(\n","          (0-8): 9 x OneFormerTransformerDecoderLayer(\n","            (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(\n","              (multihead_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","              (activation): ReLU()\n","            )\n","            (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(\n","              (self_attn): OneFormerAttention(\n","                (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","                (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","              )\n","              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","              (activation): ReLU()\n","            )\n","            (ffn): OneFormerTransformerDecoderFFNLayer(\n","              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n","              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (activation): ReLU()\n","            )\n","          )\n","        )\n","        (query_input_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (class_embed): Linear(in_features=256, out_features=20, bias=True)\n","        (mask_embed): OneFormerMLPPredictionHead(\n","          (layers): Sequential(\n","            (0): PredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): ReLU()\n","            )\n","            (1): PredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): ReLU()\n","            )\n","            (2): PredictionBlock(\n","              (0): Linear(in_features=256, out_features=256, bias=True)\n","              (1): Identity()\n","            )\n","          )\n","        )\n","      )\n","      (level_embed): Embedding(3, 256)\n","    )\n","    (task_encoder): OneFormerTaskModel(\n","      (task_mlp): OneFormerMLPPredictionHead(\n","        (layers): Sequential(\n","          (0): PredictionBlock(\n","            (0): Linear(in_features=77, out_features=256, bias=True)\n","            (1): ReLU()\n","          )\n","          (1): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Identity()\n","          )\n","        )\n","      )\n","    )\n","    (text_mapper): OneFormerTextMapper(\n","      (text_encoder): OneFormerTextEncoder(\n","        (transformer): OneFormerTextTransformer(\n","          (layers): Sequential(\n","            (0): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (1): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (2): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (3): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (4): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","            (5): OneFormerTextTransformerLayer(\n","              (self_attn): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","              )\n","              (layer_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","              (mlp): OneFormerTextMLP(\n","                (activation_fn): QuickGELUActivation()\n","                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              )\n","              (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            )\n","          )\n","        )\n","        (ln_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (token_embedding): Embedding(49408, 256)\n","      )\n","      (text_projector): OneFormerMLPPredictionHead(\n","        (layers): Sequential(\n","          (0): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): ReLU()\n","          )\n","          (1): PredictionBlock(\n","            (0): Linear(in_features=256, out_features=256, bias=True)\n","            (1): Identity()\n","          )\n","        )\n","      )\n","      (prompt_ctx): Embedding(16, 256)\n","    )\n","  )\n","  (matcher): OneFormerHungarianMatcher()\n","  (criterion): OneFormerLoss(\n","    (matcher): OneFormerHungarianMatcher()\n","  )\n",")"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["# for name, params in oneformer_cityscapes_model.named_parameters():\n","#     if name.startswith(\"model.pixel_level_module\") or name.startswith(\"model.task_encoder\"):\n","#         params.requires_grad = False\n","#     if name.startswith(\"model.transformer_module\"):\n","#         if not any(keyword in name for keyword in [\"query_input_projection\", \"class_embed\", \"mask_embed\", \"level_embed\"]):\n","#             params.requires_grad = False\n","\n","# cnt = 0\n","# for name, params in oneformer_cityscapes_model.named_parameters():\n","#     if params.requires_grad:\n","#         print(name)\n","#         cnt += 1\n","# cnt"],"metadata":{"id":"3guah-Dbn93g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for name, module in oneformer_cityscapes_model.named_modules():\n","    print(name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yk2YFLKjPI3r","executionInfo":{"status":"ok","timestamp":1705324165384,"user_tz":-60,"elapsed":1283,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"6aa80329-a6b8-40c2-92fd-25a356977629"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","model\n","model.pixel_level_module\n","model.pixel_level_module.encoder\n","model.pixel_level_module.encoder.embeddings\n","model.pixel_level_module.encoder.embeddings.patch_embeddings\n","model.pixel_level_module.encoder.embeddings.patch_embeddings.projection\n","model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.0\n","model.pixel_level_module.encoder.embeddings.patch_embeddings.projection.1\n","model.pixel_level_module.encoder.embeddings.norm\n","model.pixel_level_module.encoder.embeddings.dropout\n","model.pixel_level_module.encoder.encoder\n","model.pixel_level_module.encoder.encoder.levels\n","model.pixel_level_module.encoder.encoder.levels.0\n","model.pixel_level_module.encoder.encoder.levels.0.layers\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.output\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.drop_path\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.intermediate\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.output\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.output.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.0.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.output\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.drop_path\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.intermediate\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.output\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.output.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.1.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.output\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.drop_path\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.intermediate\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.output\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.output.dense\n","model.pixel_level_module.encoder.encoder.levels.0.layers.2.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.0.downsample\n","model.pixel_level_module.encoder.encoder.levels.0.downsample.reduction\n","model.pixel_level_module.encoder.encoder.levels.0.downsample.norm\n","model.pixel_level_module.encoder.encoder.levels.1\n","model.pixel_level_module.encoder.encoder.levels.1.layers\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.output\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.drop_path\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.intermediate\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.output\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.output.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.0.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.output\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.drop_path\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.intermediate\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.output\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.output.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.1.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.output\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.drop_path\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.intermediate\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.output\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.output.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.2.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.output\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.drop_path\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.intermediate\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.output\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.output.dense\n","model.pixel_level_module.encoder.encoder.levels.1.layers.3.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.1.downsample\n","model.pixel_level_module.encoder.encoder.levels.1.downsample.reduction\n","model.pixel_level_module.encoder.encoder.levels.1.downsample.norm\n","model.pixel_level_module.encoder.encoder.levels.2\n","model.pixel_level_module.encoder.encoder.levels.2.layers\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.0.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.1.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.2.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.3.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.4.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.5.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.6.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.7.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.8.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.9.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.10.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.11.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.12.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.13.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.14.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.15.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.16.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.drop_path\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.intermediate\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.output\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.output.dense\n","model.pixel_level_module.encoder.encoder.levels.2.layers.17.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.2.downsample\n","model.pixel_level_module.encoder.encoder.levels.2.downsample.reduction\n","model.pixel_level_module.encoder.encoder.levels.2.downsample.norm\n","model.pixel_level_module.encoder.encoder.levels.3\n","model.pixel_level_module.encoder.encoder.levels.3.layers\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.drop_path\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.intermediate\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.0.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.drop_path\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.intermediate\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.1.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.drop_path\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.intermediate\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.2.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.drop_path\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.intermediate\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.3.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.layernorm_before\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.query.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.key\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.base_layer\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.lora_dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.lora_dropout.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.lora_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.lora_A.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.lora_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.lora_B.default\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.lora_embedding_A\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.value.lora_embedding_B\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.self.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.attention.output.dropout\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.drop_path\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.layernorm_after\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.intermediate\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.intermediate.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.intermediate.intermediate_act_fn\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.output\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.output.dense\n","model.pixel_level_module.encoder.encoder.levels.3.layers.4.output.dropout\n","model.pixel_level_module.encoder.hidden_states_norms\n","model.pixel_level_module.encoder.hidden_states_norms.stage1\n","model.pixel_level_module.encoder.hidden_states_norms.stage2\n","model.pixel_level_module.encoder.hidden_states_norms.stage3\n","model.pixel_level_module.encoder.hidden_states_norms.stage4\n","model.pixel_level_module.decoder\n","model.pixel_level_module.decoder.position_embedding\n","model.pixel_level_module.decoder.input_projections\n","model.pixel_level_module.decoder.input_projections.0\n","model.pixel_level_module.decoder.input_projections.0.0\n","model.pixel_level_module.decoder.input_projections.0.1\n","model.pixel_level_module.decoder.input_projections.1\n","model.pixel_level_module.decoder.input_projections.1.0\n","model.pixel_level_module.decoder.input_projections.1.1\n","model.pixel_level_module.decoder.input_projections.2\n","model.pixel_level_module.decoder.input_projections.2.0\n","model.pixel_level_module.decoder.input_projections.2.1\n","model.pixel_level_module.decoder.encoder\n","model.pixel_level_module.decoder.encoder.layers\n","model.pixel_level_module.decoder.encoder.layers.0\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.sampling_offsets\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.attention_weights\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.value_proj\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn.output_proj\n","model.pixel_level_module.decoder.encoder.layers.0.self_attn_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.0.fc1\n","model.pixel_level_module.decoder.encoder.layers.0.fc2\n","model.pixel_level_module.decoder.encoder.layers.0.final_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.1\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.sampling_offsets\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.attention_weights\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.value_proj\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn.output_proj\n","model.pixel_level_module.decoder.encoder.layers.1.self_attn_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.1.fc1\n","model.pixel_level_module.decoder.encoder.layers.1.fc2\n","model.pixel_level_module.decoder.encoder.layers.1.final_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.2\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.sampling_offsets\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.attention_weights\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.value_proj\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn.output_proj\n","model.pixel_level_module.decoder.encoder.layers.2.self_attn_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.2.fc1\n","model.pixel_level_module.decoder.encoder.layers.2.fc2\n","model.pixel_level_module.decoder.encoder.layers.2.final_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.3\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.sampling_offsets\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.attention_weights\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.value_proj\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn.output_proj\n","model.pixel_level_module.decoder.encoder.layers.3.self_attn_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.3.fc1\n","model.pixel_level_module.decoder.encoder.layers.3.fc2\n","model.pixel_level_module.decoder.encoder.layers.3.final_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.4\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.sampling_offsets\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.attention_weights\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.value_proj\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn.output_proj\n","model.pixel_level_module.decoder.encoder.layers.4.self_attn_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.4.fc1\n","model.pixel_level_module.decoder.encoder.layers.4.fc2\n","model.pixel_level_module.decoder.encoder.layers.4.final_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.5\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.sampling_offsets\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.attention_weights\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.value_proj\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn.output_proj\n","model.pixel_level_module.decoder.encoder.layers.5.self_attn_layer_norm\n","model.pixel_level_module.decoder.encoder.layers.5.fc1\n","model.pixel_level_module.decoder.encoder.layers.5.fc2\n","model.pixel_level_module.decoder.encoder.layers.5.final_layer_norm\n","model.pixel_level_module.decoder.mask_projection\n","model.pixel_level_module.decoder.adapter_1\n","model.pixel_level_module.decoder.adapter_1.0\n","model.pixel_level_module.decoder.adapter_1.1\n","model.pixel_level_module.decoder.layer_1\n","model.pixel_level_module.decoder.layer_1.0\n","model.pixel_level_module.decoder.layer_1.1\n","model.pixel_level_module.decoder.layer_1.2\n","model.transformer_module\n","model.transformer_module.position_embedder\n","model.transformer_module.queries_embedder\n","model.transformer_module.decoder\n","model.transformer_module.decoder.query_transformer\n","model.transformer_module.decoder.query_transformer.decoder\n","model.transformer_module.decoder.query_transformer.decoder.layers\n","model.transformer_module.decoder.query_transformer.decoder.layers.0\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.self_attn\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.self_attn.out_proj\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.multihead_attn\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.multihead_attn.out_proj\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.linear1\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.dropout\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.linear2\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm1\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm2\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.norm3\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.dropout1\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.dropout2\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.dropout3\n","model.transformer_module.decoder.query_transformer.decoder.layers.0.activation\n","model.transformer_module.decoder.query_transformer.decoder.layers.1\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.self_attn\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.self_attn.out_proj\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.multihead_attn\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.multihead_attn.out_proj\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.linear1\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.dropout\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.linear2\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm1\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm2\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.norm3\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.dropout1\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.dropout2\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.dropout3\n","model.transformer_module.decoder.query_transformer.decoder.layers.1.activation\n","model.transformer_module.decoder.query_transformer.decoder.norm\n","model.transformer_module.decoder.decoder_norm\n","model.transformer_module.decoder.layers\n","model.transformer_module.decoder.layers.0\n","model.transformer_module.decoder.layers.0.cross_attn\n","model.transformer_module.decoder.layers.0.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.0.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.0.cross_attn.norm\n","model.transformer_module.decoder.layers.0.cross_attn.dropout\n","model.transformer_module.decoder.layers.0.cross_attn.activation\n","model.transformer_module.decoder.layers.0.self_attn\n","model.transformer_module.decoder.layers.0.self_attn.self_attn\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.0.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.0.self_attn.norm\n","model.transformer_module.decoder.layers.0.self_attn.dropout\n","model.transformer_module.decoder.layers.0.self_attn.activation\n","model.transformer_module.decoder.layers.0.ffn\n","model.transformer_module.decoder.layers.0.ffn.linear1\n","model.transformer_module.decoder.layers.0.ffn.dropout\n","model.transformer_module.decoder.layers.0.ffn.linear2\n","model.transformer_module.decoder.layers.0.ffn.norm\n","model.transformer_module.decoder.layers.0.ffn.activation\n","model.transformer_module.decoder.layers.1\n","model.transformer_module.decoder.layers.1.cross_attn\n","model.transformer_module.decoder.layers.1.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.1.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.1.cross_attn.norm\n","model.transformer_module.decoder.layers.1.cross_attn.dropout\n","model.transformer_module.decoder.layers.1.cross_attn.activation\n","model.transformer_module.decoder.layers.1.self_attn\n","model.transformer_module.decoder.layers.1.self_attn.self_attn\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.1.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.1.self_attn.norm\n","model.transformer_module.decoder.layers.1.self_attn.dropout\n","model.transformer_module.decoder.layers.1.self_attn.activation\n","model.transformer_module.decoder.layers.1.ffn\n","model.transformer_module.decoder.layers.1.ffn.linear1\n","model.transformer_module.decoder.layers.1.ffn.dropout\n","model.transformer_module.decoder.layers.1.ffn.linear2\n","model.transformer_module.decoder.layers.1.ffn.norm\n","model.transformer_module.decoder.layers.1.ffn.activation\n","model.transformer_module.decoder.layers.2\n","model.transformer_module.decoder.layers.2.cross_attn\n","model.transformer_module.decoder.layers.2.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.2.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.2.cross_attn.norm\n","model.transformer_module.decoder.layers.2.cross_attn.dropout\n","model.transformer_module.decoder.layers.2.cross_attn.activation\n","model.transformer_module.decoder.layers.2.self_attn\n","model.transformer_module.decoder.layers.2.self_attn.self_attn\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.2.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.2.self_attn.norm\n","model.transformer_module.decoder.layers.2.self_attn.dropout\n","model.transformer_module.decoder.layers.2.self_attn.activation\n","model.transformer_module.decoder.layers.2.ffn\n","model.transformer_module.decoder.layers.2.ffn.linear1\n","model.transformer_module.decoder.layers.2.ffn.dropout\n","model.transformer_module.decoder.layers.2.ffn.linear2\n","model.transformer_module.decoder.layers.2.ffn.norm\n","model.transformer_module.decoder.layers.2.ffn.activation\n","model.transformer_module.decoder.layers.3\n","model.transformer_module.decoder.layers.3.cross_attn\n","model.transformer_module.decoder.layers.3.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.3.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.3.cross_attn.norm\n","model.transformer_module.decoder.layers.3.cross_attn.dropout\n","model.transformer_module.decoder.layers.3.cross_attn.activation\n","model.transformer_module.decoder.layers.3.self_attn\n","model.transformer_module.decoder.layers.3.self_attn.self_attn\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.3.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.3.self_attn.norm\n","model.transformer_module.decoder.layers.3.self_attn.dropout\n","model.transformer_module.decoder.layers.3.self_attn.activation\n","model.transformer_module.decoder.layers.3.ffn\n","model.transformer_module.decoder.layers.3.ffn.linear1\n","model.transformer_module.decoder.layers.3.ffn.dropout\n","model.transformer_module.decoder.layers.3.ffn.linear2\n","model.transformer_module.decoder.layers.3.ffn.norm\n","model.transformer_module.decoder.layers.3.ffn.activation\n","model.transformer_module.decoder.layers.4\n","model.transformer_module.decoder.layers.4.cross_attn\n","model.transformer_module.decoder.layers.4.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.4.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.4.cross_attn.norm\n","model.transformer_module.decoder.layers.4.cross_attn.dropout\n","model.transformer_module.decoder.layers.4.cross_attn.activation\n","model.transformer_module.decoder.layers.4.self_attn\n","model.transformer_module.decoder.layers.4.self_attn.self_attn\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.4.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.4.self_attn.norm\n","model.transformer_module.decoder.layers.4.self_attn.dropout\n","model.transformer_module.decoder.layers.4.self_attn.activation\n","model.transformer_module.decoder.layers.4.ffn\n","model.transformer_module.decoder.layers.4.ffn.linear1\n","model.transformer_module.decoder.layers.4.ffn.dropout\n","model.transformer_module.decoder.layers.4.ffn.linear2\n","model.transformer_module.decoder.layers.4.ffn.norm\n","model.transformer_module.decoder.layers.4.ffn.activation\n","model.transformer_module.decoder.layers.5\n","model.transformer_module.decoder.layers.5.cross_attn\n","model.transformer_module.decoder.layers.5.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.5.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.5.cross_attn.norm\n","model.transformer_module.decoder.layers.5.cross_attn.dropout\n","model.transformer_module.decoder.layers.5.cross_attn.activation\n","model.transformer_module.decoder.layers.5.self_attn\n","model.transformer_module.decoder.layers.5.self_attn.self_attn\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.5.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.5.self_attn.norm\n","model.transformer_module.decoder.layers.5.self_attn.dropout\n","model.transformer_module.decoder.layers.5.self_attn.activation\n","model.transformer_module.decoder.layers.5.ffn\n","model.transformer_module.decoder.layers.5.ffn.linear1\n","model.transformer_module.decoder.layers.5.ffn.dropout\n","model.transformer_module.decoder.layers.5.ffn.linear2\n","model.transformer_module.decoder.layers.5.ffn.norm\n","model.transformer_module.decoder.layers.5.ffn.activation\n","model.transformer_module.decoder.layers.6\n","model.transformer_module.decoder.layers.6.cross_attn\n","model.transformer_module.decoder.layers.6.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.6.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.6.cross_attn.norm\n","model.transformer_module.decoder.layers.6.cross_attn.dropout\n","model.transformer_module.decoder.layers.6.cross_attn.activation\n","model.transformer_module.decoder.layers.6.self_attn\n","model.transformer_module.decoder.layers.6.self_attn.self_attn\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.6.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.6.self_attn.norm\n","model.transformer_module.decoder.layers.6.self_attn.dropout\n","model.transformer_module.decoder.layers.6.self_attn.activation\n","model.transformer_module.decoder.layers.6.ffn\n","model.transformer_module.decoder.layers.6.ffn.linear1\n","model.transformer_module.decoder.layers.6.ffn.dropout\n","model.transformer_module.decoder.layers.6.ffn.linear2\n","model.transformer_module.decoder.layers.6.ffn.norm\n","model.transformer_module.decoder.layers.6.ffn.activation\n","model.transformer_module.decoder.layers.7\n","model.transformer_module.decoder.layers.7.cross_attn\n","model.transformer_module.decoder.layers.7.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.7.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.7.cross_attn.norm\n","model.transformer_module.decoder.layers.7.cross_attn.dropout\n","model.transformer_module.decoder.layers.7.cross_attn.activation\n","model.transformer_module.decoder.layers.7.self_attn\n","model.transformer_module.decoder.layers.7.self_attn.self_attn\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.7.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.7.self_attn.norm\n","model.transformer_module.decoder.layers.7.self_attn.dropout\n","model.transformer_module.decoder.layers.7.self_attn.activation\n","model.transformer_module.decoder.layers.7.ffn\n","model.transformer_module.decoder.layers.7.ffn.linear1\n","model.transformer_module.decoder.layers.7.ffn.dropout\n","model.transformer_module.decoder.layers.7.ffn.linear2\n","model.transformer_module.decoder.layers.7.ffn.norm\n","model.transformer_module.decoder.layers.7.ffn.activation\n","model.transformer_module.decoder.layers.8\n","model.transformer_module.decoder.layers.8.cross_attn\n","model.transformer_module.decoder.layers.8.cross_attn.multihead_attn\n","model.transformer_module.decoder.layers.8.cross_attn.multihead_attn.out_proj\n","model.transformer_module.decoder.layers.8.cross_attn.norm\n","model.transformer_module.decoder.layers.8.cross_attn.dropout\n","model.transformer_module.decoder.layers.8.cross_attn.activation\n","model.transformer_module.decoder.layers.8.self_attn\n","model.transformer_module.decoder.layers.8.self_attn.self_attn\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.k_proj\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.v_proj\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.q_proj\n","model.transformer_module.decoder.layers.8.self_attn.self_attn.out_proj\n","model.transformer_module.decoder.layers.8.self_attn.norm\n","model.transformer_module.decoder.layers.8.self_attn.dropout\n","model.transformer_module.decoder.layers.8.self_attn.activation\n","model.transformer_module.decoder.layers.8.ffn\n","model.transformer_module.decoder.layers.8.ffn.linear1\n","model.transformer_module.decoder.layers.8.ffn.dropout\n","model.transformer_module.decoder.layers.8.ffn.linear2\n","model.transformer_module.decoder.layers.8.ffn.norm\n","model.transformer_module.decoder.layers.8.ffn.activation\n","model.transformer_module.decoder.query_input_projection\n","model.transformer_module.decoder.query_input_projection.original_module\n","model.transformer_module.decoder.query_input_projection.modules_to_save\n","model.transformer_module.decoder.query_input_projection.modules_to_save.default\n","model.transformer_module.decoder.class_embed\n","model.transformer_module.decoder.class_embed.original_module\n","model.transformer_module.decoder.class_embed.modules_to_save\n","model.transformer_module.decoder.class_embed.modules_to_save.default\n","model.transformer_module.decoder.mask_embed\n","model.transformer_module.decoder.mask_embed.original_module\n","model.transformer_module.decoder.mask_embed.original_module.layers\n","model.transformer_module.decoder.mask_embed.original_module.layers.0\n","model.transformer_module.decoder.mask_embed.original_module.layers.0.0\n","model.transformer_module.decoder.mask_embed.original_module.layers.0.1\n","model.transformer_module.decoder.mask_embed.original_module.layers.1\n","model.transformer_module.decoder.mask_embed.original_module.layers.1.0\n","model.transformer_module.decoder.mask_embed.original_module.layers.1.1\n","model.transformer_module.decoder.mask_embed.original_module.layers.2\n","model.transformer_module.decoder.mask_embed.original_module.layers.2.0\n","model.transformer_module.decoder.mask_embed.original_module.layers.2.1\n","model.transformer_module.decoder.mask_embed.modules_to_save\n","model.transformer_module.decoder.mask_embed.modules_to_save.default\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.0\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.0.0\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.0.1\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.1\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.1.0\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.1.1\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.2\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.2.0\n","model.transformer_module.decoder.mask_embed.modules_to_save.default.layers.2.1\n","model.transformer_module.level_embed\n","model.transformer_module.level_embed.original_module\n","model.transformer_module.level_embed.modules_to_save\n","model.transformer_module.level_embed.modules_to_save.default\n","model.task_encoder\n","model.task_encoder.task_mlp\n","model.task_encoder.task_mlp.layers\n","model.task_encoder.task_mlp.layers.0\n","model.task_encoder.task_mlp.layers.0.0\n","model.task_encoder.task_mlp.layers.0.1\n","model.task_encoder.task_mlp.layers.1\n","model.task_encoder.task_mlp.layers.1.0\n","model.task_encoder.task_mlp.layers.1.1\n","model.text_mapper\n","model.text_mapper.original_module\n","model.text_mapper.original_module.text_encoder\n","model.text_mapper.original_module.text_encoder.transformer\n","model.text_mapper.original_module.text_encoder.transformer.layers\n","model.text_mapper.original_module.text_encoder.transformer.layers.0\n","model.text_mapper.original_module.text_encoder.transformer.layers.0.self_attn\n","model.text_mapper.original_module.text_encoder.transformer.layers.0.self_attn.out_proj\n","model.text_mapper.original_module.text_encoder.transformer.layers.0.layer_norm1\n","model.text_mapper.original_module.text_encoder.transformer.layers.0.mlp\n","model.text_mapper.original_module.text_encoder.transformer.layers.0.mlp.activation_fn\n","model.text_mapper.original_module.text_encoder.transformer.layers.0.mlp.fc1\n","model.text_mapper.original_module.text_encoder.transformer.layers.0.mlp.fc2\n","model.text_mapper.original_module.text_encoder.transformer.layers.0.layer_norm2\n","model.text_mapper.original_module.text_encoder.transformer.layers.1\n","model.text_mapper.original_module.text_encoder.transformer.layers.1.self_attn\n","model.text_mapper.original_module.text_encoder.transformer.layers.1.self_attn.out_proj\n","model.text_mapper.original_module.text_encoder.transformer.layers.1.layer_norm1\n","model.text_mapper.original_module.text_encoder.transformer.layers.1.mlp\n","model.text_mapper.original_module.text_encoder.transformer.layers.1.mlp.activation_fn\n","model.text_mapper.original_module.text_encoder.transformer.layers.1.mlp.fc1\n","model.text_mapper.original_module.text_encoder.transformer.layers.1.mlp.fc2\n","model.text_mapper.original_module.text_encoder.transformer.layers.1.layer_norm2\n","model.text_mapper.original_module.text_encoder.transformer.layers.2\n","model.text_mapper.original_module.text_encoder.transformer.layers.2.self_attn\n","model.text_mapper.original_module.text_encoder.transformer.layers.2.self_attn.out_proj\n","model.text_mapper.original_module.text_encoder.transformer.layers.2.layer_norm1\n","model.text_mapper.original_module.text_encoder.transformer.layers.2.mlp\n","model.text_mapper.original_module.text_encoder.transformer.layers.2.mlp.activation_fn\n","model.text_mapper.original_module.text_encoder.transformer.layers.2.mlp.fc1\n","model.text_mapper.original_module.text_encoder.transformer.layers.2.mlp.fc2\n","model.text_mapper.original_module.text_encoder.transformer.layers.2.layer_norm2\n","model.text_mapper.original_module.text_encoder.transformer.layers.3\n","model.text_mapper.original_module.text_encoder.transformer.layers.3.self_attn\n","model.text_mapper.original_module.text_encoder.transformer.layers.3.self_attn.out_proj\n","model.text_mapper.original_module.text_encoder.transformer.layers.3.layer_norm1\n","model.text_mapper.original_module.text_encoder.transformer.layers.3.mlp\n","model.text_mapper.original_module.text_encoder.transformer.layers.3.mlp.activation_fn\n","model.text_mapper.original_module.text_encoder.transformer.layers.3.mlp.fc1\n","model.text_mapper.original_module.text_encoder.transformer.layers.3.mlp.fc2\n","model.text_mapper.original_module.text_encoder.transformer.layers.3.layer_norm2\n","model.text_mapper.original_module.text_encoder.transformer.layers.4\n","model.text_mapper.original_module.text_encoder.transformer.layers.4.self_attn\n","model.text_mapper.original_module.text_encoder.transformer.layers.4.self_attn.out_proj\n","model.text_mapper.original_module.text_encoder.transformer.layers.4.layer_norm1\n","model.text_mapper.original_module.text_encoder.transformer.layers.4.mlp\n","model.text_mapper.original_module.text_encoder.transformer.layers.4.mlp.activation_fn\n","model.text_mapper.original_module.text_encoder.transformer.layers.4.mlp.fc1\n","model.text_mapper.original_module.text_encoder.transformer.layers.4.mlp.fc2\n","model.text_mapper.original_module.text_encoder.transformer.layers.4.layer_norm2\n","model.text_mapper.original_module.text_encoder.transformer.layers.5\n","model.text_mapper.original_module.text_encoder.transformer.layers.5.self_attn\n","model.text_mapper.original_module.text_encoder.transformer.layers.5.self_attn.out_proj\n","model.text_mapper.original_module.text_encoder.transformer.layers.5.layer_norm1\n","model.text_mapper.original_module.text_encoder.transformer.layers.5.mlp\n","model.text_mapper.original_module.text_encoder.transformer.layers.5.mlp.activation_fn\n","model.text_mapper.original_module.text_encoder.transformer.layers.5.mlp.fc1\n","model.text_mapper.original_module.text_encoder.transformer.layers.5.mlp.fc2\n","model.text_mapper.original_module.text_encoder.transformer.layers.5.layer_norm2\n","model.text_mapper.original_module.text_encoder.ln_final\n","model.text_mapper.original_module.text_encoder.token_embedding\n","model.text_mapper.original_module.text_projector\n","model.text_mapper.original_module.text_projector.layers\n","model.text_mapper.original_module.text_projector.layers.0\n","model.text_mapper.original_module.text_projector.layers.0.0\n","model.text_mapper.original_module.text_projector.layers.0.1\n","model.text_mapper.original_module.text_projector.layers.1\n","model.text_mapper.original_module.text_projector.layers.1.0\n","model.text_mapper.original_module.text_projector.layers.1.1\n","model.text_mapper.original_module.prompt_ctx\n","model.text_mapper.modules_to_save\n","model.text_mapper.modules_to_save.default\n","model.text_mapper.modules_to_save.default.text_encoder\n","model.text_mapper.modules_to_save.default.text_encoder.transformer\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0.self_attn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0.self_attn.out_proj\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0.layer_norm1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0.mlp\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0.mlp.activation_fn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0.mlp.fc1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0.mlp.fc2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.0.layer_norm2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1.self_attn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1.self_attn.out_proj\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1.layer_norm1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1.mlp\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1.mlp.activation_fn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1.mlp.fc1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1.mlp.fc2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.1.layer_norm2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2.self_attn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2.self_attn.out_proj\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2.layer_norm1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2.mlp\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2.mlp.activation_fn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2.mlp.fc1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2.mlp.fc2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.2.layer_norm2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3.self_attn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3.self_attn.out_proj\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3.layer_norm1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3.mlp\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3.mlp.activation_fn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3.mlp.fc1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3.mlp.fc2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.3.layer_norm2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4.self_attn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4.self_attn.out_proj\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4.layer_norm1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4.mlp\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4.mlp.activation_fn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4.mlp.fc1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4.mlp.fc2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.4.layer_norm2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5.self_attn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5.self_attn.out_proj\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5.layer_norm1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5.mlp\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5.mlp.activation_fn\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5.mlp.fc1\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5.mlp.fc2\n","model.text_mapper.modules_to_save.default.text_encoder.transformer.layers.5.layer_norm2\n","model.text_mapper.modules_to_save.default.text_encoder.ln_final\n","model.text_mapper.modules_to_save.default.text_encoder.token_embedding\n","model.text_mapper.modules_to_save.default.text_projector\n","model.text_mapper.modules_to_save.default.text_projector.layers\n","model.text_mapper.modules_to_save.default.text_projector.layers.0\n","model.text_mapper.modules_to_save.default.text_projector.layers.0.0\n","model.text_mapper.modules_to_save.default.text_projector.layers.0.1\n","model.text_mapper.modules_to_save.default.text_projector.layers.1\n","model.text_mapper.modules_to_save.default.text_projector.layers.1.0\n","model.text_mapper.modules_to_save.default.text_projector.layers.1.1\n","model.text_mapper.modules_to_save.default.prompt_ctx\n","matcher\n","criterion\n"]}]},{"cell_type":"code","source":["####\n","#@title Pytorch Finetuning with PEFT LORA\n","####\n","\n","####\n","#@title Pytorch Finetuning pretrained model with railsem19 both train data, val data, transforms and metrics\n","####\n","import peft\n","from peft import LoraConfig, get_peft_model\n","\n","#Dataset Loading\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\")\n","\n","dataset_subset = railsem_ds[\"validation\"].train_test_split(test_size=0.2)\n","train_subset = dataset_subset[\"train\"]\n","val_subset = dataset_subset[\"test\"]\n","\n","#Get the id2label, label2id, labels from hf repo for rail19sem dataset with 19 classes\n","json_file = hf_hub_download(repo_id=\"BhavanaMalla/railsem19-semantic\",\n","                filename=\"labels_info.json\",\n","                repo_type=\"dataset\",\n","                local_dir=\"/content\")\n","\n","with open(json_file, \"r\") as f:\n","    labels_info = json.load(f)\n","\n","id2label, label2id, labels, color_palette, readable_labels = labels_info.values()\n","\n","#custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, semantic_mask = self.dataset[idx][\"image\"], self.dataset[idx][\"semantic_mask_label\"]\n","        semantic_mask = np.array(semantic_mask)\n","        original_image = np.array(image)\n","        target_mask = np.array(semantic_mask)\n","\n","        if self.transforms:\n","            transformed = self.transforms(image=original_image, mask=target_mask)\n","            image, semantic_mask = transformed[\"image\"], transformed[\"mask\"]\n","            image = image.transpose(2, 0, 1)\n","\n","        return image, semantic_mask, original_image, target_mask\n","\n","#model config\n","oneformer_cityscapes_model = AutoModelForUniversalSegmentation.from_pretrained(checkpoint,\n","                                                                        is_training=True,\n","                                                                        id2label=id2label,\n","                                                                        ignore_mismatched_sizes=True)\n","#processor\n","cityscapes_processor = AutoProcessor.from_pretrained(checkpoint,\n","                                                    do_reduce_labels=False,\n","                                                    do_normalize=False,\n","                                                    do_rescale=False,\n","                                                    do_resize=False,)\n","\n","cityscapes_processor.image_processor.num_text = oneformer_cityscapes_model.config.num_queries - oneformer_cityscapes_model.config.text_encoder_n_ctx\n","\n","#wandb init\n","config = dict(\n","    epochs=1,\n","    classes=oneformer_cityscapes_model.config.num_classes,\n","    batch_size=2,\n","    dataset=f\"{dataset_name}_RailSem19\",\n","    architecture=f\"OF_{ckpt_name}_L\"\n",")\n","wandb.init(project=f\"OF_LORA_Finetuning_{dataset_name}_{ckpt_name}_rail19_TrainVal\", config=config)\n","print(f\"Wandb Config: {wandb.config}\")\n","\n","#Tranforms\n","#IMAGENET\n","MEAN = np.array([0.485, 0.456, 0.406])\n","STD = np.array([0.229, 0.224, 0.225])\n","\n","train_transform = A.Compose([\n","    A.LongestMaxSize(max_size=1333),\n","    A.RandomCrop(width=512, height=512),\n","    A.HorizontalFlip(p=0.5),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","val_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","test_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","#datasets\n","# train_dataset = CustomDataset(railsem_ds[\"train\"], train_transform)\n","# val_dataset = CustomDataset(railsem_ds[\"validation\"], val_transform)\n","#test_dataset = CustomDataset(railsem_ds[\"test\"], test_transform)\n","train_dataset = CustomDataset(train_subset, train_transform)\n","val_dataset = CustomDataset(val_subset, val_transform)\n","\n","\n","#collate_fn\n","def collate_fn(batch):\n","    inputs = list(zip(*batch))\n","    images = inputs[0]\n","    segmentation_maps = inputs[1]\n","\n","    batch = cityscapes_processor(images,\n","                                segmentation_maps=segmentation_maps,\n","                                task_inputs=[\"semantic\"] * len(images),\n","                                return_tensors=\"pt\")\n","    batch[\"original_images\"] = inputs[2]\n","    batch[\"original_segmentation_maps\"] = inputs[3]\n","    return batch\n","\n","#Dataloaders\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","val_dataloader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","#test_dataloader = DataLoader(dataset=test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n","\n","#Training\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","oneformer_cityscapes_model.to(device)\n","\n","optimizer = AdamW(params=oneformer_cityscapes_model.parameters(), lr=5e-5)\n","\n","metric = evaluate.load(\"mean_iou\")\n","\n","model_results = {\n","    \"train_loss\": [],\n","    \"eval_loss\": [],\n","    \"miou\": []}\n","\n","#print trainable params\n","def print_trainable_parameters(model, print_msg):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for name, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(f\"\\n-----{print_msg}......\")\n","    print(\n","        f\"trainable params: {trainable_params / 1e+6:.2f}M || all params: {all_param / 1e+6:.2f}M || trainable%: {100 * trainable_params / all_param:.2f}%\"\n","    )\n","\n","print_trainable_parameters(oneformer_cityscapes_model, print_msg=\"Base Model Trainable Params\")\n","\n","#PEFT LORA CONFIG\n","config = LoraConfig(\n","    r=32,\n","    lora_alpha=32,\n","    target_modules=[\"query\", \"value\"],\n","    lora_dropout=0.1,\n","    bias=\"lora_only\",\n","    modules_to_save=[\"text_mapper\", \"transformer_module.decoder.query_input_projection\", \"transformer_module.decoder.mask_embed\", \"transformer_module.decoder.class_embed\", \"transformer_module.decoder.level_embed\"],\n",")\n","lora_model = get_peft_model(oneformer_cityscapes_model, config)\n","print_trainable_parameters(lora_model, print_msg=\"Lora Model Trainable Params\")\n","\n","#Training Loop\n","running_loss = 0.0\n","running_eval_loss = 0.0\n","num_samples = 0\n","num_samples_eval = 0\n","batch_cnt = 0\n","for epoch in range(wandb.config.epochs):\n","    lora_model.train()\n","    for idx, batch in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"mask_labels\":  [labels.to(device) for labels in batch[\"mask_labels\"]],\n","            \"class_labels\" : [labels.to(device) for labels in batch[\"class_labels\"]],\n","            \"pixel_mask\" : batch[\"pixel_mask\"].to(device),\n","            \"text_inputs\" : batch[\"text_inputs\"].to(device),\n","            \"task_inputs\" : batch[\"task_inputs\"].to(device),\n","        }\n","        batch_dict = {k: v.to(device) if isinstance(v, torch.Tensor) else [x.to(device) for x in v] for k, v in batch_dict.items()}\n","\n","        #train forward pass\n","        outputs = lora_model(**batch_dict)\n","        train_loss = outputs.loss\n","        running_loss += train_loss.item()\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples += batch_size\n","        batch_cnt += 1\n","\n","        print(f\"Train Loss: {train_loss.item()}\")\n","\n","        # Report metrics every 25th batch\n","        if ((batch_cnt + 1) % 2) == 0:\n","            print(\"\\n------------------------------------------------------\\n\")\n","            wandb.log({\"epoch\": epoch, \"train_loss\": train_loss.item()}, step=num_samples)\n","            print(f\"Loss after {str(num_samples).zfill(5)} examples: {train_loss.item():.3f}\")\n","            print(\"\\n------------------------------------------------------\\n\")\n","\n","        #backward pass\n","        train_loss.backward()\n","        optimizer.step()\n","        del batch\n","\n","    train_loss_per_epoch = running_loss / num_samples\n","    wandb.log({\"epoch\": epoch, \"train_epoch_loss\": train_loss_per_epoch})\n","    model_results[\"train_loss\"].append(train_loss_per_epoch)\n","    #Eval Loop\n","    lora_model.eval()\n","    for idx, batch in enumerate(tqdm(val_dataloader)):\n","        #eval forward pass\n","        with torch.inference_mode():\n","            eval_outputs = lora_model(pixel_values=batch_dict[\"pixel_values\"],\n","                                               pixel_mask=batch_dict[\"pixel_mask\"],\n","                                               task_inputs=batch_dict[\"task_inputs\"]\n","                                               )\n","        eval_loss = eval_outputs.loss\n","        running_eval_loss += eval_loss.item()\n","\n","\n","        #Post process segmentation\n","        original_images = batch[\"original_images\"]\n","        target_sizes = [(img.shape[0], img.shape[1]) for img in original_images]\n","        predicted_segmentation_maps = cityscapes_processor.post_process_semantic_segmentation(eval_outputs,\n","                                                                                             target_sizes=target_sizes)\n","        #eval metrics\n","        # get ground truth segmentation maps\n","        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n","        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n","\n","        batch_size = batch[\"pixel_values\"].size(0)\n","        num_samples_eval += batch_size\n","        wandb.log({\"epoch\": epoch, \"eval_loss\": eval_loss.item()}, step=num_samples_eval)\n","        print(f\"Eval Loss: {eval_loss.item()}\")\n","\n","    eval_loss_per_epoch = running_eval_loss / num_samples_eval\n","    model_results[\"eval_loss\"].append(eval_loss_per_epoch)\n","    wandb.log({\"epoch\": epoch, \"eval_loss_per_epoch\": eval_loss_per_epoch})\n","    mean_iou_per_epoch = metric.compute(num_labels=len(id2label), ignore_index=255)\n","    model_results[\"miou\"].append(mean_iou_per_epoch)\n","    wandb.log({\"epoch\": epoch, \"mean_iou\": mean_iou_per_epoch[\"mean_iou\"]})\n","    print(\"Mean IoU:\", mean_iou_per_epoch[\"mean_iou\"])\n","    del batch\n","\n","# Save the model in the exchangeable ONNX format\n","torch.onnx.export(lora_model, f\"oneformer_lora_{dataset_name}_{ckpt_name}_finetune.onnx\")\n","wandb.save(f\"oneformer_lora_{dataset_name}_{ckpt_name}_finetune.onnx\")\n","#Save the model to hugging face\n","model_id = f\"oneformer_lora_{dataset_name}_{ckpt_name}_finetune\"\n","lora_model.save_pretrained(model_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"SSDVYtMX27GQ","executionInfo":{"status":"error","timestamp":1705326308006,"user_tz":-60,"elapsed":8982,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"21384eef-3331-4944-d684-0a091ece2573"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'load_dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-a3d7708f14b8>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Dataset Loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrailsem_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BhavanaMalla/railsem19-semantic-split355\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdataset_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrailsem_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gaQwSX1uNbFq"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1qGuv86Wehz8GpkJqKF8P7HqSvqTRCwyG","authorship_tag":"ABX9TyMq8l5xa6LGJU4nvKH9f2/j"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"91fcbd379a6e417c970154b6e1bd2d3e":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_1559eb2163974aa68f5403b7380b5747","IPY_MODEL_fa1d7006f6f9463aba46e07829a7026b","IPY_MODEL_080d6ad2a5f54e2eb442eba49f916dd4","IPY_MODEL_72fbe645c1594252b9cfc06245dfb0cb"],"layout":"IPY_MODEL_b5023b041a3b4a0aa811199f81e76bdf"}},"c5ef533a0632448e9df7cf4f5365b4ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c29f5bb26ef341de801404875db2337f","placeholder":"​","style":"IPY_MODEL_6a9a4c0fb3d547acaadc11ccd9dd32f9","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"9af46f20f4b14c87987e5c51a674b5bc":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_aefbe91b8f3b46c6accc1062a6c1ed41","placeholder":"​","style":"IPY_MODEL_da4d73b3f37b47d78a5a261626db1fd6","value":""}},"4ce183ee79bc45f1b25d151fe5f016f0":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_b01c198438ea430d993fa24ba249c341","style":"IPY_MODEL_a5a2ea8dad014a018bdbb06e9f5efdb2","value":true}},"eb0363c842d347abb3d1a4569c13ce10":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_cfcdf24ed8054b3eb631505c15315885","style":"IPY_MODEL_36c9fe3015124f5e96c26cb909516580","tooltip":""}},"2580c216f6004fa19682167e84da56fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_932857fecc924c01a9c508d1fca13346","placeholder":"​","style":"IPY_MODEL_db2867d55c504381b70d06e505339907","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"b5023b041a3b4a0aa811199f81e76bdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"c29f5bb26ef341de801404875db2337f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a9a4c0fb3d547acaadc11ccd9dd32f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aefbe91b8f3b46c6accc1062a6c1ed41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da4d73b3f37b47d78a5a261626db1fd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b01c198438ea430d993fa24ba249c341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5a2ea8dad014a018bdbb06e9f5efdb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfcdf24ed8054b3eb631505c15315885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36c9fe3015124f5e96c26cb909516580":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"932857fecc924c01a9c508d1fca13346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db2867d55c504381b70d06e505339907":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba661e51c1b14efbaaafce8151f6d6f0":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74ee7c50cd55495db07510ae70d927aa","placeholder":"​","style":"IPY_MODEL_95343f74a7c14d8e82e199c33f941520","value":"Connecting..."}},"74ee7c50cd55495db07510ae70d927aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95343f74a7c14d8e82e199c33f941520":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1559eb2163974aa68f5403b7380b5747":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69036b9161564aff85ded31225ae879d","placeholder":"​","style":"IPY_MODEL_e16b58dc0ba945e980b4176e1d6d84f7","value":"Token is valid (permission: write)."}},"fa1d7006f6f9463aba46e07829a7026b":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89cb6e3e172247f0a095aed16ee35eae","placeholder":"​","style":"IPY_MODEL_1f1ad0d18f554d799a17cc0c1160458e","value":"Your token has been saved in your configured git credential helpers (store)."}},"080d6ad2a5f54e2eb442eba49f916dd4":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f70caad1fee4c2db91c93ff00634de3","placeholder":"​","style":"IPY_MODEL_2aee003dfc1f445b8fecb5e3e320b5f6","value":"Your token has been saved to /root/.cache/huggingface/token"}},"72fbe645c1594252b9cfc06245dfb0cb":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d166fcbb93c4319b8d45811f2a96c46","placeholder":"​","style":"IPY_MODEL_1712bcf4e9a74a01b03dd2e5b3442ae3","value":"Login successful"}},"69036b9161564aff85ded31225ae879d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e16b58dc0ba945e980b4176e1d6d84f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89cb6e3e172247f0a095aed16ee35eae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f1ad0d18f554d799a17cc0c1160458e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f70caad1fee4c2db91c93ff00634de3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aee003dfc1f445b8fecb5e3e320b5f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d166fcbb93c4319b8d45811f2a96c46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1712bcf4e9a74a01b03dd2e5b3442ae3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"350f3749ea1c4efabc5328d6598dfd31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be59cd0267ee4b189bf6ca00dae062ca","IPY_MODEL_aeccb86c624947eeb9b75a6d4badb337","IPY_MODEL_0d23b07174434ce08dc5a5c1b23fa5cf"],"layout":"IPY_MODEL_86870893cb1e4a78961f446ad7ad2789"}},"be59cd0267ee4b189bf6ca00dae062ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e6d053044d14033bd86a54a520574be","placeholder":"​","style":"IPY_MODEL_dd02acc3c8df496fb9102d622f6d817c","value":"config.json: 100%"}},"aeccb86c624947eeb9b75a6d4badb337":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc7a6b53f65a4215bfebfaf26a6f1aef","max":78112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a34b46de23f473ca6f45d86328ece10","value":78112}},"0d23b07174434ce08dc5a5c1b23fa5cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2542210e2f5e4eaaab495d7b3975d66b","placeholder":"​","style":"IPY_MODEL_a3163cc54eb54d1f8d24b8e876e67a79","value":" 78.1k/78.1k [00:00&lt;00:00, 1.08MB/s]"}},"86870893cb1e4a78961f446ad7ad2789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e6d053044d14033bd86a54a520574be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd02acc3c8df496fb9102d622f6d817c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc7a6b53f65a4215bfebfaf26a6f1aef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a34b46de23f473ca6f45d86328ece10":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2542210e2f5e4eaaab495d7b3975d66b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3163cc54eb54d1f8d24b8e876e67a79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a93c9e7ca5448ddb37341fb3dec398a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3865263e6e8f4c479cfbbaf0c1f066d2","IPY_MODEL_adb1f1f39a3a47e3b52a66731682fa15","IPY_MODEL_7b2c556cfe524baa86ab3f7ab48d0f49"],"layout":"IPY_MODEL_d0e978d20f564e9c9cbaa2fbf03bd613"}},"3865263e6e8f4c479cfbbaf0c1f066d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ef722daea0947d19722945d6abbcad5","placeholder":"​","style":"IPY_MODEL_9efe717e9bce4f71805dbf20722f26d3","value":"pytorch_model.bin: 100%"}},"adb1f1f39a3a47e3b52a66731682fa15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_197f05bb8d6f40cea64ccadf86a6a124","max":892260781,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6318dfcb9b6467bb5a7595738999543","value":892260781}},"7b2c556cfe524baa86ab3f7ab48d0f49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_341943a43d4946098912afac5d326f40","placeholder":"​","style":"IPY_MODEL_c66d177ae36942b4862ad7df49c3d033","value":" 892M/892M [00:06&lt;00:00, 197MB/s]"}},"d0e978d20f564e9c9cbaa2fbf03bd613":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ef722daea0947d19722945d6abbcad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9efe717e9bce4f71805dbf20722f26d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"197f05bb8d6f40cea64ccadf86a6a124":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6318dfcb9b6467bb5a7595738999543":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"341943a43d4946098912afac5d326f40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c66d177ae36942b4862ad7df49c3d033":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81ea210c029c4f0695346ab719837a65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b37e35208da44f3c9963bf48ab762bbe","IPY_MODEL_b0f3e52bbfb74245b886c4cb0835dbb5","IPY_MODEL_9225137e684f4394a1c1b0341814ced9"],"layout":"IPY_MODEL_673ac3cdad9c4865be1c2ec7b3a180d1"}},"b37e35208da44f3c9963bf48ab762bbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e196113086a742a1ac1ae502e69800d3","placeholder":"​","style":"IPY_MODEL_3c648fc93cd84f26ae93cbe6ad987cf0","value":"preprocessor_config.json: 100%"}},"b0f3e52bbfb74245b886c4cb0835dbb5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abe5b15540b2409aa4107d06444936e5","max":1525,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1cfc842794af4cf8a2d246bab56de11a","value":1525}},"9225137e684f4394a1c1b0341814ced9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c181e28392243dc856d543d1037a083","placeholder":"​","style":"IPY_MODEL_79d4a4b35042499ab260d8ff93972961","value":" 1.52k/1.52k [00:00&lt;00:00, 65.2kB/s]"}},"673ac3cdad9c4865be1c2ec7b3a180d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e196113086a742a1ac1ae502e69800d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c648fc93cd84f26ae93cbe6ad987cf0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abe5b15540b2409aa4107d06444936e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cfc842794af4cf8a2d246bab56de11a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c181e28392243dc856d543d1037a083":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79d4a4b35042499ab260d8ff93972961":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f847256031349b9adf66ba7a9d80631":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7bcc109866244b2a578ef01ff8f43f9","IPY_MODEL_a1bd83bd6d7f47c8ad8590b773c99af6","IPY_MODEL_e013283446a645659244ba667c0b46be"],"layout":"IPY_MODEL_f547d118c723490db017b38af2ba84bd"}},"d7bcc109866244b2a578ef01ff8f43f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9f23447f3c84e5ca45ae5b4db393f23","placeholder":"​","style":"IPY_MODEL_eec98d54979848b0a27464865960716a","value":"tokenizer_config.json: 100%"}},"a1bd83bd6d7f47c8ad8590b773c99af6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_485a88e24ae4442cb2de8831323fca28","max":813,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf4e74547f104ee88aaf9695065d1431","value":813}},"e013283446a645659244ba667c0b46be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe79b59de5894978a6e1ae60c69ad06d","placeholder":"​","style":"IPY_MODEL_995afb8ce9a24be28681578581418fe8","value":" 813/813 [00:00&lt;00:00, 35.7kB/s]"}},"f547d118c723490db017b38af2ba84bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9f23447f3c84e5ca45ae5b4db393f23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eec98d54979848b0a27464865960716a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"485a88e24ae4442cb2de8831323fca28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf4e74547f104ee88aaf9695065d1431":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe79b59de5894978a6e1ae60c69ad06d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"995afb8ce9a24be28681578581418fe8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"691b0deea63d473a98a93cd9622c61d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5beb990d358b458abdc2a49fe49c9763","IPY_MODEL_205a97ce351c4a40bf8162d5c96ab1c2","IPY_MODEL_b4f41b6a9aee4233bd511e14809793fa"],"layout":"IPY_MODEL_9cbb70d40c514441a1c7dda82341307b"}},"5beb990d358b458abdc2a49fe49c9763":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8a7ab4ca7e5477d9d60ef05975687ec","placeholder":"​","style":"IPY_MODEL_e3e623233e0e49e2bf43ecf4c32cc93c","value":"vocab.json: 100%"}},"205a97ce351c4a40bf8162d5c96ab1c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_165eb3ae8dd14a5fa5660b62344251b2","max":1059962,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a3c42870af44bc096ed971cd8775724","value":1059962}},"b4f41b6a9aee4233bd511e14809793fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d7a945a970544418012df0a126c54a3","placeholder":"​","style":"IPY_MODEL_eb7f1c36bbb24b1abf8ec06b40e4abdb","value":" 1.06M/1.06M [00:00&lt;00:00, 4.82MB/s]"}},"9cbb70d40c514441a1c7dda82341307b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8a7ab4ca7e5477d9d60ef05975687ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3e623233e0e49e2bf43ecf4c32cc93c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"165eb3ae8dd14a5fa5660b62344251b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a3c42870af44bc096ed971cd8775724":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d7a945a970544418012df0a126c54a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb7f1c36bbb24b1abf8ec06b40e4abdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"611162d783994462b1fb75f4ca74dd8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_561de7ced93d4e7f8ba3ac5ae569fd68","IPY_MODEL_4ea711e4efca4db19bf7a98b1d503213","IPY_MODEL_8e88b0e798ae4cd0909bb36cd3b6a445"],"layout":"IPY_MODEL_1ef8f7752c234566887ef106026ad4d7"}},"561de7ced93d4e7f8ba3ac5ae569fd68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67aed2a5fdc64786ac8b24dbf9e7ae1a","placeholder":"​","style":"IPY_MODEL_6bac6f2be83a4961b120592d283e2897","value":"merges.txt: 100%"}},"4ea711e4efca4db19bf7a98b1d503213":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60d29a12f0fe403886c9d1e3fe35521d","max":524619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a137e5880be34a24b28508b53b2a4977","value":524619}},"8e88b0e798ae4cd0909bb36cd3b6a445":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_841a05e6b81147189e58af1ecfe542e4","placeholder":"​","style":"IPY_MODEL_2db28ca705b14ceda46bbdc7907ab80a","value":" 525k/525k [00:00&lt;00:00, 3.64MB/s]"}},"1ef8f7752c234566887ef106026ad4d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67aed2a5fdc64786ac8b24dbf9e7ae1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bac6f2be83a4961b120592d283e2897":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60d29a12f0fe403886c9d1e3fe35521d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a137e5880be34a24b28508b53b2a4977":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"841a05e6b81147189e58af1ecfe542e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2db28ca705b14ceda46bbdc7907ab80a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"310811ac282d4fb5a2cd57c4517ac8cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3859acaac69496a9bc48c21e250475e","IPY_MODEL_2bea0a2f24d742b79cdd13734eb1a152","IPY_MODEL_fa9810376469423c81ae792283198cca"],"layout":"IPY_MODEL_c30c19ef23f64395b00f97ca75e410c3"}},"e3859acaac69496a9bc48c21e250475e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0177e74a46d34d878042123e9efb8a61","placeholder":"​","style":"IPY_MODEL_818cea53d53c40d8bd2ab23cb4ef2ae2","value":"special_tokens_map.json: 100%"}},"2bea0a2f24d742b79cdd13734eb1a152":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fb9b80b14b54c5eb355af05273b6dea","max":472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a17e4ec8f22d4115adc239e190f279c1","value":472}},"fa9810376469423c81ae792283198cca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd36b53c664e4f59b89e6009f70d3519","placeholder":"​","style":"IPY_MODEL_d767c40dab1041cbbd81ca338cb1cc77","value":" 472/472 [00:00&lt;00:00, 29.9kB/s]"}},"c30c19ef23f64395b00f97ca75e410c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0177e74a46d34d878042123e9efb8a61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"818cea53d53c40d8bd2ab23cb4ef2ae2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fb9b80b14b54c5eb355af05273b6dea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a17e4ec8f22d4115adc239e190f279c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd36b53c664e4f59b89e6009f70d3519":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d767c40dab1041cbbd81ca338cb1cc77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8022094846a44f50b0e41ebd728c84a0":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_041d8ddb004e4bd989065e9bd191f4e9","IPY_MODEL_c1d9efd672144e01a10cfefbb07168f3"],"layout":"IPY_MODEL_07e4defe9a7f4e188381c10b279ddb7c"}},"041d8ddb004e4bd989065e9bd191f4e9":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c46bfeabc97424d91821df6c7233d08","placeholder":"​","style":"IPY_MODEL_76a27bc65c504d7689581354a1414e89","value":"0.020 MB of 0.020 MB uploaded\r"}},"c1d9efd672144e01a10cfefbb07168f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_58810df1c5f1418694bfd55a081b32e6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a5809367a304ba9a271a6aff7a33c0e","value":1}},"07e4defe9a7f4e188381c10b279ddb7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c46bfeabc97424d91821df6c7233d08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76a27bc65c504d7689581354a1414e89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58810df1c5f1418694bfd55a081b32e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a5809367a304ba9a271a6aff7a33c0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2aeaa6c6a5df4ccf830e9d06305b8a66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81755e9266064bb6b17bb5314c0827a6","IPY_MODEL_8ed8cf93f15649f68aab21684ee98d82","IPY_MODEL_6f59cf3a55624579bf9623d111d1c607"],"layout":"IPY_MODEL_2543a03701da400ca56589fbd08750a4"}},"81755e9266064bb6b17bb5314c0827a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6eea20e76a7410b8a34714e52ed3191","placeholder":"​","style":"IPY_MODEL_07327916e56c47cab7059da0d5e1f449","value":"100%"}},"8ed8cf93f15649f68aab21684ee98d82":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ca32d8cac484412bb3f743c7f29a514","max":75,"min":0,"orientation":"horizontal","style":"IPY_MODEL_860a03f0983649a9809875fb66e9c585","value":75}},"6f59cf3a55624579bf9623d111d1c607":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66a434f08824456abef1a0aec6efbf0e","placeholder":"​","style":"IPY_MODEL_9d9b4af892204fffbe49d7bc50e8179b","value":" 75/75 [29:04&lt;00:00, 22.37s/it]"}},"2543a03701da400ca56589fbd08750a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6eea20e76a7410b8a34714e52ed3191":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07327916e56c47cab7059da0d5e1f449":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ca32d8cac484412bb3f743c7f29a514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"860a03f0983649a9809875fb66e9c585":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66a434f08824456abef1a0aec6efbf0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d9b4af892204fffbe49d7bc50e8179b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0003466ea4e7484c87550337a8239879":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_0cad075fb9ba4abead00dedd8af5174f","IPY_MODEL_87b969e45ee94944bc92364263e6e098"],"layout":"IPY_MODEL_2b14993f1b8b4c0aaf19fc9bfa72cc99"}},"0cad075fb9ba4abead00dedd8af5174f":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19cfe36032e4d939ddf63f147b6ee3d","placeholder":"​","style":"IPY_MODEL_c38d02c6337045bba678bd22e001088a","value":"0.019 MB of 0.019 MB uploaded\r"}},"87b969e45ee94944bc92364263e6e098":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c47150146a244f21a7365ee71484d360","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e92583e296864e02bd42a80962535d59","value":1}},"2b14993f1b8b4c0aaf19fc9bfa72cc99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d19cfe36032e4d939ddf63f147b6ee3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c38d02c6337045bba678bd22e001088a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c47150146a244f21a7365ee71484d360":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e92583e296864e02bd42a80962535d59":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55267da5821f4647b59973294f9abfb4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d675360392c245b0b8854b7eb6873f9a","IPY_MODEL_cd03aab41657492bac03c36f01953b62","IPY_MODEL_e320ad39cd11465aad9c6d0a1f1e1d83"],"layout":"IPY_MODEL_6aede4df15294349a3749589c9f3ebe6"}},"d675360392c245b0b8854b7eb6873f9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16f34813d50b4083a9dd0be474eb8a20","placeholder":"​","style":"IPY_MODEL_8e5a669c2a6143809962f2546d11c03d","value":" 84%"}},"cd03aab41657492bac03c36f01953b62":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeb23828635041daa3ca85244be8cd3c","max":250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc04d377b60a482c9dd6288419107749","value":210}},"e320ad39cd11465aad9c6d0a1f1e1d83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_984df39d9580428a888a40fb012e4f9c","placeholder":"​","style":"IPY_MODEL_5589638401a04a42a133be98a619702c","value":" 210/250 [1:18:21&lt;16:36, 24.91s/it]"}},"6aede4df15294349a3749589c9f3ebe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16f34813d50b4083a9dd0be474eb8a20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e5a669c2a6143809962f2546d11c03d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eeb23828635041daa3ca85244be8cd3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc04d377b60a482c9dd6288419107749":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"984df39d9580428a888a40fb012e4f9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5589638401a04a42a133be98a619702c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86499d904f7c47f1bc17f859e733c185":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_d581202569ac4bf6ba4a3ea777a0d0a2","IPY_MODEL_c176f999a31a4df0b1b756e2a8aa1227"],"layout":"IPY_MODEL_5862d6f1dc7644c7b442adb82ae1d555"}},"d581202569ac4bf6ba4a3ea777a0d0a2":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48d5e1354f3c44e99e852b343ecf099a","placeholder":"​","style":"IPY_MODEL_2da0581c49784688a2516beef7bfc6c5","value":"0.019 MB of 0.019 MB uploaded\r"}},"c176f999a31a4df0b1b756e2a8aa1227":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_20514f4be69c4a8d9dbcff4f30efa44e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cac69ddf627a4252964faa9c86c92d2e","value":1}},"5862d6f1dc7644c7b442adb82ae1d555":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48d5e1354f3c44e99e852b343ecf099a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2da0581c49784688a2516beef7bfc6c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20514f4be69c4a8d9dbcff4f30efa44e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cac69ddf627a4252964faa9c86c92d2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"327a9a95704e4e69bee0b63afa5f23bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e6777c6380f4b9d96fe23245480e501","IPY_MODEL_c5a63d920e2c42db9760d776ab1e7350","IPY_MODEL_c5bb26a3232b485bb25006250fca10a7"],"layout":"IPY_MODEL_6800bebebec04e5aad57840a77a04b89"}},"8e6777c6380f4b9d96fe23245480e501":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6af034d41e64cb5abc91ae3f285d120","placeholder":"​","style":"IPY_MODEL_88fef41184384ea6812d55f5a3eb9ce2","value":" 66%"}},"c5a63d920e2c42db9760d776ab1e7350":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_90779e97ac19421a8acb9fe5e7bd1309","max":250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_889c3d80677145e9a12c742881f2c06d","value":166}},"c5bb26a3232b485bb25006250fca10a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66058fb9b35743538ff43f2ecb787b49","placeholder":"​","style":"IPY_MODEL_e741c0ec88a4483682062fd11d4d366e","value":" 166/250 [44:12&lt;23:19, 16.67s/it]"}},"6800bebebec04e5aad57840a77a04b89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6af034d41e64cb5abc91ae3f285d120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88fef41184384ea6812d55f5a3eb9ce2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90779e97ac19421a8acb9fe5e7bd1309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"889c3d80677145e9a12c742881f2c06d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66058fb9b35743538ff43f2ecb787b49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e741c0ec88a4483682062fd11d4d366e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e91969c41674638a868e42ec9e2cfe0":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_23cbd7dddd7345fba7f77b37af227ffe","IPY_MODEL_e2482a81d2104575b1b605c965782f5c"],"layout":"IPY_MODEL_7c5ac8e5fa4548f69aa2b76422cc916b"}},"23cbd7dddd7345fba7f77b37af227ffe":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02fc74850b1a478484e3ecf992652133","placeholder":"​","style":"IPY_MODEL_4866279cb64141e4815c261744a29a5b","value":"0.017 MB of 0.017 MB uploaded\r"}},"e2482a81d2104575b1b605c965782f5c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f60b315d7c3a4c509cf041f15c8a8e93","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e64c6eece2b4096a58bf80d4ea3ee74","value":1}},"7c5ac8e5fa4548f69aa2b76422cc916b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02fc74850b1a478484e3ecf992652133":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4866279cb64141e4815c261744a29a5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f60b315d7c3a4c509cf041f15c8a8e93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e64c6eece2b4096a58bf80d4ea3ee74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c10db7aa31b1484b8368b4138d879b5f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_175d8d1620a749deb5a2a6a729e93cb8","IPY_MODEL_9586ae04136d43a6be94ad6c23753492","IPY_MODEL_2d9af20799ca4ba2948289032b4ded62"],"layout":"IPY_MODEL_2646c4ba4b7249c4aaedbaf8352fac70"}},"175d8d1620a749deb5a2a6a729e93cb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77e0e3e632b442e3b8166c0ccf242bb3","placeholder":"​","style":"IPY_MODEL_c99a46cdb16947b39b17a7e7d7d6a3b2","value":"100%"}},"9586ae04136d43a6be94ad6c23753492":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30ffb8b77bfa437f8a9e25ef64a592e3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3326743d2bfd4965ac061977556ee489","value":2}},"2d9af20799ca4ba2948289032b4ded62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f35b82393e6460ab23fbde15c1e810d","placeholder":"​","style":"IPY_MODEL_976e23fca8d94c68bbe4ec57e31c0923","value":" 2/2 [06:27&lt;00:00, 197.08s/it]"}},"2646c4ba4b7249c4aaedbaf8352fac70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77e0e3e632b442e3b8166c0ccf242bb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c99a46cdb16947b39b17a7e7d7d6a3b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30ffb8b77bfa437f8a9e25ef64a592e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3326743d2bfd4965ac061977556ee489":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f35b82393e6460ab23fbde15c1e810d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"976e23fca8d94c68bbe4ec57e31c0923":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e2e40038cee4e71bd9fcc1af6c6bc43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_587b981ace3c42eda18c45a236e22083","IPY_MODEL_d751cd419ad7403b9bf888441ae815fc","IPY_MODEL_8993b4eb179b4efc9546f8e059a516fe"],"layout":"IPY_MODEL_b512a15ec9784b8f99c888062736e92e"}},"587b981ace3c42eda18c45a236e22083":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fff5b1e76fdf48938cb3dfb5c31f6f3f","placeholder":"​","style":"IPY_MODEL_33d6c9a5417141ddae2ab84116047dd1","value":"  0%"}},"d751cd419ad7403b9bf888441ae815fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_57eef240334f45bbb37d76123be39f59","max":3000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a29d655476324102aba9ce15152a0bb5","value":5}},"8993b4eb179b4efc9546f8e059a516fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_954a91f37f1047988b6c545297ba2ffb","placeholder":"​","style":"IPY_MODEL_bc28da5cac374cefaa61490810402430","value":" 5/3000 [22:14&lt;194:56:26, 234.32s/it]"}},"b512a15ec9784b8f99c888062736e92e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fff5b1e76fdf48938cb3dfb5c31f6f3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33d6c9a5417141ddae2ab84116047dd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57eef240334f45bbb37d76123be39f59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a29d655476324102aba9ce15152a0bb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"954a91f37f1047988b6c545297ba2ffb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc28da5cac374cefaa61490810402430":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}