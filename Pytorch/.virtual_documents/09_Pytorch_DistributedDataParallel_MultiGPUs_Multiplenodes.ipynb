

















import torch
import os

os.cpu_count(), torch.cuda.device_count()
#torch.cuda.current_device(), torch.cuda.mem_get_info()


#example to show the data creation logic below - for self reference
data = [(torch.rand(20), torch.rand(1)) for _ in range(10)]
len(data), torch.rand(20), torch.rand(1) #10 datasamples, each datasample has 10 features and 1 label


%load_ext autoreload
%autoreload 2


#Example code of usual pytorch training with single GPU
#we use this code as base for adding DDP to enable Distributed Data Parallel training

%%writefile single_gpy.py
import torch
from torch import nn
import torch.nn.functional as F
from torch.optim import SGD
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

import argparse


#Create Custom Dummy dataset
class MyTinyDataset(Dataset):
    def __init__(self, size):
        self.size = size
        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)] #regression dataset

    def __len__(self):
        return self.size

    def __getitem__(self, index):
        data, label = self.data[index]
        return data, label

#Trainer class
class Trainer:
    def __init__(self,
                 model: torch.nn.Module,
                 dataloader: torch.utils.data.DataLoader,
                 optimizer: torch.optim.Optimizer,
                 save_every: int,
                 gpu_id: int):
        self.gpu_id = gpu_id
        self.model = model.to(gpu_id)
        self.train_dataloader = dataloader
        self.optimizer = optimizer
        self.save_every = save_every

    def _run_batch(self, x, y):
        self.optimizer.zero_grad()
        y_pred = self.model(x)
        loss = F.cross_entropy(y_pred, y)
        loss.backward()
        self.optimizer.step()

    def _run_epoch(self, epoch):
        batch_size = len(next(iter(self.train_dataloader))[0])
        print(f"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}")
        for x, y in self.train_dataloader:
            x, y = x.to(self.gpu_id), y.to(self.gpu_id)
            self._run_batch(x, y)

    def _save_checkpoint(self, epoch):
        checkpoint_path = "checkpoint.pt"
        ckp = self.model.state_dict()
        torch.save(obj=ckp, f=checkpoint_path)
        print(f"Epoch {epoch} | Training checkpoint saved at {checkpoint_path}")

    def train(self, max_epochs: int):
        for epoch in range(max_epochs):
            self._run_epoch(epoch)
            #save checkpoint
            if epoch % self.save_every == 0 or epoch == max_epochs-1:
                self._save_checkpoint(epoch)


#Getting and processing data
def load_train_objs():
    model = torch.nn.Linear(in_features=20, out_features=1) #simple model with just linear layer
    dataset = MyTinyDataset(size=2048) # load your dataset
    optimizer = SGD(params=model.parameters(), lr=1e-3)
    return dataset, model, optimizer

def prepare_dataloader(dataset, batch_size):
    train_dataloader = DataLoader(dataset=dataset,
                                  batch_size=batch_size,
                                  shuffle=True,
                                  pin_memory=True)
    return train_dataloader


#Main method to launch everything
def main(device, total_epochs, save_every, batch_size):
    #Get the dataset, model and optimizer
    dataset, model, optimizer = load_train_objs()
    train_dataloader = prepare_dataloader(dataset, batch_size)
    trainer = Trainer(model, train_dataloader, optimizer, save_every, device)
    trainer.train(total_epochs)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="simple distributed training job")
    parser.add_argument("total_epochs", type=int, help="Total epochs to train the model")
    parser.add_argument("save_every", type=int, help="How often to save a snapshot or checkpoint")
    parser.add_argument("--batch_size" , default=32, type=int, help="Input batch size on each device (default: 32)")
    args = parser.parse_args()

    if torch.cuda.is_available():
        device = 0  #shorthand for cuda:0
    else: device = "cpu"
    main(device, args.total_epochs, args.save_every, args.batch_size)



#excute the above code as python script
!python single_gpy.py 50 10





#Migration of code with single GPU training to single machine mutli GPU training i.e Distributed Training

%%writefile multigpu.py
import torch
from torch import nn
import torch.nn.functional as F
from torch.optim import SGD
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

#-------------NEW------------------
import torch.multiprocessing as mp
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
#----------------------------------

import argparse
import os

#Create Custom Dummy dataset
class MyTinyDataset(Dataset):
    def __init__(self, size):
        self.size = size
        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)] #regression dataset

    def __len__(self):
        return self.size

    def __getitem__(self, index):
        data, label = self.data[index]
        return data, label

#-------------NEW------------------
def ddp_set_up(rank: int, world_size: int):
    """Set up environment variables and intialise the the process group
    Args:
        rank: Unique identifier of each process
        world_size: Total number of processes
    """
    #setting up environment variables
    os.environ["MASTER_ADDR"] = "localhost" #MASTER_ADDR: refers to the ipaddress of the machine thats running the rank 0 process. If our setup is single machine with multi GPU setup, rank 0 process is gonna be on same machine, so ipaddr of our machine or localhost is set for MASTER_ADDR #It is called master because, this machine coordinates the communication across all of our processes.
    os.environ["MASTER_PORT"] = "12345" #MASTER_PORT: any free port on our machine
    #init_process_group - groups all of the processes running in all our GPUs allowing the processes to communicate in the group(typically each GPU runs one process)
    init_process_group(backend="nccl", #nncl - NVIDIA collective communication library used for distributed communciations across CUDA GPUs some other collective communication libraries are gloo, infinity band, mpi
                       rank=rank, #rank: unique identifier assigned to each process, ranges from 0 to world_size-1
                       world_size=world_size) #world_size : total number of processes in a group
    torch.cuda.set_device(rank)
#-------------NEW------------------

#Trainer class
class Trainer:
    def __init__(self,
                 model: torch.nn.Module,
                 dataloader: torch.utils.data.DataLoader,
                 optimizer: torch.optim.Optimizer,
                 save_every: int,
                 gpu_id: int):
        self.gpu_id = gpu_id
        self.model = model.to(gpu_id) #needed otherwise model reside on cpu device
        #-------------NEW------------------
        self.model = DDP(module=model, device_ids=[self.gpu_id]) #wrap the model with DDP before training, device_ids is a single list that contains gpu_id that model lives on
        #-------------NEW------------------
        self.train_dataloader = dataloader
        self.optimizer = optimizer
        self.save_every = save_every


    def _run_batch(self, x, y):
        self.optimizer.zero_grad()
        y_pred = self.model(x)
        loss = F.cross_entropy(y_pred, y)
        loss.backward()
        self.optimizer.step()

    def _run_epoch(self, epoch):
        batch_size = len(next(iter(self.train_dataloader))[0])
        print(f"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}")
        #-------------NEW------------------
        self.train_dataloader.set_epoch(epoch) #itâ€™s necessary to use set_epoch to guarantee a different shuffling order:In distributed mode, calling the set_epoch() method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.
        #-------------NEW------------------
        for x, y in self.train_dataloader:
            x, y = x.to(self.gpu_id), y.to(self.gpu_id)
            self._run_batch(x, y)

    def _save_checkpoint(self, epoch):
        checkpoint_path = "checkpoint.pt"
        #-------------NEW------------------
        #ckp = self.model.state_dict() #removed
        ckp = self.model.module.state_dict() #accessing the model state_dict with model.module.state_dict()
        #-------------NEW------------------
        torch.save(obj=ckp, f=checkpoint_path)
        print(f"Epoch {epoch} | Training checkpoint saved at {checkpoint_path}")

    def train(self, max_epochs: int):
        for epoch in range(max_epochs):
            self._run_epoch(epoch)
            #save checkpoint
            #-------------NEW------------------
            #if epoch % self.save_every == 0 or epoch == max_epochs-1: #removed
            if self.gpu_id == 0 and (epoch % self.save_every == 0 or epoch == max_epochs-1): #DDP works by launching a process on each GPU and each process is gonna initialize an object of Trainer class. Earlier code saves checkpoint in all processes but as all the checkpoints will be same on all processes - redundacy. So to avoid checkpoint redundacy, save checkpoint only from rank 0 process.
            #-------------NEW------------------
                self._save_checkpoint(epoch)


#Getting and processing data
def load_train_objs():
    model = torch.nn.Linear(in_features=20, out_features=1) #simple model with just linear layer
    dataset = MyTinyDataset(size=2048) # load your dataset
    optimizer = SGD(params=model.parameters(), lr=1e-3)
    return dataset, model, optimizer

def prepare_dataloader(dataset, batch_size):
    train_dataloader = DataLoader(dataset=dataset,
                                  batch_size=batch_size,
                                  #-------------NEW------------------
                                  #shuffle=True,
                                  sampler=DistributedSampler(dataset), # Add sampler=DistributedDataSampler to DataLoader to send non overlapping samples of chunks of the input batch across all of my GPUs, turn shuffle as False as DistributedDataSampler already handles it
                                  shuffle=False,
                                  #-------------NEW------------------
                                  pin_memory=True)
    return train_dataloader

#update main() to initialize and destroy process groups, also takes rank and world size as args, replace device to rank
#-------------NEW------------------
#def main(device, total_epochs, save_every, batch_size):
def main(rank: int, world_size: int, total_epochs, save_every, batch_size):
    ddp_set_up(rank=rank, world_size=world_size)
#-------------NEW------------------
    dataset, model, optimizer = load_train_objs()
    train_dataloader = prepare_dataloader(dataset, batch_size)
    #-------------NEW------------------
    # trainer = Trainer(model, train_dataloader, optimizer, save_every, device)
    #trainer.train(total_epochs)
    trainer = Trainer(model, train_dataloader, optimizer, save_every, rank)
    trainer.train(total_epochs)
    destroy_process_group() #destrpy the process group
    #-------------NEW------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="simple distributed training job")
    parser.add_argument("total_epochs", type=int, help="Total epochs to train the model")
    parser.add_argument("save_every", type=int, help="How often to save a snapshot or checkpoint")
    parser.add_argument("--batch_size" , default=32, type=int, help="Input batch size on each device (default: 32)")
    args = parser.parse_args()

    #-------------NEW------------------
    #device = 0 if torch.cuda.is_available() else "cpu" #shorthand for cuda:0
    # main(device, args.total_epochs, args.save_every, args.batch_size)
    world_size = torch.cuda.device_count() #not device agnostic- so need to make sure to use GPU runtime, make it device agnostic to handle cases when no gpu is available
    mp.spawn(fn=main, args=(world_size, args.total_epochs, args.save_every, args.batch_size), nprocs=world_size) #use mp.spawn() from torch.multiprocessing that takes in a function(here main()) and spawns that across all of our processes in the distribute group, It takes args=(world_size, total_epochs, save_every() as args and nprocs that referes to number of processes which is nprocs=world_size. No need to send rank as parameter as mp.spawn() automatically assigns rank mp.spawn will automatically provide the rank as the first argument to the target function to get rank for each process of main
    #-------------NEW------------------


torch.cuda.device_count()


!python multigpu.py 50 10





import os
os.environ


%%writefile multigpu_torchrun.py
import torch
from torch import nn
import torch.nn.functional as F
from torch.optim import SGD
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

import argparse
import os

import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.distributed import init_process_group, destroy_process_group

#Create Custom Dummy dataset
class MyTinyDataset(Dataset):
    def __init__(self, size):
        self.size = size
        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)] #regression dataset

    def __len__(self):
        return self.size

    def __getitem__(self, index):
        data, label = self.data[index]
        return data, label

def ddp_set_up():
    init_process_group(backend="nccl")
    torch.cuda.set_device(os.environ["LOCAL_RANK"])

#Trainer class
class Trainer:
    def __init__(self,
                 model: torch.nn.Module,
                 dataloader: torch.utils.data.DataLoader,
                 optimizer: torch.optim.Optimizer,
                 save_every: int,
                 snapshot_path: str):
        self.gpu_id = int(os.environ["LOCAL_RANK"])
        self.model = model.to(self.gpu_id)
        self.train_dataloader = dataloader
        self.optimizer = optimizer
        self.save_every = save_every
        self.epochs_runs = 0 #useful for resuming
        #Snapshot check
        self.snapshot_path = snapshot_path
        if os.path.exists(self.snapshot_path):
            print(f"Snapshot found at {self.snapshot_path}... Loading snapshot")
            self._load_snapshot(self.snapshot_path)
        self.ddp_model = DDP(self.model, device_ids=[self.gpu_id])

    def _load_snapshot(self, snapshot_path):
        #Loading snapshot
        device_loc = f"cuda:{self.gpu_id}"
        snapshot = torch.load(f=snapshot_path, map_location=device_loc)
        self.model.load_state_dict(snapshot["MODEL_STATE"])
        self.epochs_runs = snapshot["EPOCHS_RUN"]
        print(f"Resuming training from snapshot at Epochs: {self.epochs_runs}")

    def _run_batch(self, x, y):
        self.optimizer.zero_grad()
        y_pred = self.model(x)
        loss = F.cross_entropy(y_pred, y)
        loss.backward()
        self.optimizer.step()

    def _run_epoch(self, epoch):
        batch_size = len(next(iter(self.train_dataloader))[0])
        print(f"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}")
        self.train_dataloader.sampler.set_epoch(epoch)
        for x, y in self.train_dataloader:
            x, y = x.to(self.gpu_id), y.to(self.gpu_id)
            self._run_batch(x, y)

    def _save_snapshot(self, epoch):
        snapshot = {"MODEL_STATE": self.model.module.state_dict(),
                    "EPOCHS_RUN": epoch}
        torch.save(obj=snapshot, f=self.snapshot_path)
        print(f"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}")

    def train(self, max_epochs: int):
        for epoch in range(self.epochs_runs, max_epochs):
            self._run_epoch(epoch)
            #save snapshot
            if self.gpu_id == 0 and (epoch % self.save_every == 0 or epoch == max_epochs-1):
                self._save_snapshot(epoch)

#Getting and processing data
def load_train_objs():
    model = torch.nn.Linear(in_features=20, out_features=1) #simple model with just linear layer
    dataset = MyTinyDataset(size=2048) # load your dataset
    optimizer = SGD(params=model.parameters(), lr=1e-3)
    return dataset, model, optimizer

def prepare_dataloader(dataset, batch_size):
    train_dataloader = DataLoader(dataset=dataset,
                                  batch_size=batch_size,
                                  shuffle=False,
                                  sampler=DistributedSampler(dataset),
                                  pin_memory=True)
    return train_dataloader


#Main method to launch everything
def main(total_epochs, save_every, batch_size, snapshot_path: str="snapshot.pt"):
    ddp_set_up()
    #Get the dataset, model and optimizer
    dataset, model, optimizer = load_train_objs()
    train_dataloader = prepare_dataloader(dataset, batch_size)
    trainer = Trainer(model, train_dataloader, optimizer, save_every, snapshot_path)
    trainer.train(total_epochs)
    destroy_process_group()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="simple distributed training job with torchrun")
    parser.add_argument("total_epochs", type=int, help="Total epochs to train the model")
    parser.add_argument("save_every", type=int, help="How often to save a snapshot or checkpoint")
    parser.add_argument("--batch_size" , default=32, type=int, help="Input batch size on each device (default: 32)")
    args = parser.parse_args()

    main(args.total_epochs, args.save_every, args.batch_size)






!torchrun --standalone --nproc_per_node 1 multigpu_torchrun.py 50 10





%%writefile multinode_torchrun.py
import torch
from torch import nn
import torch.nn.functional as F
from torch.optim import SGD
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

import argparse
import os

import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.distributed import init_process_group, destroy_process_group

#Create Custom Dummy dataset
class MyTinyDataset(Dataset):
    def __init__(self, size):
        self.size = size
        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)] #regression dataset

    def __len__(self):
        return self.size

    def __getitem__(self, index):
        data, label = self.data[index]
        return data, label

def ddp_set_up():
    init_process_group(backend="nccl")
    torch.cuda.set_device(os.environ["LOCAL_RANK"])

#Trainer class
class Trainer:
    def __init__(self,
                 model: torch.nn.Module,
                 dataloader: torch.utils.data.DataLoader,
                 optimizer: torch.optim.Optimizer,
                 save_every: int,
                 snapshot_path: str):
        self.local_rank = int(os.environ["LOCAL_RANK"])
        self.global_rank = int(os.environ["RANK"])
        self.model = model.to(self.local_rank)
        self.train_dataloader = dataloader
        self.optimizer = optimizer
        self.save_every = save_every
        self.epochs_runs = 0 #useful for resuming
        #Snapshot check
        self.snapshot_path = snapshot_path
        if os.path.exists(self.snapshot_path):
            print(f"Snapshot found at {self.snapshot_path}... Loading snapshot")
            self._load_snapshot(self.snapshot_path)
        self.ddp_model = DDP(self.model, device_ids=[self.local_rank])

    def _load_snapshot(self, snapshot_path):
        #Loading snapshot
        device_loc = f"cuda:{self.local_rank}"
        snapshot = torch.load(f=snapshot_path, map_location=device_loc)
        self.model.load_state_dict(snapshot["MODEL_STATE"])
        self.epochs_runs = snapshot["EPOCHS_RUN"]
        print(f"Resuming training from snapshot at Epochs: {self.epochs_runs}")

    def _run_batch(self, x, y):
        self.optimizer.zero_grad()
        y_pred = self.model(x)
        loss = F.cross_entropy(y_pred, y)
        loss.backward()
        self.optimizer.step()

    def _run_epoch(self, epoch):
        batch_size = len(next(iter(self.train_dataloader))[0])
        print(f"[GPU{self.local_rank}] Epoch {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}")
        self.train_dataloader.sampler.set_epoch(epoch)
        for x, y in self.train_dataloader:
            x, y = x.to(self.local_rank), y.to(self.local_rank)
            self._run_batch(x, y)

    def _save_snapshot(self, epoch):
        snapshot = {"MODEL_STATE": self.model.module.state_dict(),
                    "EPOCHS_RUN": epoch}
        torch.save(obj=snapshot, f=self.snapshot_path)
        print(f"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}")

    def train(self, max_epochs: int):
        for epoch in range(self.epochs_runs, max_epochs):
            self._run_epoch(epoch)
            #save snapshot
            if self.local_rank == 0 and (epoch % self.save_every == 0 or epoch == max_epochs-1):
                self._save_snapshot(epoch)

#Getting and processing data
def load_train_objs():
    model = torch.nn.Linear(in_features=20, out_features=1) #simple model with just linear layer
    dataset = MyTinyDataset(size=2048) # load your dataset
    optimizer = SGD(params=model.parameters(), lr=1e-3)
    return dataset, model, optimizer

def prepare_dataloader(dataset, batch_size):
    train_dataloader = DataLoader(dataset=dataset,
                                  batch_size=batch_size,
                                  shuffle=False,
                                  sampler=DistributedSampler(dataset),
                                  pin_memory=True)
    return train_dataloader


#Main method to launch everything
def main(total_epochs, save_every, batch_size, snapshot_path: str="snapshot.pt"):
    ddp_set_up()
    #Get the dataset, model and optimizer
    dataset, model, optimizer = load_train_objs()
    train_dataloader = prepare_dataloader(dataset, batch_size)
    trainer = Trainer(model, train_dataloader, optimizer, save_every, snapshot_path)
    trainer.train(total_epochs)
    destroy_process_group()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="simple distributed training job with torchrun")
    parser.add_argument("total_epochs", type=int, help="Total epochs to train the model")
    parser.add_argument("save_every", type=int, help="How often to save a snapshot or checkpoint")
    parser.add_argument("--batch_size" , default=32, type=int, help="Input batch size on each device (default: 32)")
    args = parser.parse_args()

    main(args.total_epochs, args.save_every, args.batch_size)



!torchrun --nproc_per_node 1 --nnodes 1 --node_rank 0 --rdzv_id 456 --rdzv_backend c10d --rdzv_endpoint=172.31.43.139:29603 multinode_torchrun.py 50 10









