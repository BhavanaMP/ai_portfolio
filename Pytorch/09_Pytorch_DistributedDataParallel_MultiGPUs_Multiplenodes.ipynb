{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP6k1VNiKLagj7eYOkDc+HW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Ref: https://pytorch.org/tutorials/beginner/dist_overview.html\n","Videos Ref:\n","- https://youtu.be/3XUG7cjte2U?si=KFBAQCDN8i74xLRI\n","- https://pytorch.org/tutorials/beginner/ddp_series_intro.html?utm_source=distr_landing&utm_medium=ddp_series_intro\n","\n","###**Pytorch Distributed Training**\n","Instead of training the large model with large data on a single machine/gpu/device, we gonna use distributed training paradigm to use multiple systems/devices/gpus that enables us to give large compute power and also saves us time.\n","\n","Pytorch Distributed Training has 2 Paradigms\n","- Data Parallel Training\n","    - Data is too large, and needs multiple resources to process the data in parallel. In this case, we have one model and is replicated into each resource or device. Each resource receives different copies of data that will processed in parallell across devices. The model replicas are then synchronized across different devices\n","- Model Parallel Training\n","    - Model is too large, which cannot fit in a single device or machine. In this case. the model is partitioned or sharded and the shards are then each shard is put on different device or a different servers\n","-----------------------------\n","- Data Parallel Training\n","    - DataParallel `torch.nn.DataParallel`\n","        - Single-machine multi-GPUs\n","    - Distributed Data Parallel - DDP `torch.nn.parallel.DistributedDataParallel`(recommended)\n","        - Single-machine multi-GPUs\n","    - Distributed Data Parallel + launching script - DDP `torch.nn.parallel.DistributedDataParallel`\n","        - Multi-machine GPUs\n","    -  `torch.distributed.elastic`\n","    - Fully Sharded Data Parallel (FSDP)\n","- Distributed training / Distributed Model Parallel Training\n","    - Remote Procedure Call (RPC) distributed training `torch.distributed.rpc`\n","- Hybrid Parallel Solutions\n","    - Combination of both DataParallel and ModelParallel training paradigms"],"metadata":{"id":"IyxOGve4x1bd"}},{"cell_type":"markdown","source":["###**Data Parallel Training**\n","PyTorch provides several options for data-parallel training. For applications that gradually grow from simple to complex and from prototype to production, the common development trajectory would be:\n","\n","- Use single-device training if the data and model can fit in one GPU, and training speed is not a concern.\n","\n","- Use **single-machine multi-GPU** `DataParallel` to make use of multiple GPUs on a single machine to speed up training with minimal code changes.\n","\n","- Use **single-machine multi-GPU** `DistributedDataParallel`, if you would like to further speed up training and are willing to write a little more code to set it up.\n","\n","- Use **multi-machine multiple machines on a cluster/multiple machines with multi-GPU on a cluster** `DistributedDataParallel` and the `launching script`, if the application needs to scale across machine boundaries.\n","\n","- Use `torch.distributed.elastic` to launch distributed training if errors (e.g., out-of-memory) are expected or if resources can join and leave dynamically during training."],"metadata":{"id":"ab0e8cmC1Azs"}},{"cell_type":"markdown","source":["####**DataParallel - `torch.nn.DataParallel`**\n","    \n","The `DataParallel package` enables single-machine multi-GPU parallelism with the lowest coding hurdle. It only requires a one-line change to the application code. The tutorial of Data Parallelism shows an example https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html.\n","\n","Although DataParallel is very easy to use, it usually does not offer the best performance because `it replicates the model in every forward pass`, and `its single-process multi-thread parallelism` naturally `suffers from Python's GIL Global Interpreter Lock contention`.\n","\n","To get better performance, consider using `DistributedDataParallel`."],"metadata":{"id":"zhnWdxBL1nei"}},{"cell_type":"markdown","source":["####**Distributed Data-Parallel**\n","\n","`Distributed Data-Parallel` Training (DDP) is a widely adopted `single-program multiple-data training paradigm`. With DDP, `the model is replicated on every process`, and `every model replica will be fed with a different set of input data samples` with the help of `DistributedSampler`.\n","\n","DDP takes care of gradient communication to keep model replicas synchronized and overlaps it with the gradient computations to speed up training.\n","\n","Compared to `DataParallel`, `DistributedDataParallel` requires one more step to set up, i.e., `calling init_process_group`. DDP uses `multi-process parallelism`, and `hence there is no GIL contention across model replicas`. In case of single machine, multi GPU setup, DDP launches one process per GPU where as each process has it own local copy of the model where as DP launches single process with multi threads parallelism. All the model replicas, optimizers, params are same in all the processes because of AllReduce synchronization but only thing that changes is the input data that is fed to each model replica per process. This is done with the help of  `DistributedSampler`. The sampler ensures that each process receives differents inputs from our dataloader to different processes. And this different chunks of data are concurrently processed by the different processes effectively unlike single GPU process training. This is the main idea behind DataParallel. Because of different inputs chunks to each replica, we get different params, gradients and ultimately different model for each process. To get same model in all processes,we perform synchronization during bacward pass. Note the synchorinization of gradients across all processes happens during loss.backward() step before updating the params using optimizer.step() using `bucketed ring AllReduce Synchornization algorithm` which gathers all replicas gradients and average them. Because of this all the model replcas will have same gradients and then the optimizer.step() will updates the params using these averaged gradients and hence all models will be same and doesnt result in different models in each process cuz of receiving different inputs.\n","\n","Moreover, the model is broadcast at DDP construction time instead of in every forward pass, which also helps to speed up training. DDP is shipped with several performance optimization technologies. For a more in-depth explanation, refer to this paper (http://www.vldb.org/pvldb/vol13/p3005-li.pdf).\n","\n","**DDP materials are listed below**:\n","\n","- DDP notes https://pytorch.org/docs/stable/notes/ddp.html offer a starter example and some brief descriptions of its design and implementation. If this is your first time using DDP, start from this document.\n","\n","- Getting Started with Distributed Data Parallel https://pytorch.org/tutorials/intermediate/ddp_tutorial.html explains some common problems with DDP training, including unbalanced workload, checkpointing, and multi-device models. Note that, DDP can be easily combined with single-machine multi-device model parallelism which is described in the Single-Machine Model Parallel Best Practices tutorial.https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html\n","\n","- The Launching and configuring distributed data parallel applications document https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md shows how to use the DDP launching script.\n","\n","- The Shard Optimizer States With ZeroRedundancyOptimizer https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html recipe demonstrates how ZeroRedundancyOptimizer helps to reduce optimizer memory footprint.\n","\n","- The Distributed Training with Uneven Inputs Using the Join Context Manager tutorial https://pytorch.org/tutorials/advanced/generic_join.html walks through using the generic join context for distributed training with uneven inputs."],"metadata":{"id":"IDaEeLfw0p11"}},{"cell_type":"markdown","source":["####**torch.distributed.elastic**\n","With the growth of the application complexity and scale, failure recovery becomes a requirement. Sometimes it is inevitable to hit errors like out-of-memory (OOM) when using DDP, but DDP itself cannot recover from those errors, and it is not possible to handle them using a standard try-except construct.\n","\n","This is because DDP requires all processes to operate in a closely synchronized manner and all `AllReduce` communications launched in different processes must match. If one of the processes in the group throws an exception, it is likely to lead to desynchronization (mismatched AllReduce operations) which would then cause a crash or hang.\n","\n","`torch.distributed.elastic` adds `fault tolerance` and the ability to make use of a dynamic pool of machines (elasticity)."],"metadata":{"id":"o3HqFdNC3igJ"}},{"cell_type":"markdown","source":["####**Fully Sharded Data Parallel (FSDP)**\n","Training AI models at a large scale is a challenging task that requires a lot of compute power and resources. It also comes with considerable engineering complexity to handle the training of these very large models. PyTorch FSDP, released in PyTorch 1.11 makes this easier.`\n","\n","**How FSDP works?**\n","\n","In `DistributedDataParallel, (DDP) training`, **each process/ worker owns a replica of the model and processes a batch of data, finally it uses all-reduce to sum up gradients over different workers**. In DDP the model weights and optimizer states are replicated across all workers.\n","\n","`FSDP` is a type of **data parallelism** that `shards model parameters, optimizer states and gradients across DDP ranks`.\n","\n","When training with FSDP, the GPU memory footprint is smaller than when training with DDP across all workers. This makes the training of some very large models feasible by allowing larger models or batch sizes to fit on device.\n","\n","This comes with the cost of increased communication volume. The communication overhead is reduced by internal optimizations like overlapping communication and computation.\n","\n","https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?utm_source=distr_landing&utm_medium=FSDP_getting_started\n","\n","One way to view FSDP’s sharding is to decompose the DDP gradient all-reduce into reduce-scatter and all-gather. Specifically, during the backward pass, FSDP reduces and scatters gradients, ensuring that each rank possesses a shard of the gradients. Then it updates the corresponding shard of the parameters in the optimizer step. Finally, in the subsequent forward pass, it performs an all-gather operation to collect and combine the updated parameter shards."],"metadata":{"id":"MSnlYvpk6EMZ"}},{"cell_type":"code","source":["import torch\n","import os\n","\n","os.cpu_count(), torch.cuda.device_count()\n","#torch.cuda.current_device(), torch.cuda.mem_get_info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nP1ogcKHOYVr","executionInfo":{"status":"ok","timestamp":1703602812437,"user_tz":-60,"elapsed":291,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"ca8b4e71-c0f4-463e-d5dd-ba6849c3a10e"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 0)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["#example to show the data creation logic below - for self reference\n","data = [(torch.rand(20), torch.rand(1)) for _ in range(10)]\n","len(data), torch.rand(20), torch.rand(1) #10 datasamples, each datasample has 10 features and 1 label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2PTpJcZpcyh8","executionInfo":{"status":"ok","timestamp":1703599693589,"user_tz":-60,"elapsed":261,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"32ac395b-26dc-42aa-df57-8dbf671426af"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10,\n"," tensor([0.0174, 0.5772, 0.4586, 0.6001, 0.4476, 0.4665, 0.4148, 0.4230, 0.4823,\n","         0.6494, 0.2060, 0.1267, 0.8235, 0.7276, 0.1596, 0.6896, 0.7913, 0.7325,\n","         0.9080, 0.6098]),\n"," tensor([0.7896]))"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"pOFl0WvUnW2f","executionInfo":{"status":"ok","timestamp":1703602406474,"user_tz":-60,"elapsed":247,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#Example code of usual pytorch training with single GPU\n","#we use this code as base for adding DDP to enable Distributed Data Parallel training\n","\n","%%writefile single_gpy.py\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.optim import SGD\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm.auto import tqdm\n","\n","import argparse\n","\n","\n","#Create Custom Dummy dataset\n","class MyTinyDataset(Dataset):\n","    def __init__(self, size):\n","        self.size = size\n","        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)] #regression dataset\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __getitem__(self, index):\n","        data, label = self.data[index]\n","        return data, label\n","\n","#Trainer class\n","class Trainer:\n","    def __init__(self,\n","                 model: torch.nn.Module,\n","                 dataloader: torch.utils.data.DataLoader,\n","                 optimizer: torch.optim.Optimizer,\n","                 save_every: int,\n","                 gpu_id: int):\n","        self.gpu_id = gpu_id\n","        self.model = model.to(gpu_id)\n","        self.train_dataloader = dataloader\n","        self.optimizer = optimizer\n","        self.save_every = save_every\n","\n","    def _run_batch(self, x, y):\n","        self.optimizer.zero_grad()\n","        y_pred = self.model(x)\n","        loss = F.cross_entropy(y_pred, y)\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def _run_epoch(self, epoch):\n","        batch_size = len(next(iter(self.train_dataloader))[0])\n","        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}\")\n","        for x, y in self.train_dataloader:\n","            x, y = x.to(self.gpu_id), y.to(self.gpu_id)\n","            self._run_batch(x, y)\n","\n","    def _save_checkpoint(self, epoch):\n","        checkpoint_path = \"checkpoint.pt\"\n","        ckp = self.model.state_dict()\n","        torch.save(obj=ckp, f=checkpoint_path)\n","        print(f\"Epoch {epoch} | Training checkpoint saved at {checkpoint_path}\")\n","\n","    def train(self, max_epochs: int):\n","        for epoch in range(max_epochs):\n","            self._run_epoch(epoch)\n","            #save checkpoint\n","            if epoch % self.save_every == 0 or epoch == max_epochs-1:\n","                self._save_checkpoint(epoch)\n","\n","\n","#Getting and processing data\n","def load_train_objs():\n","    model = torch.nn.Linear(in_features=20, out_features=1) #simple model with just linear layer\n","    dataset = MyTinyDataset(size=2048) # load your dataset\n","    optimizer = SGD(params=model.parameters(), lr=1e-3)\n","    return dataset, model, optimizer\n","\n","def prepare_dataloader(dataset, batch_size):\n","    train_dataloader = DataLoader(dataset=dataset,\n","                                  batch_size=batch_size,\n","                                  shuffle=True,\n","                                  pin_memory=True)\n","    return train_dataloader\n","\n","\n","#Main method to launch everything\n","def main(device, total_epochs, save_every, batch_size):\n","    #Get the dataset, model and optimizer\n","    dataset, model, optimizer = load_train_objs()\n","    train_dataloader = prepare_dataloader(dataset, batch_size)\n","    trainer = Trainer(model, train_dataloader, optimizer, save_every, device)\n","    trainer.train(total_epochs)\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser(description=\"simple distributed training job\")\n","    parser.add_argument(\"total_epochs\", type=int, help=\"Total epochs to train the model\")\n","    parser.add_argument(\"save_every\", type=int, help=\"How often to save a snapshot or checkpoint\")\n","    parser.add_argument(\"--batch_size\" , default=32, type=int, help=\"Input batch size on each device (default: 32)\")\n","    args = parser.parse_args()\n","\n","    if torch.cuda.is_available():\n","        device = 0  #shorthand for cuda:0\n","    else: device = \"cpu\"\n","    main(device, args.total_epochs, args.save_every, args.batch_size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Em_8I3MjMzRJ","executionInfo":{"status":"ok","timestamp":1703604291242,"user_tz":-60,"elapsed":7,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"410d8579-f816-43a0-f408-53770bfff72b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting single_gpy.py\n"]}]},{"cell_type":"code","source":["#excute the above code as python script\n","!python single_gpy.py 50 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3EQ61YqGm9ek","executionInfo":{"status":"ok","timestamp":1703604375404,"user_tz":-60,"elapsed":6457,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"d8d0ab22-55f0-44f0-c333-6d87c51ec871"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["[GPUcpu] Epoch 0 | Batchsize: 32 | Steps: 64\n","Epoch 0 | Training checkpoint saved at checkpoint.pt\n","[GPUcpu] Epoch 1 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 2 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 3 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 4 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 5 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 6 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 7 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 8 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 9 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 10 | Batchsize: 32 | Steps: 64\n","Epoch 10 | Training checkpoint saved at checkpoint.pt\n","[GPUcpu] Epoch 11 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 12 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 13 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 14 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 15 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 16 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 17 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 18 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 19 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 20 | Batchsize: 32 | Steps: 64\n","Epoch 20 | Training checkpoint saved at checkpoint.pt\n","[GPUcpu] Epoch 21 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 22 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 23 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 24 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 25 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 26 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 27 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 28 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 29 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 30 | Batchsize: 32 | Steps: 64\n","Epoch 30 | Training checkpoint saved at checkpoint.pt\n","[GPUcpu] Epoch 31 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 32 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 33 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 34 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 35 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 36 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 37 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 38 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 39 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 40 | Batchsize: 32 | Steps: 64\n","Epoch 40 | Training checkpoint saved at checkpoint.pt\n","[GPUcpu] Epoch 41 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 42 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 43 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 44 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 45 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 46 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 47 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 48 | Batchsize: 32 | Steps: 64\n","[GPUcpu] Epoch 49 | Batchsize: 32 | Steps: 64\n","Epoch 49 | Training checkpoint saved at checkpoint.pt\n"]}]},{"cell_type":"markdown","source":["Ref: https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-series/multigpu.py\n","\n","https://discuss.pytorch.org/t/use-distributed-data-parallel-correctly/82500/11"],"metadata":{"id":"NQHAUajXDjXZ"}},{"cell_type":"code","source":["#Migration of code with single GPU training to single machine mutli GPU training i.e Distributed Training\n","\n","%%writefile multigpu.py\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.optim import SGD\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm.auto import tqdm\n","\n","#-------------NEW------------------\n","import torch.multiprocessing as mp\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.distributed import init_process_group, destroy_process_group\n","#----------------------------------\n","\n","import argparse\n","import os\n","\n","#Create Custom Dummy dataset\n","class MyTinyDataset(Dataset):\n","    def __init__(self, size):\n","        self.size = size\n","        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)] #regression dataset\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __getitem__(self, index):\n","        data, label = self.data[index]\n","        return data, label\n","\n","#-------------NEW------------------\n","def ddp_set_up(rank: int, world_size: int):\n","    \"\"\"Set up environment variables and intialise the the process group\n","    Args:\n","        rank: Unique identifier of each process\n","        world_size: Total number of processes\n","    \"\"\"\n","    #setting up environment variables\n","    os.environ[\"MASTER_ADDR\"] = \"localhost\" #MASTER_ADDR: refers to the ipaddress of the machine thats running the rank 0 process. If our setup is single machine with multi GPU setup, rank 0 process is gonna be on same machine, so ipaddr of our machine or localhost is set for MASTER_ADDR #It is called master because, this machine coordinates the communication across all of our processes.\n","    os.environ[\"MASTER_PORT\"] = \"12345\" #MASTER_PORT: any free port on our machine\n","    #init_process_group - groups all of the processes running in all our GPUs allowing the processes to communicate in the group(typically each GPU runs one process)\n","    init_process_group(backend=\"nccl\", #nncl - NVIDIA collective communication library used for distributed communciations across CUDA GPUs some other collective communication libraries are gloo, infinity band, mpi\n","                       rank=rank, #rank: unique identifier assigned to each process, ranges from 0 to world_size-1\n","                       world_size=world_size) #world_size : total number of processes in a group\n","    torch.cuda.set_device(rank)\n","#-------------NEW------------------\n","\n","#Trainer class\n","class Trainer:\n","    def __init__(self,\n","                 model: torch.nn.Module,\n","                 dataloader: torch.utils.data.DataLoader,\n","                 optimizer: torch.optim.Optimizer,\n","                 save_every: int,\n","                 gpu_id: int):\n","        self.gpu_id = gpu_id\n","        self.model = model.to(gpu_id) #needed otherwise model reside on cpu device\n","        #-------------NEW------------------\n","        self.model = DDP(module=model, device_ids=[self.gpu_id]) #wrap the model with DDP before training, device_ids is a single list that contains gpu_id that model lives on\n","        #-------------NEW------------------\n","        self.train_dataloader = dataloader\n","        self.optimizer = optimizer\n","        self.save_every = save_every\n","\n","\n","    def _run_batch(self, x, y):\n","        self.optimizer.zero_grad()\n","        y_pred = self.model(x)\n","        loss = F.cross_entropy(y_pred, y)\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def _run_epoch(self, epoch):\n","        batch_size = len(next(iter(self.train_dataloader))[0])\n","        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}\")\n","        #-------------NEW------------------\n","        self.train_dataloader.set_epoch(epoch) #it’s necessary to use set_epoch to guarantee a different shuffling order:In distributed mode, calling the set_epoch() method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.\n","        #-------------NEW------------------\n","        for x, y in self.train_dataloader:\n","            x, y = x.to(self.gpu_id), y.to(self.gpu_id)\n","            self._run_batch(x, y)\n","\n","    def _save_checkpoint(self, epoch):\n","        checkpoint_path = \"checkpoint.pt\"\n","        #-------------NEW------------------\n","        #ckp = self.model.state_dict() #removed\n","        ckp = self.model.module.state_dict() #accessing the model state_dict with model.module.state_dict()\n","        #-------------NEW------------------\n","        torch.save(obj=ckp, f=checkpoint_path)\n","        print(f\"Epoch {epoch} | Training checkpoint saved at {checkpoint_path}\")\n","\n","    def train(self, max_epochs: int):\n","        for epoch in range(max_epochs):\n","            self._run_epoch(epoch)\n","            #save checkpoint\n","            #-------------NEW------------------\n","            #if epoch % self.save_every == 0 or epoch == max_epochs-1: #removed\n","            if self.gpu_id == 0 and (epoch % self.save_every == 0 or epoch == max_epochs-1): #DDP works by launching a process on each GPU and each process is gonna initialize an object of Trainer class. Earlier code saves checkpoint in all processes but as all the checkpoints will be same on all processes - redundacy. So to avoid checkpoint redundacy, save checkpoint only from rank 0 process.\n","            #-------------NEW------------------\n","                self._save_checkpoint(epoch)\n","\n","\n","#Getting and processing data\n","def load_train_objs():\n","    model = torch.nn.Linear(in_features=20, out_features=1) #simple model with just linear layer\n","    dataset = MyTinyDataset(size=2048) # load your dataset\n","    optimizer = SGD(params=model.parameters(), lr=1e-3)\n","    return dataset, model, optimizer\n","\n","def prepare_dataloader(dataset, batch_size):\n","    train_dataloader = DataLoader(dataset=dataset,\n","                                  batch_size=batch_size,\n","                                  #-------------NEW------------------\n","                                  #shuffle=True,\n","                                  sampler=DistributedSampler(dataset), # Add sampler=DistributedDataSampler to DataLoader to send non overlapping samples of chunks of the input batch across all of my GPUs, turn shuffle as False as DistributedDataSampler already handles it\n","                                  shuffle=False,\n","                                  #-------------NEW------------------\n","                                  pin_memory=True)\n","    return train_dataloader\n","\n","#update main() to initialize and destroy process groups, also takes rank and world size as args, replace device to rank\n","#-------------NEW------------------\n","#def main(device, total_epochs, save_every, batch_size):\n","def main(rank: int, world_size: int, total_epochs, save_every, batch_size):\n","    ddp_set_up(rank=rank, world_size=world_size)\n","#-------------NEW------------------\n","    dataset, model, optimizer = load_train_objs()\n","    train_dataloader = prepare_dataloader(dataset, batch_size)\n","    #-------------NEW------------------\n","    # trainer = Trainer(model, train_dataloader, optimizer, save_every, device)\n","    #trainer.train(total_epochs)\n","    trainer = Trainer(model, train_dataloader, optimizer, save_every, rank)\n","    trainer.train(total_epochs)\n","    destroy_process_group() #destrpy the process group\n","    #-------------NEW------------------\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser(description=\"simple distributed training job\")\n","    parser.add_argument(\"total_epochs\", type=int, help=\"Total epochs to train the model\")\n","    parser.add_argument(\"save_every\", type=int, help=\"How often to save a snapshot or checkpoint\")\n","    parser.add_argument(\"--batch_size\" , default=32, type=int, help=\"Input batch size on each device (default: 32)\")\n","    args = parser.parse_args()\n","\n","    #-------------NEW------------------\n","    #device = 0 if torch.cuda.is_available() else \"cpu\" #shorthand for cuda:0\n","    # main(device, args.total_epochs, args.save_every, args.batch_size)\n","    world_size = torch.cuda.device_count() #not device agnostic- so need to make sure to use GPU runtime, make it device agnostic to handle cases when no gpu is available\n","    mp.spawn(fn=main, args=(world_size, args.total_epochs, args.save_every, args.batch_size), nprocs=world_size) #use mp.spawn() from torch.multiprocessing that takes in a function(here main()) and spawns that across all of our processes in the distribute group, It takes args=(world_size, total_epochs, save_every() as args and nprocs that referes to number of processes which is nprocs=world_size. No need to send rank as parameter as mp.spawn() automatically assigns rank mp.spawn will automatically provide the rank as the first argument to the target function to get rank for each process of main\n","    #-------------NEW------------------"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iW9QT1gasgt3","executionInfo":{"status":"ok","timestamp":1703616103669,"user_tz":-60,"elapsed":266,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"2310b8a8-5324-4e62-dc1d-37fc9e2f6b10"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting multigpu.py\n"]}]},{"cell_type":"code","source":["torch.cuda.device_count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQCD3SVhHbe1","executionInfo":{"status":"ok","timestamp":1703610816188,"user_tz":-60,"elapsed":9,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"a1f7ec65-e71f-452f-80e0-38e296fe6cbf"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["!python multigpu.py 50 10"],"metadata":{"id":"yD90L5BZDJk6","executionInfo":{"status":"ok","timestamp":1703616111684,"user_tz":-60,"elapsed":3476,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#### Fault tolerance in distributed multi GPU training using `torchrun`\n","\n","In distributed training, a single process failure can disrupt the entire training job. Since the susceptibility for failure can be higher here, making your training script robust is particularly important here. You might also prefer your training job to be elastic, for example, compute resources can join and leave dynamically over the course of the job.\n","\n","PyTorch offers a utility called torchrun that provides fault-tolerance and elastic training. When a failure occurs, torchrun logs the errors and attempts to automatically restart all the processes from the last saved “snapshot” of the training job.\n","\n","The snapshot saves more than just the model state; it can include details about the number of epochs run, optimizer states or any other stateful attribute of the training job necessary for its continuity.\n","\n","**Why use torchrun?**\n","\n","torchrun handles the minutiae of distributed training so that you don’t need to. For instance,\n","\n","You don’t need to set environment variables or explicitly pass the rank and world_size; torchrun assigns this along with several other environment variables.\n","\n","No need to call mp.spawn in your script; you only need a generic main() entry point, and launch the script with torchrun. This way the same script can be run in non-distributed as well as single-node and multinode setups.\n","\n","Gracefully restarting training from the last saved training snapshot.\n","\n","###**Graceful restarts**\n","For graceful restarts, you should structure your train script like:\n","\n","```\n","def main():\n","  load_snapshot(snapshot_path)\n","  initialize()\n","  train()\n","\n","def train():\n","  for batch in iter(dataset):\n","    train_step(batch)\n","\n","    if should_checkpoint:\n","      save_snapshot(snapshot_path)\n","\n","```\n","\n","If a failure occurs, torchrun will terminate all the processes and restart them. Each process entry point first loads and initializes the last saved snapshot, and continues training from there. So at any failure, you only lose the training progress from the last saved snapshot.\n","\n","In elastic training, whenever there are any membership changes (adding or removing nodes), torchrun will terminate and spawn processes on available devices. Having this structure ensures your training job can continue without manual intervention.\n","\n","#### **Diff for multigpu.py v/s multigpu_torchrun.py**\n","**Process group initialization**\n","torchrun assigns RANK and WORLD_SIZE automatically, among other envvariables"],"metadata":{"id":"T7OnvAN9IxsT"}},{"cell_type":"code","source":["import os\n","os.environ"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ZyuY_QjTv-M","executionInfo":{"status":"ok","timestamp":1703614036397,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"f8ae6129-c78f-474a-951d-694f42db0819"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["environ{'SHELL': '/bin/bash',\n","        'NV_LIBCUBLAS_VERSION': '12.2.5.6-1',\n","        'NVIDIA_VISIBLE_DEVICES': 'all',\n","        'COLAB_JUPYTER_TRANSPORT': 'ipc',\n","        'NV_NVML_DEV_VERSION': '12.2.140-1',\n","        'NV_CUDNN_PACKAGE_NAME': 'libcudnn8',\n","        'CGROUP_MEMORY_EVENTS': '/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events',\n","        'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.19.3-1+cuda12.2',\n","        'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.19.3-1',\n","        'VM_GCE_METADATA_HOST': '169.254.169.253',\n","        'HOSTNAME': 'e83b5f9a121f',\n","        'LANGUAGE': 'en_US',\n","        'TBE_RUNTIME_ADDR': '172.28.0.1:8011',\n","        'GCE_METADATA_TIMEOUT': '3',\n","        'NVIDIA_REQUIRE_CUDA': 'cuda>=12.2 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526',\n","        'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-12-2=12.2.5.6-1',\n","        'NV_NVTX_VERSION': '12.2.140-1',\n","        'COLAB_JUPYTER_IP': '172.28.0.12',\n","        'NV_CUDA_CUDART_DEV_VERSION': '12.2.140-1',\n","        'NV_LIBCUSPARSE_VERSION': '12.1.2.141-1',\n","        'COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL': 'http://172.28.0.1:8013/',\n","        'NV_LIBNPP_VERSION': '12.2.1.4-1',\n","        'NCCL_VERSION': '2.19.3-1',\n","        'KMP_LISTEN_PORT': '6000',\n","        'TF_FORCE_GPU_ALLOW_GROWTH': 'true',\n","        'ENV': '/root/.bashrc',\n","        'PWD': '/',\n","        'COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT': '30s',\n","        'TBE_EPHEM_CREDS_ADDR': '172.28.0.1:8009',\n","        'TBE_CREDS_ADDR': '172.28.0.1:8008',\n","        'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.6.50-1+cuda12.2',\n","        'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility',\n","        'COLAB_JUPYTER_TOKEN': '',\n","        'LAST_FORCED_REBUILD': '20231205',\n","        'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-12-2=12.2.142-1',\n","        'NV_LIBNPP_PACKAGE': 'libnpp-12-2=12.2.1.4-1',\n","        'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev',\n","        'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20',\n","        'NV_LIBCUBLAS_DEV_VERSION': '12.2.5.6-1',\n","        'NVIDIA_PRODUCT_NAME': 'CUDA',\n","        'COLAB_KERNEL_MANAGER_PROXY_HOST': '172.28.0.12',\n","        'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-12-2',\n","        'NV_CUDA_CUDART_VERSION': '12.2.140-1',\n","        'COLAB_WARMUP_DEFAULTS': '1',\n","        'HOME': '/root',\n","        'LANG': 'en_US.UTF-8',\n","        'COLUMNS': '100',\n","        'CUDA_VERSION': '12.2.2',\n","        'CLOUDSDK_CONFIG': '/content/.config',\n","        'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-2=12.2.5.6-1',\n","        'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-12-2=12.2.2-1',\n","        'COLAB_RELEASE_TAG': 'release-colab_20231219-060136_RC00',\n","        'KMP_TARGET_PORT': '9000',\n","        'KMP_EXTRA_ARGS': '--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/m-s-2hs183tot2vho --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true',\n","        'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-12-2=12.2.1.4-1',\n","        'COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS': '/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages',\n","        'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-2',\n","        'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000',\n","        'CLOUDSDK_PYTHON': 'python3',\n","        'NV_LIBNPP_DEV_VERSION': '12.2.1.4-1',\n","        'NO_GCE_CHECK': 'False',\n","        'PYTHONPATH': '/env/python',\n","        'NV_LIBCUSPARSE_DEV_VERSION': '12.1.2.141-1',\n","        'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\n","        'NV_CUDNN_VERSION': '8.9.6.50',\n","        'SHLVL': '0',\n","        'NV_CUDA_LIB_VERSION': '12.2.2-1',\n","        'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service',\n","        'NVARCH': 'x86_64',\n","        'NV_CUDNN_PACKAGE_DEV': 'libcudnn8-dev=8.9.6.50-1+cuda12.2',\n","        'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-2',\n","        'NV_LIBNCCL_PACKAGE': 'libnccl2=2.19.3-1+cuda12.2',\n","        'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\n","        'COLAB_GPU': '',\n","        'NV_CUDA_NSIGHT_COMPUTE_VERSION': '12.2.2-1',\n","        'GCS_READ_CACHE_BLOCK_SIZE_MB': '16',\n","        'NV_NVPROF_VERSION': '12.2.142-1',\n","        'LC_ALL': 'en_US.UTF-8',\n","        'COLAB_FILE_HANDLER_ADDR': 'localhost:3453',\n","        'PATH': '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin',\n","        'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2',\n","        'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer',\n","        'NV_LIBNCCL_PACKAGE_VERSION': '2.19.3-1',\n","        'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command',\n","        'DEBIAN_FRONTEND': 'noninteractive',\n","        'COLAB_BACKEND_VERSION': 'next',\n","        'OLDPWD': '/',\n","        'JPY_PARENT_PID': '84',\n","        'TERM': 'xterm-color',\n","        'CLICOLOR': '1',\n","        'PAGER': 'cat',\n","        'GIT_PAGER': 'cat',\n","        'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n","        'ENABLE_DIRECTORYPREFETCHER': '1',\n","        'USE_AUTH_EPHEM': '1',\n","        'PYDEVD_USE_FRAME_EVAL': 'NO'}"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["%%writefile multigpu_torchrun.py\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.optim import SGD\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm.auto import tqdm\n","\n","import argparse\n","import os\n","\n","import torch.multiprocessing as mp\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","#Create Custom Dummy dataset\n","class MyTinyDataset(Dataset):\n","    def __init__(self, size):\n","        self.size = size\n","        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)] #regression dataset\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __getitem__(self, index):\n","        data, label = self.data[index]\n","        return data, label\n","\n","def ddp_set_up():\n","    init_process_group(backend=\"nccl\")\n","    torch.cuda.set_device(os.environ[\"LOCAL_RANK\"])\n","\n","#Trainer class\n","class Trainer:\n","    def __init__(self,\n","                 model: torch.nn.Module,\n","                 dataloader: torch.utils.data.DataLoader,\n","                 optimizer: torch.optim.Optimizer,\n","                 save_every: int,\n","                 snapshot_path: str):\n","        self.gpu_id = int(os.environ[\"LOCAL_RANK\"])\n","        self.model = model.to(self.gpu_id)\n","        self.train_dataloader = dataloader\n","        self.optimizer = optimizer\n","        self.save_every = save_every\n","        self.epochs_runs = 0 #useful for resuming\n","        #Snapshot check\n","        self.snapshot_path = snapshot_path\n","        if os.path.exists(self.snapshot_path):\n","            print(f\"Snapshot found at {self.snapshot_path}... Loading snapshot\")\n","            self._load_snapshot(self.snapshot_path)\n","        self.ddp_model = DDP(self.model, device_ids=[self.gpu_id])\n","\n","    def _load_snapshot(self, snapshot_path):\n","        #Loading snapshot\n","        device_loc = f\"cuda:{self.gpu_id}\"\n","        snapshot = torch.load(f=snapshot_path, map_location=device_loc)\n","        self.model.load_state_dict(snapshot[\"MODEL_STATE\"])\n","        self.epochs_runs = snapshot[\"EPOCHS_RUN\"]\n","        print(f\"Resuming training from snapshot at Epochs: {self.epochs_runs}\")\n","\n","    def _run_batch(self, x, y):\n","        self.optimizer.zero_grad()\n","        y_pred = self.model(x)\n","        loss = F.cross_entropy(y_pred, y)\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def _run_epoch(self, epoch):\n","        batch_size = len(next(iter(self.train_dataloader))[0])\n","        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}\")\n","        self.train_dataloader.sampler.set_epoch(epoch)\n","        for x, y in self.train_dataloader:\n","            x, y = x.to(self.gpu_id), y.to(self.gpu_id)\n","            self._run_batch(x, y)\n","\n","    def _save_snapshot(self, epoch):\n","        snapshot = {\"MODEL_STATE\": self.model.module.state_dict(),\n","                    \"EPOCHS_RUN\": epoch}\n","        torch.save(obj=snapshot, f=self.snapshot_path)\n","        print(f\"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}\")\n","\n","    def train(self, max_epochs: int):\n","        for epoch in range(self.epochs_runs, max_epochs):\n","            self._run_epoch(epoch)\n","            #save snapshot\n","            if self.gpu_id == 0 and (epoch % self.save_every == 0 or epoch == max_epochs-1):\n","                self._save_snapshot(epoch)\n","\n","#Getting and processing data\n","def load_train_objs():\n","    model = torch.nn.Linear(in_features=20, out_features=1) #simple model with just linear layer\n","    dataset = MyTinyDataset(size=2048) # load your dataset\n","    optimizer = SGD(params=model.parameters(), lr=1e-3)\n","    return dataset, model, optimizer\n","\n","def prepare_dataloader(dataset, batch_size):\n","    train_dataloader = DataLoader(dataset=dataset,\n","                                  batch_size=batch_size,\n","                                  shuffle=False,\n","                                  sampler=DistributedSampler(dataset),\n","                                  pin_memory=True)\n","    return train_dataloader\n","\n","\n","#Main method to launch everything\n","def main(total_epochs, save_every, batch_size, snapshot_path: str=\"snapshot.pt\"):\n","    ddp_set_up()\n","    #Get the dataset, model and optimizer\n","    dataset, model, optimizer = load_train_objs()\n","    train_dataloader = prepare_dataloader(dataset, batch_size)\n","    trainer = Trainer(model, train_dataloader, optimizer, save_every, snapshot_path)\n","    trainer.train(total_epochs)\n","    destroy_process_group()\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser(description=\"simple distributed training job with torchrun\")\n","    parser.add_argument(\"total_epochs\", type=int, help=\"Total epochs to train the model\")\n","    parser.add_argument(\"save_every\", type=int, help=\"How often to save a snapshot or checkpoint\")\n","    parser.add_argument(\"--batch_size\" , default=32, type=int, help=\"Input batch size on each device (default: 32)\")\n","    args = parser.parse_args()\n","\n","    main(args.total_epochs, args.save_every, args.batch_size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WC5KYUl0IxLA","executionInfo":{"status":"ok","timestamp":1703616544664,"user_tz":-60,"elapsed":241,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"4f01cc79-6146-4723-86ca-40e0139b8c5a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting multigpu_torchrun.py\n"]}]},{"cell_type":"markdown","source":["`--standalone` represents that this is a single machine/single node set up\n","\n","`--nproc_per_node` represents how many GPUs/processes per machine/node to use. This is gonna be our local world_size. As I have single GPU in this machine, I set it to 1 using `--nproc_per_node 1`. We can also set it as `--nproc_per_node gpu` which lets torchrun know to use all available GPU's"],"metadata":{"id":"GF8zvS8VciK9"}},{"cell_type":"code","source":["!torchrun --standalone --nproc_per_node 1 multigpu_torchrun.py 50 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtUTCUy4cWFl","executionInfo":{"status":"ok","timestamp":1703616556003,"user_tz":-60,"elapsed":9166,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"78a6819f-4f76-4320-a2a7-f8ea6885a6fc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[2023-12-26 18:49:09,650] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n","Traceback (most recent call last):\n","  File \"/content/multigpu_torchrun.py\", line 123, in <module>\n","    main(args.total_epochs, args.save_every, args.batch_size)\n","  File \"/content/multigpu_torchrun.py\", line 108, in main\n","    ddp_set_up()\n","  File \"/content/multigpu_torchrun.py\", line 30, in ddp_set_up\n","    init_process_group(backend=\"nccl\")\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 74, in wrapper\n","    func_return = func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 1148, in init_process_group\n","    default_pg, _ = _new_process_group_helper(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 1279, in _new_process_group_helper\n","    backend_class = ProcessGroupNCCL(backend_prefix_store, group_rank, group_size, pg_options)\n","RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!\n","[2023-12-26 18:49:14,895] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 11963) of binary: /usr/bin/python3\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 806, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n","    raise ChildFailedError(\n","torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n","============================================================\n","multigpu_torchrun.py FAILED\n","------------------------------------------------------------\n","Failures:\n","  <NO_OTHER_FAILURES>\n","------------------------------------------------------------\n","Root Cause (first observed failure):\n","[0]:\n","  time      : 2023-12-26_18:49:14\n","  host      : e83b5f9a121f\n","  rank      : 0 (local_rank: 0)\n","  exitcode  : 1 (pid: 11963)\n","  error_file: <N/A>\n","  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n","============================================================\n"]}]},{"cell_type":"markdown","source":["Ref: https://pytorch.org/tutorials/intermediate/ddp_series_multinode.html"],"metadata":{"id":"JBigg0o8itvr"}},{"cell_type":"code","source":["%%writefile multinode_torchrun.py\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.optim import SGD\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm.auto import tqdm\n","\n","import argparse\n","import os\n","\n","import torch.multiprocessing as mp\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","#Create Custom Dummy dataset\n","class MyTinyDataset(Dataset):\n","    def __init__(self, size):\n","        self.size = size\n","        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)] #regression dataset\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __getitem__(self, index):\n","        data, label = self.data[index]\n","        return data, label\n","\n","def ddp_set_up():\n","    init_process_group(backend=\"nccl\")\n","    torch.cuda.set_device(os.environ[\"LOCAL_RANK\"])\n","\n","#Trainer class\n","class Trainer:\n","    def __init__(self,\n","                 model: torch.nn.Module,\n","                 dataloader: torch.utils.data.DataLoader,\n","                 optimizer: torch.optim.Optimizer,\n","                 save_every: int,\n","                 snapshot_path: str):\n","        self.local_rank = int(os.environ[\"LOCAL_RANK\"])\n","        self.global_rank = int(os.environ[\"RANK\"])\n","        self.model = model.to(self.local_rank)\n","        self.train_dataloader = dataloader\n","        self.optimizer = optimizer\n","        self.save_every = save_every\n","        self.epochs_runs = 0 #useful for resuming\n","        #Snapshot check\n","        self.snapshot_path = snapshot_path\n","        if os.path.exists(self.snapshot_path):\n","            print(f\"Snapshot found at {self.snapshot_path}... Loading snapshot\")\n","            self._load_snapshot(self.snapshot_path)\n","        self.ddp_model = DDP(self.model, device_ids=[self.local_rank])\n","\n","    def _load_snapshot(self, snapshot_path):\n","        #Loading snapshot\n","        device_loc = f\"cuda:{self.local_rank}\"\n","        snapshot = torch.load(f=snapshot_path, map_location=device_loc)\n","        self.model.load_state_dict(snapshot[\"MODEL_STATE\"])\n","        self.epochs_runs = snapshot[\"EPOCHS_RUN\"]\n","        print(f\"Resuming training from snapshot at Epochs: {self.epochs_runs}\")\n","\n","    def _run_batch(self, x, y):\n","        self.optimizer.zero_grad()\n","        y_pred = self.model(x)\n","        loss = F.cross_entropy(y_pred, y)\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def _run_epoch(self, epoch):\n","        batch_size = len(next(iter(self.train_dataloader))[0])\n","        print(f\"[GPU{self.local_rank}] Epoch {epoch} | Batchsize: {batch_size} | Steps: {len(self.train_dataloader)}\")\n","        self.train_dataloader.sampler.set_epoch(epoch)\n","        for x, y in self.train_dataloader:\n","            x, y = x.to(self.local_rank), y.to(self.local_rank)\n","            self._run_batch(x, y)\n","\n","    def _save_snapshot(self, epoch):\n","        snapshot = {\"MODEL_STATE\": self.model.module.state_dict(),\n","                    \"EPOCHS_RUN\": epoch}\n","        torch.save(obj=snapshot, f=self.snapshot_path)\n","        print(f\"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}\")\n","\n","    def train(self, max_epochs: int):\n","        for epoch in range(self.epochs_runs, max_epochs):\n","            self._run_epoch(epoch)\n","            #save snapshot\n","            if self.local_rank == 0 and (epoch % self.save_every == 0 or epoch == max_epochs-1):\n","                self._save_snapshot(epoch)\n","\n","#Getting and processing data\n","def load_train_objs():\n","    model = torch.nn.Linear(in_features=20, out_features=1) #simple model with just linear layer\n","    dataset = MyTinyDataset(size=2048) # load your dataset\n","    optimizer = SGD(params=model.parameters(), lr=1e-3)\n","    return dataset, model, optimizer\n","\n","def prepare_dataloader(dataset, batch_size):\n","    train_dataloader = DataLoader(dataset=dataset,\n","                                  batch_size=batch_size,\n","                                  shuffle=False,\n","                                  sampler=DistributedSampler(dataset),\n","                                  pin_memory=True)\n","    return train_dataloader\n","\n","\n","#Main method to launch everything\n","def main(total_epochs, save_every, batch_size, snapshot_path: str=\"snapshot.pt\"):\n","    ddp_set_up()\n","    #Get the dataset, model and optimizer\n","    dataset, model, optimizer = load_train_objs()\n","    train_dataloader = prepare_dataloader(dataset, batch_size)\n","    trainer = Trainer(model, train_dataloader, optimizer, save_every, snapshot_path)\n","    trainer.train(total_epochs)\n","    destroy_process_group()\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser(description=\"simple distributed training job with torchrun\")\n","    parser.add_argument(\"total_epochs\", type=int, help=\"Total epochs to train the model\")\n","    parser.add_argument(\"save_every\", type=int, help=\"How often to save a snapshot or checkpoint\")\n","    parser.add_argument(\"--batch_size\" , default=32, type=int, help=\"Input batch size on each device (default: 32)\")\n","    args = parser.parse_args()\n","\n","    main(args.total_epochs, args.save_every, args.batch_size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-BtK033gyWj","executionInfo":{"status":"ok","timestamp":1703618174898,"user_tz":-60,"elapsed":293,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"e384d7c2-5ffc-4412-e32a-408948751223"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting multinode_torchrun.py\n"]}]},{"cell_type":"code","source":["!torchrun --nproc_per_node 1 --nnodes 1 --node_rank 0 --rdzv_id 456 --rdzv_backend c10d --rdzv_endpoint=172.31.43.139:29603 multinode_torchrun.py 50 10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOwi_WAdhlgD","executionInfo":{"status":"ok","timestamp":1703618241920,"user_tz":-60,"elapsed":63233,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"57880756-5bb0-419c-e1d6-0ad73a28059c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[2023-12-26 19:16:20,640] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n","[E socket.cpp:922] [c10d] The client socket has timed out after 60s while trying to connect to (172.31.43.139, 29603).\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py\", line 155, in _create_tcp_store\n","    store = TCPStore(\n","TimeoutError: The client socket has timed out after 60s while trying to connect to (172.31.43.139, 29603).\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/torchrun\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 806, in main\n","    run(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in run\n","    elastic_launch(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n","    return launch_agent(self._config, self._entrypoint, list(args))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 237, in launch_agent\n","    rdzv_handler=rdzv_registry.get_rendezvous_handler(rdzv_parameters),\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/rendezvous/registry.py\", line 65, in get_rendezvous_handler\n","    return handler_registry.create_handler(params)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/rendezvous/api.py\", line 258, in create_handler\n","    handler = creator(params)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/rendezvous/registry.py\", line 36, in _create_c10d_handler\n","    backend, store = create_backend(params)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py\", line 250, in create_backend\n","    store = _create_tcp_store(params)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py\", line 175, in _create_tcp_store\n","    raise RendezvousConnectionError(\n","torch.distributed.elastic.rendezvous.api.RendezvousConnectionError: The connection to the C10d store has failed. See inner exception for details.\n"]}]},{"cell_type":"markdown","source":["###**Distributed Training / Distributed Model Parallel Training**"],"metadata":{"id":"ev1H8dPj5fUi"}},{"cell_type":"markdown","source":["####**RPC-Based Distributed Training**\n","Many training paradigms do not fit into data parallelism.\n","\n","e.g., parameter server paradigm, distributed pipeline parallelism, reinforcement learning applications with multiple observers or agents, etc.\n","\n","`torch.distributed.rpc` aims at supporting general distributed training scenarios.\n","\n","`torch.distributed.rpc` has 4 main pillars:\n","\n","- `RPC` supports remote execution i.e running a given arbitrary user function or modules remotely i.e on a remote worker. Thats the main  motive of any RPC system.\n","\n","- `RRef` stands for remote reference. It allows us to efficiently access and reference remote data objects.It basically  helps to manage the lifetime of a remote object. The reference counting protocol is presented in the RRef notes.https://pytorch.org/docs/stable/rpc/rref.html#remote-reference-protocol\n","\n","- `Distributed Autograd` extends the Pytorch autograd engine beyond machine boundaries. Originally Pytorch's Aautograd is a per process/worker/device process. Distributed Autograd extends it beyond machine/process boundaries. Please refer to Distributed Autograd Design for more details.https://pytorch.org/docs/stable/rpc/distributed_autograd.html#distributed-autograd-design\n","\n","- `Distributed Optimizer` automatically reaches out to all participating workers to update parameters using gradients computed by the distributed autograd engine.\n","\n","**RPC Tutorials are listed below:**\n","\n","- The Getting Started with Distributed RPC Framework tutorial first uses a simple Reinforcement Learning (RL) example to demonstrate RPC and RRef. Then, it applies a basic distributed model parallelism to an RNN example to show how to use distributed autograd and distributed optimizer. https://pytorch.org/tutorials/intermediate/rpc_tutorial.html\n","\n","- The Implementing a `Parameter Server Using Distributed RPC Framework` tutorial borrows the spirit of HogWild! training https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf and applies it to an asynchronous parameter server (PS) training application. https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html\n","\n","- The `Distributed Pipeline Parallelism Using RPC` tutorial https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html extends the single-machine pipeline parallel example (presented in Single-Machine Model Parallel Best Practices https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html) to a distributed environment and shows how to implement it using RPC.\n","\n","- The Implementing `Batch RPC Processing Using Asynchronous Executions` tutorial https://pytorch.org/tutorials/intermediate/rpc_async_execution.html demonstrates how to implement RPC batch processing using the `@rpc.functions.async_execution` decorator, which can help speed up inference and training. It uses RL and PS examples similar to those in the above tutorials 1 and 2.\n","\n","- The Combining Distributed DataParallel with Distributed RPC Framework tutorial https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.htmldemonstrates how to combine DDP with RPC to train a model using distributed data parallelism combined with distributed model parallelism."],"metadata":{"id":"Yzk-5lGr3_yD"}},{"cell_type":"code","source":[],"metadata":{"id":"0OSOuUin00c1"},"execution_count":null,"outputs":[]}]}