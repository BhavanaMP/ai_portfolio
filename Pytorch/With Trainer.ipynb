{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxfAitq+aouxxAbfZ0w5vO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PJS2oGfzMrK2"},"outputs":[],"source":["!pip install transformers datasets evaluate albumentations torchinfo torchmetrics accelerate timm wandb peft"]},{"cell_type":"code","source":["#natten is needed for dinat checkpoints\n","!pip3 install natten -f https://shi-labs.com/natten/wheels/cu113/torch1.10.1/index.html --quiet"],"metadata":{"id":"XVSNf1sNAndy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets, transformers, evaluate, accelerate, natten\n","from datasets import load_dataset\n","from transformers import AutoProcessor, AutoModelForUniversalSegmentation, OneFormerForUniversalSegmentation\n","from transformers import Trainer, TrainingArguments\n","from huggingface_hub import hf_hub_download\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import albumentations as A\n","from torch.optim import AdamW\n","from tqdm.auto import tqdm\n","import torchinfo, torchmetrics\n","\n","from pathlib import Path\n","import requests\n","import zipfile\n","import json\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import wandb\n","from huggingface_hub import notebook_login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95c9dcad-f8b3-486f-e15a-3890a1e878e7","id":"RKA4vTXsa59m"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"id":"r3ZECUYmA-El"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["notebook_login()"],"metadata":{"id":"HFNe-ZfkA-2U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Loading dataset\n","railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-split355\")\n","\n","#Label creation\n","config_file = hf_hub_download(repo_id=\"BhavanaMalla/railsem19-semantic-split355\",\n","                filename=\"rs19-config.json\",\n","                repo_type=\"dataset\",\n","                local_dir=\"/content\")\n","\n","with open(config_file, \"r\") as f:\n","    config_json = json.load(f)\n","\n","json_labels = config_json[\"labels\"]\n","\n","extract_labels = []\n","readable_labels = []\n","color_palette = []\n","for label in json_labels:\n","    extract_labels.append(label[\"name\"])\n","    color_palette.append(label[\"color\"])\n","    readable_labels.append(label[\"readable\"])\n","\n","id2label = {idx: label for idx, label in enumerate(extract_labels)}\n","label2id = {label: idx for idx, label in enumerate(extract_labels)}\n","labels = extract_labels"],"metadata":{"id":"KtOQ8FT1Af7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["railsem_ds[\"train\"][0].keys(), railsem_ds[\"train\"].features, railsem_ds[\"train\"][0][\"image\"].mode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJ8oin4CbF20","outputId":"7a201014-9d5d-4642-e6e6-0882c6fc5f5d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(dict_keys(['image', 'semantic_mask_label', 'json']),\n"," {'image': Image(decode=True, id=None),\n","  'semantic_mask_label': Image(decode=True, id=None),\n","  'json': {'frame': Value(dtype='string', id=None),\n","   'imgHeight': Value(dtype='int64', id=None),\n","   'imgWidth': Value(dtype='int64', id=None),\n","   'objects': [{'boundingbox': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n","     'label': Value(dtype='string', id=None),\n","     'polygon': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n","     'polyline': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n","     'polyline-pair': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None)}]}},\n"," 'RGB')"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#checkpoint = \"shi-labs/oneformer_coco_swin_large\"\n","#checkpoint = \"shi-labs/oneformer_cityscapes_swin_large\"\n","#checkpoint = \"shi-labs/oneformer_ade20k_swin_large\"\n","#checkpoint = \"shi-labs/oneformer_coco_dinat_large\"\n","#checkpoint = \"shi-labs/oneformer_cityscapes_dinat_large\"\n","#checkpoint = \"shi-labs/oneformer_ade20k_dinat_large\"\n","checkpoint = \"shi-labs/oneformer_ade20k_swin_tiny\"\n","\n","oneformer_coco_swin = AutoModelForUniversalSegmentation.from_pretrained(checkpoint,\n","                                                                        is_training=True,\n","                                                                        id2label=id2label,\n","                                                                        ignore_mismatched_sizes=True)\n","coco_swin_processor = AutoProcessor.from_pretrained(checkpoint, do_reduce_labels=False, do_rescale=False, do_resize=False, do_normalize=False)\n","coco_swin_processor.image_processor.num_text = oneformer_coco_swin.config.num_queries - oneformer_coco_swin.config.text_encoder_n_ctx\n","\n","\n","train_dataset = railsem_ds[\"train\"]\n","val_dataset = railsem_ds[\"validation\"]\n","\n","#jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n","# #IMAGENET\n","# MEAN = np.array([0.485, 0.456, 0.406])\n","# STD = np.array([0.229, 0.224, 0.225])\n","#ADE20K\n","MEAN = np.array([123.675, 116.280, 103.530]) / 255\n","STD = np.array([58.395, 57.120, 57.375]) / 255\n","\n","train_transform = A.Compose([\n","    A.LongestMaxSize(max_size=1333),\n","    A.RandomCrop(width=512, height=512),\n","    A.HorizontalFlip(p=0.5),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","test_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=MEAN, std=STD),\n","])\n","\n","\n","def train_transforms(batch):\n","    # images = [jitter(x) for x in example_batch['image']]\n","    # images = [x for x in batch['image']]\n","    # segmentation_maps = [np.array(x) for x in batch['semantic_mask_label']]\n","\n","    transformed_images = []\n","    transformed_segmentation_maps = []\n","\n","    for image, segmentation_map in zip(batch['image'], batch['semantic_mask_label']):\n","        image_np = np.array(image)\n","        segmentation_map_np = np.array(segmentation_map)\n","        transformed = train_transform(image=image_np, mask=segmentation_map_np)\n","\n","        # convert the transformed image to C, H, W\n","        transformed_image = transformed['image'].transpose(2,0,1)\n","        transformed_images.append(transformed_image)\n","        transformed_segmentation_maps.append(transformed['mask'])\n","\n","    inputs = coco_swin_processor(images=transformed_images,\n","                                 task_inputs=[\"semantic\"] * len(transformed_images),\n","                                 segmentation_maps=transformed_segmentation_maps,\n","                                 return_tensors=\"pt\")\n","    return inputs\n","\n","\n","def val_transforms(batch):\n","    # images = [x for x in transformed['image']]\n","    # segmentation_maps = [np.array(x) for x in transformed['semantic_mask_label']]\n","\n","    transformed_images = []\n","    transformed_segmentation_maps = []\n","\n","    for image, segmentation_map in zip(batch['image'], batch['semantic_mask_label']):\n","        image_np = np.array(image)\n","        segmentation_map_np = np.array(segmentation_map)\n","        transformed = train_transform(image=image_np, mask=segmentation_map_np)\n","\n","         # convert the transformed image to C, H, W\n","        transformed_image = transformed['image'].transpose(2, 0, 1)\n","        transformed_images.append(transformed_image)\n","        transformed_segmentation_maps.append(transformed['mask'])\n","\n","    inputs = coco_swin_processor(transformed_images,\n","                                 task_inputs=[\"semantic\"] * len(transformed_images),\n","                                 segmentation_maps=transformed_segmentation_maps,\n","                                 return_tensors=\"pt\")\n","    return inputs\n","\n","\n","# Set transforms\n","train_dataset.set_transform(train_transforms)\n","val_dataset.set_transform(val_transforms)\n","\n","def collate_fn(batch): #List[Dict]\n","    batch_dict = {}\n","    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n","    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n","    class_labels = [example[\"class_labels\"] for example in batch]\n","    mask_labels = [example[\"mask_labels\"] for example in batch]\n","    text_inputs = torch.stack([example[\"text_inputs\"] for example in batch])\n","    task_inputs = torch.stack([example[\"task_inputs\"] for example in batch])\n","    batch_dict.update({\"pixel_values\": pixel_values,\n","                       \"pixel_mask\": pixel_mask,\n","                       \"class_labels\": class_labels,\n","                       \"mask_labels\": mask_labels,\n","                       \"text_inputs\": text_inputs,\n","                       \"task_inputs\": task_inputs})\n","    return batch_dict\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"89fc9a51-5c3c-49ec-e55c-f8ed00a0b19c","id":"kOk_0i_Ta59o"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_ade20k_swin_tiny and are newly initialized: ['model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_projector.layers.1.0.weight', 'model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_ade20k_swin_tiny and are newly initialized because the shapes did not match:\n","- model.transformer_module.decoder.class_embed.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([20, 256]) in the model instantiated\n","- model.transformer_module.decoder.class_embed.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([20]) in the model instantiated\n","- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([20]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:426: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["oneformer_coco_swin.config.to_dict().keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YqM128ANZGep","outputId":"e46a246b-e1ff-47b7-e0a5-bb4238a99f29"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['backbone_config', 'ignore_value', 'num_queries', 'no_object_weight', 'class_weight', 'mask_weight', 'dice_weight', 'contrastive_weight', 'contrastive_temperature', 'train_num_points', 'oversample_ratio', 'importance_sample_ratio', 'init_std', 'init_xavier_std', 'layer_norm_eps', 'is_training', 'use_auxiliary_loss', 'output_auxiliary_logits', 'strides', 'task_seq_len', 'text_encoder_width', 'text_encoder_context_length', 'text_encoder_num_layers', 'text_encoder_vocab_size', 'text_encoder_proj_layers', 'text_encoder_n_ctx', 'conv_dim', 'mask_dim', 'hidden_dim', 'encoder_feedforward_dim', 'norm', 'encoder_layers', 'decoder_layers', 'use_task_norm', 'num_attention_heads', 'dropout', 'dim_feedforward', 'pre_norm', 'enforce_input_proj', 'query_dec_layers', 'common_stride', 'num_hidden_layers', 'return_dict', 'output_hidden_states', 'output_attentions', 'torchscript', 'torch_dtype', 'use_bfloat16', 'tf_legacy_loss', 'pruned_heads', 'tie_word_embeddings', 'is_encoder_decoder', 'is_decoder', 'cross_attention_hidden_size', 'add_cross_attention', 'tie_encoder_decoder', 'max_length', 'min_length', 'do_sample', 'early_stopping', 'num_beams', 'num_beam_groups', 'diversity_penalty', 'temperature', 'top_k', 'top_p', 'typical_p', 'repetition_penalty', 'length_penalty', 'no_repeat_ngram_size', 'encoder_no_repeat_ngram_size', 'bad_words_ids', 'num_return_sequences', 'chunk_size_feed_forward', 'output_scores', 'return_dict_in_generate', 'forced_bos_token_id', 'forced_eos_token_id', 'remove_invalid_values', 'exponential_decay_length_penalty', 'suppress_tokens', 'begin_suppress_tokens', 'architectures', 'finetuning_task', 'id2label', 'label2id', 'tokenizer_class', 'prefix', 'bos_token_id', 'pad_token_id', 'eos_token_id', 'sep_token_id', 'decoder_start_token_id', 'task_specific_params', 'problem_type', '_name_or_path', 'transformers_version', 'max_seq_len', 'model_type', 'num_classes'])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":[" oneformer_coco_swin.config.num_labels, oneformer_coco_swin.config.id2label, oneformer_coco_swin.config.num_classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7Op8I1gdqcw","outputId":"9af903db-ea0b-4806-bc5d-1485bdb7a582"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(19,\n"," {0: 'road',\n","  1: 'sidewalk',\n","  2: 'construction',\n","  3: 'tram-track',\n","  4: 'fence',\n","  5: 'pole',\n","  6: 'traffic-light',\n","  7: 'traffic-sign',\n","  8: 'vegetation',\n","  9: 'terrain',\n","  10: 'sky',\n","  11: 'human',\n","  12: 'rail-track',\n","  13: 'car',\n","  14: 'truck',\n","  15: 'trackbed',\n","  16: 'on-rails',\n","  17: 'rail-raised',\n","  18: 'rail-embedded'},\n"," 150)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["oneformer_coco_swin"],"metadata":{"id":"JyXtCP1pe-da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coco_swin_processor.image_processor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRARPbhlRZJk","outputId":"2a093345-2f0a-44f8-eb35-f8cf3419a76c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OneFormerImageProcessor {\n","  \"_max_size\": 1333,\n","  \"class_info_file\": \"coco_panoptic.json\",\n","  \"do_normalize\": false,\n","  \"do_reduce_labels\": false,\n","  \"do_rescale\": false,\n","  \"do_resize\": false,\n","  \"ignore_index\": 255,\n","  \"image_mean\": [\n","    0.48500001430511475,\n","    0.4560000002384186,\n","    0.4059999883174896\n","  ],\n","  \"image_processor_type\": \"OneFormerImageProcessor\",\n","  \"image_std\": [\n","    0.2290000021457672,\n","    0.2239999920129776,\n","    0.22499999403953552\n","  ],\n","  \"metadata\": {\n","    \"0\": \"person\",\n","    \"1\": \"bicycle\",\n","    \"10\": \"fire hydrant\",\n","    \"100\": \"road\",\n","    \"101\": \"roof\",\n","    \"102\": \"sand\",\n","    \"103\": \"sea\",\n","    \"104\": \"shelf\",\n","    \"105\": \"snow\",\n","    \"106\": \"stairs\",\n","    \"107\": \"tent\",\n","    \"108\": \"towel\",\n","    \"109\": \"wall-brick\",\n","    \"11\": \"stop sign\",\n","    \"110\": \"wall-stone\",\n","    \"111\": \"wall-tile\",\n","    \"112\": \"wall-wood\",\n","    \"113\": \"water-other\",\n","    \"114\": \"window-blind\",\n","    \"115\": \"window-other\",\n","    \"116\": \"tree-merged\",\n","    \"117\": \"fence-merged\",\n","    \"118\": \"ceiling-merged\",\n","    \"119\": \"sky-other-merged\",\n","    \"12\": \"parking meter\",\n","    \"120\": \"cabinet-merged\",\n","    \"121\": \"table-merged\",\n","    \"122\": \"floor-other-merged\",\n","    \"123\": \"pavement-merged\",\n","    \"124\": \"mountain-merged\",\n","    \"125\": \"grass-merged\",\n","    \"126\": \"dirt-merged\",\n","    \"127\": \"paper-merged\",\n","    \"128\": \"food-other-merged\",\n","    \"129\": \"building-other-merged\",\n","    \"13\": \"bench\",\n","    \"130\": \"rock-merged\",\n","    \"131\": \"wall-other-merged\",\n","    \"132\": \"rug-merged\",\n","    \"14\": \"bird\",\n","    \"15\": \"cat\",\n","    \"16\": \"dog\",\n","    \"17\": \"horse\",\n","    \"18\": \"sheep\",\n","    \"19\": \"cow\",\n","    \"2\": \"car\",\n","    \"20\": \"elephant\",\n","    \"21\": \"bear\",\n","    \"22\": \"zebra\",\n","    \"23\": \"giraffe\",\n","    \"24\": \"backpack\",\n","    \"25\": \"umbrella\",\n","    \"26\": \"handbag\",\n","    \"27\": \"tie\",\n","    \"28\": \"suitcase\",\n","    \"29\": \"frisbee\",\n","    \"3\": \"motorcycle\",\n","    \"30\": \"skis\",\n","    \"31\": \"snowboard\",\n","    \"32\": \"sports ball\",\n","    \"33\": \"kite\",\n","    \"34\": \"baseball bat\",\n","    \"35\": \"baseball glove\",\n","    \"36\": \"skateboard\",\n","    \"37\": \"surfboard\",\n","    \"38\": \"tennis racket\",\n","    \"39\": \"bottle\",\n","    \"4\": \"airplane\",\n","    \"40\": \"wine glass\",\n","    \"41\": \"cup\",\n","    \"42\": \"fork\",\n","    \"43\": \"knife\",\n","    \"44\": \"spoon\",\n","    \"45\": \"bowl\",\n","    \"46\": \"banana\",\n","    \"47\": \"apple\",\n","    \"48\": \"sandwich\",\n","    \"49\": \"orange\",\n","    \"5\": \"bus\",\n","    \"50\": \"broccoli\",\n","    \"51\": \"carrot\",\n","    \"52\": \"hot dog\",\n","    \"53\": \"pizza\",\n","    \"54\": \"donut\",\n","    \"55\": \"cake\",\n","    \"56\": \"chair\",\n","    \"57\": \"couch\",\n","    \"58\": \"potted plant\",\n","    \"59\": \"bed\",\n","    \"6\": \"train\",\n","    \"60\": \"dining table\",\n","    \"61\": \"toilet\",\n","    \"62\": \"tv\",\n","    \"63\": \"laptop\",\n","    \"64\": \"mouse\",\n","    \"65\": \"remote\",\n","    \"66\": \"keyboard\",\n","    \"67\": \"cell phone\",\n","    \"68\": \"microwave\",\n","    \"69\": \"oven\",\n","    \"7\": \"truck\",\n","    \"70\": \"toaster\",\n","    \"71\": \"sink\",\n","    \"72\": \"refrigerator\",\n","    \"73\": \"book\",\n","    \"74\": \"clock\",\n","    \"75\": \"vase\",\n","    \"76\": \"scissors\",\n","    \"77\": \"teddy bear\",\n","    \"78\": \"hair drier\",\n","    \"79\": \"toothbrush\",\n","    \"8\": \"boat\",\n","    \"80\": \"banner\",\n","    \"81\": \"blanket\",\n","    \"82\": \"bridge\",\n","    \"83\": \"cardboard\",\n","    \"84\": \"counter\",\n","    \"85\": \"curtain\",\n","    \"86\": \"door-stuff\",\n","    \"87\": \"floor-wood\",\n","    \"88\": \"flower\",\n","    \"89\": \"fruit\",\n","    \"9\": \"traffic light\",\n","    \"90\": \"gravel\",\n","    \"91\": \"house\",\n","    \"92\": \"light\",\n","    \"93\": \"mirror-stuff\",\n","    \"94\": \"net\",\n","    \"95\": \"pillow\",\n","    \"96\": \"platform\",\n","    \"97\": \"playingfield\",\n","    \"98\": \"railroad\",\n","    \"99\": \"river\",\n","    \"class_names\": [\n","      \"person\",\n","      \"bicycle\",\n","      \"car\",\n","      \"motorcycle\",\n","      \"airplane\",\n","      \"bus\",\n","      \"train\",\n","      \"truck\",\n","      \"boat\",\n","      \"traffic light\",\n","      \"fire hydrant\",\n","      \"stop sign\",\n","      \"parking meter\",\n","      \"bench\",\n","      \"bird\",\n","      \"cat\",\n","      \"dog\",\n","      \"horse\",\n","      \"sheep\",\n","      \"cow\",\n","      \"elephant\",\n","      \"bear\",\n","      \"zebra\",\n","      \"giraffe\",\n","      \"backpack\",\n","      \"umbrella\",\n","      \"handbag\",\n","      \"tie\",\n","      \"suitcase\",\n","      \"frisbee\",\n","      \"skis\",\n","      \"snowboard\",\n","      \"sports ball\",\n","      \"kite\",\n","      \"baseball bat\",\n","      \"baseball glove\",\n","      \"skateboard\",\n","      \"surfboard\",\n","      \"tennis racket\",\n","      \"bottle\",\n","      \"wine glass\",\n","      \"cup\",\n","      \"fork\",\n","      \"knife\",\n","      \"spoon\",\n","      \"bowl\",\n","      \"banana\",\n","      \"apple\",\n","      \"sandwich\",\n","      \"orange\",\n","      \"broccoli\",\n","      \"carrot\",\n","      \"hot dog\",\n","      \"pizza\",\n","      \"donut\",\n","      \"cake\",\n","      \"chair\",\n","      \"couch\",\n","      \"potted plant\",\n","      \"bed\",\n","      \"dining table\",\n","      \"toilet\",\n","      \"tv\",\n","      \"laptop\",\n","      \"mouse\",\n","      \"remote\",\n","      \"keyboard\",\n","      \"cell phone\",\n","      \"microwave\",\n","      \"oven\",\n","      \"toaster\",\n","      \"sink\",\n","      \"refrigerator\",\n","      \"book\",\n","      \"clock\",\n","      \"vase\",\n","      \"scissors\",\n","      \"teddy bear\",\n","      \"hair drier\",\n","      \"toothbrush\",\n","      \"banner\",\n","      \"blanket\",\n","      \"bridge\",\n","      \"cardboard\",\n","      \"counter\",\n","      \"curtain\",\n","      \"door-stuff\",\n","      \"floor-wood\",\n","      \"flower\",\n","      \"fruit\",\n","      \"gravel\",\n","      \"house\",\n","      \"light\",\n","      \"mirror-stuff\",\n","      \"net\",\n","      \"pillow\",\n","      \"platform\",\n","      \"playingfield\",\n","      \"railroad\",\n","      \"river\",\n","      \"road\",\n","      \"roof\",\n","      \"sand\",\n","      \"sea\",\n","      \"shelf\",\n","      \"snow\",\n","      \"stairs\",\n","      \"tent\",\n","      \"towel\",\n","      \"wall-brick\",\n","      \"wall-stone\",\n","      \"wall-tile\",\n","      \"wall-wood\",\n","      \"water-other\",\n","      \"window-blind\",\n","      \"window-other\",\n","      \"tree-merged\",\n","      \"fence-merged\",\n","      \"ceiling-merged\",\n","      \"sky-other-merged\",\n","      \"cabinet-merged\",\n","      \"table-merged\",\n","      \"floor-other-merged\",\n","      \"pavement-merged\",\n","      \"mountain-merged\",\n","      \"grass-merged\",\n","      \"dirt-merged\",\n","      \"paper-merged\",\n","      \"food-other-merged\",\n","      \"building-other-merged\",\n","      \"rock-merged\",\n","      \"wall-other-merged\",\n","      \"rug-merged\"\n","    ],\n","    \"thing_ids\": [\n","      0,\n","      1,\n","      2,\n","      3,\n","      4,\n","      5,\n","      6,\n","      7,\n","      8,\n","      9,\n","      10,\n","      11,\n","      12,\n","      13,\n","      14,\n","      15,\n","      16,\n","      17,\n","      18,\n","      19,\n","      20,\n","      21,\n","      22,\n","      23,\n","      24,\n","      25,\n","      26,\n","      27,\n","      28,\n","      29,\n","      30,\n","      31,\n","      32,\n","      33,\n","      34,\n","      35,\n","      36,\n","      37,\n","      38,\n","      39,\n","      40,\n","      41,\n","      42,\n","      43,\n","      44,\n","      45,\n","      46,\n","      47,\n","      48,\n","      49,\n","      50,\n","      51,\n","      52,\n","      53,\n","      54,\n","      55,\n","      56,\n","      57,\n","      58,\n","      59,\n","      60,\n","      61,\n","      62,\n","      63,\n","      64,\n","      65,\n","      66,\n","      67,\n","      68,\n","      69,\n","      70,\n","      71,\n","      72,\n","      73,\n","      74,\n","      75,\n","      76,\n","      77,\n","      78,\n","      79\n","    ]\n","  },\n","  \"num_labels\": 133,\n","  \"num_text\": 134,\n","  \"processor_class\": \"OneFormerProcessor\",\n","  \"repo_path\": \"shi-labs/oneformer_demo\",\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"longest_edge\": 1333,\n","    \"shortest_edge\": 800\n","  }\n","}"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["coco_swin_processor.image_processor.num_text, coco_swin_processor.image_processor.num_labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFlA-eCW0SVn","outputId":"ea02ef8e-645e-465b-e7df-98c03bfdf3d3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(134, 133, 133)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["sample = train_dataset[0]\n","sample.keys(), sample[\"pixel_values\"].shape, sample[\"pixel_mask\"].shape, sample[\"mask_labels\"].shape, sample[\"class_labels\"].shape, sample[\"text_inputs\"].shape, sample[\"task_inputs\"].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84kvub27rxw7","outputId":"6620ea07-0ba3-4aeb-e7fe-b32241cef622"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(dict_keys(['pixel_values', 'pixel_mask', 'mask_labels', 'class_labels', 'text_inputs', 'task_inputs']),\n"," torch.Size([3, 512, 512]),\n"," torch.Size([512, 512]),\n"," torch.Size([11, 512, 512]),\n"," torch.Size([11]),\n"," torch.Size([134, 77]),\n"," torch.Size([77]))"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["import numpy as np\n","(sample[\"pixel_values\"].max(), sample[\"pixel_values\"].min()), np.unique(sample[\"pixel_mask\"]), np.unique(sample[\"mask_labels\"]), np.unique(sample[\"class_labels\"]), np.unique(sample[\"text_inputs\"]), np.unique(sample[\"task_inputs\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWrBNwB8TF8S","outputId":"3c536f4d-369c-4c5c-f9da-094b8ad9c9dc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((tensor(2.6400), tensor(-2.1008)),\n"," array([1]),\n"," array([0., 1.], dtype=float32),\n"," array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 11, 18]),\n"," array([    0,   320,   593,  1125,  1550,  1615,  1691,  2292,  2533,\n","         2840,  3231,  4440,  4629,  9629, 10297, 11652, 16451, 29119,\n","        49406, 49407]),\n"," array([    0,   518,   533,  1550, 10549, 29119, 49406, 49407]))"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["#Training settings\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","metric = evaluate.load(\"mean_iou\")\n","\n","training_args = TrainingArguments(\n","    \"rail19_semantic_seg\",\n","    learning_rate=5e-5,\n","    remove_unused_columns=False,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    save_total_limit=3,\n","    evaluation_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    save_steps=2,\n","    eval_steps=2,\n","    logging_steps=1,\n","    eval_accumulation_steps=5,\n","    load_best_model_at_end=True,\n","    # fp16=True,\n","    # push_to_hub=True,\n","    # hub_model_id=hub_model_id,\n","    # hub_strategy=\"end\",\n",")\n","\n","#Metrics\n","def compute_metrics(eval_pred):\n","    print(\"inside met\")\n","    with torch.no_grad():\n","        logits, labels = eval_pred\n","        print(logits.shape)\n","        print(labels.shape)\n","        logits_tensor = torch.from_numpy(logits)\n","        # scale the logits to the size of the label\n","        logits_tensor = nn.functional.interpolate(\n","            logits_tensor,\n","            size=labels.shape[-2:],\n","            mode=\"bilinear\",\n","            align_corners=False,\n","        ).argmax(dim=1)\n","\n","        pred_labels = logits_tensor.detach().cpu().numpy()\n","\n","        metrics = metric._compute(\n","                predictions=pred_labels,\n","                references=labels,\n","                num_labels=len(id2label),\n","                ignore_index=255,\n","                reduce_labels=coco_swin_processor.do_reduce_labels,\n","        )\n","\n","    # add per category metrics as individual key-value pairs\n","    per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n","    per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n","\n","    metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n","    metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n","\n","    return metrics\n","\n","#trainer\n","trainer = Trainer(\n","    model=oneformer_coco_swin,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    # compute_metrics=compute_metrics,\n","    data_collator=collate_fn,\n",")"],"metadata":{"id":"U5FPmkHcB165"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train\n","trainer.train()"],"metadata":{"id":"B3prB9CXq-bE","outputId":"97739d4e-51d0-4efd-ac1a-911b8bd930a6","colab":{"base_uri":"https://localhost:8080/","height":355}},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"output with shape [] doesn't match the broadcast shape [1]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-4a0145cbf3ef>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1868\u001b[0m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_globalstep_last_logged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1870\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtr_loss_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: output with shape [] doesn't match the broadcast shape [1]"]}]},{"cell_type":"code","source":["#Override Trainer\n","class MyTrainer(Trainer):\n","\n","    def training_step(self, model, inputs):\n","        print(\"inside training step\")\n","        print(inputs.keys())\n","        print(inputs[\"pixel_values\"].shape)\n","\n","        loss = super().training_step(model, inputs)\n","\n","        print(loss, loss.shape) #tensor([110.4585])\n","        return loss\n","\n","    def _inner_training_loop(self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None):\n","        print(\"Inside mine\")\n","        return super()._inner_training_loop(batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n","\n","\n","\n","training_args = TrainingArguments(\n","    \"rail19_semantic_seg\",\n","    learning_rate=5e-5,\n","    remove_unused_columns=False,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=2,\n","    logging_steps=1,\n",")\n","\n","#trainer\n","mytrainer = MyTrainer(\n","    model=oneformer_coco_swin,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    data_collator=collate_fn\n",")\n","\n","#training\n","mytrainer.train()"],"metadata":{"id":"5XSdiPHLoA5D","outputId":"b26f2a7d-f2ce-4a20-d1a2-b9cab23337e1","colab":{"base_uri":"https://localhost:8080/","height":459}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inside mine\n","inside training step\n","dict_keys(['pixel_values', 'pixel_mask', 'class_labels', 'mask_labels', 'text_inputs', 'task_inputs'])\n","torch.Size([2, 3, 512, 512])\n","tensor([104.6475]) torch.Size([1])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"output with shape [] doesn't match the broadcast shape [1]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-117-4925a6d975d9>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmytrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1556\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-117-4925a6d975d9>\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inside mine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1868\u001b[0m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_globalstep_last_logged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1870\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtr_loss_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: output with shape [] doesn't match the broadcast shape [1]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SllFyeVdjdGS"},"execution_count":null,"outputs":[]}]}