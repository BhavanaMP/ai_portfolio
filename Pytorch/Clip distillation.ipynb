{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","authorship_tag":"ABX9TyPXRKKI/Nb4monecV2nSwMk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# import locale\n","# locale.getpreferredencoding = lambda: \"UTF-8\""],"metadata":{"id":"3n6zM3s3tg0Z","executionInfo":{"status":"ok","timestamp":1709383185782,"user_tz":-60,"elapsed":16,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpPZJvfbrrbB","executionInfo":{"status":"ok","timestamp":1709383248627,"user_tz":-60,"elapsed":62860,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"dffe6142-cb3a-49be-f290-f0dc01983635"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-3qt598dv\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-3qt598dv\n","  Resolved https://github.com/huggingface/transformers to commit 831bc25d8fdb85768402f772cf65cc3d7872b211\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8593748 sha256=998e1c0de7cc95a5ad361a51a6a31dc5f4bab584d3ed9b896e02ccc2234e1c9d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-09o7jh71/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n","Successfully built transformers\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.38.1\n","    Uninstalling transformers-4.38.1:\n","      Successfully uninstalled transformers-4.38.1\n","Successfully installed transformers-4.39.0.dev0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMuIk94rMAOz","executionInfo":{"status":"ok","timestamp":1709383263056,"user_tz":-60,"elapsed":14444,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"afcf85fa-2b28-4679-cd9b-ec3327409ce8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb\n","  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchmetrics\n","  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-1.40.6-py2.py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Installing collected packages: torchinfo, smmap, setproctitle, sentry-sdk, lightning-utilities, docker-pycreds, dill, responses, multiprocess, gitdb, torchmetrics, GitPython, wandb, datasets, evaluate\n","Successfully installed GitPython-3.1.42 datasets-2.18.0 dill-0.3.8 docker-pycreds-0.4.0 evaluate-0.4.1 gitdb-4.0.11 lightning-utilities-0.10.1 multiprocess-0.70.16 responses-0.18.0 sentry-sdk-1.40.6 setproctitle-1.3.3 smmap-5.0.1 torchinfo-1.8.0 torchmetrics-1.3.1 wandb-0.16.3\n"]}],"source":["!pip install datasets evaluate wandb torchmetrics torchinfo"]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.cuda.amp import GradScaler, autocast\n","from transformers import AutoModelForUniversalSegmentation, AutoProcessor\n","import torchvision.transforms as tvt\n","from torchvision.transforms.functional import crop\n","from torchmetrics.classification import Accuracy, MulticlassAccuracy, MulticlassJaccardIndex, JaccardIndex\n","import albumentations as A\n","from tqdm.auto import tqdm\n","from huggingface_hub import notebook_login\n","from datasets import load_dataset\n","import wandb\n","import evaluate\n","import statistics\n","from copy import deepcopy\n","from huggingface_hub import hf_hub_download\n","import torchinfo\n","\n","import numpy as np\n","import pandas as pd\n","import random\n","import requests\n","import json\n","from pathlib import Path\n","import os\n","from typing import List, Dict, Tuple\n","import warnings\n","from PIL import Image as PILImage\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","#warnings.filterwarnings(\"ignore\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","def set_seeds(seed):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.empty_cache()\n","    torch.backends.cudnn.benchmark = True\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.enabled = True"],"metadata":{"id":"5-zKn2VURvcK","executionInfo":{"status":"ok","timestamp":1709383280462,"user_tz":-60,"elapsed":17424,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# import os\n","# os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"],"metadata":{"id":"iQL9HbLRiPaT","executionInfo":{"status":"ok","timestamp":1709383280463,"user_tz":-60,"elapsed":19,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#!huggingface-cli login\n","from huggingface_hub import login\n","from google.colab import userdata\n","import wandb\n","\n","login(token=userdata.get('HF_TOKEN'))\n","wandb.login(key=userdata.get('WANDB_API_KEY'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-3lOygnRzIe","executionInfo":{"status":"ok","timestamp":1709383286893,"user_tz":-60,"elapsed":6448,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"22d8a23e-2810-4b98-c4e1-884ad7681281"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["def get_labels():\n","    data_directory = Path(\".\")\n","    json_file_path = data_directory / \"labels_info.json\"\n","    if json_file_path.is_file():\n","        print(f\"[INFO]: Found {json_file_path}.Skipping Download...\")\n","    else:\n","        print(\"[INFO]: Downloading labels_info.json from hub\")\n","        json_file_path = hf_hub_download(\n","            repo_id=\"BhavanaMalla/railsem19-semantic-expanded\",\n","            filename=\"labels_info.json\",\n","            repo_type=\"dataset\",\n","            local_dir=data_directory\n","        )\n","    with open(json_file_path, \"r\") as f:\n","        labels_info = json.load(f)\n","    id2label = labels_info[\"id2label\"]\n","    label2id = labels_info[\"label2id\"]\n","    labels = labels_info[\"labels\"]\n","    color_palette = labels_info[\"color_palette\"]\n","\n","    # add the background label\n","    id2label[\"19\"] = \"background\"\n","    label2id[\"background\"] = 19\n","    labels.append(\"background\")\n","    color_palette.append([0, 0, 0])\n","\n","    # correcting the labels\n","    id2label = {int(key): value.replace('-', '_') \\\n","                for key, value in id2label.items()}\n","    label2id = {key.replace('-', '_'): value for key, value in label2id.items()}\n","    labels = [label.replace('-', '_') for label in labels]\n","    return {\"id2label\": id2label, \"label2id\": label2id, \"labels\": labels,\n","            \"color_palette\": color_palette}"],"metadata":{"id":"v6T8bqbUOipW","executionInfo":{"status":"ok","timestamp":1709383286893,"user_tz":-60,"elapsed":23,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def load_railsem_dataset():\n","    print(f\"[INFO]: Extracting Railsem19 dataset from hub on\")\n","    railsem_ds = load_dataset(\"BhavanaMalla/railsem19-semantic-expanded\")\n","    return railsem_ds"],"metadata":{"id":"OWCZWNLkOl7X","executionInfo":{"status":"ok","timestamp":1709383286893,"user_tz":-60,"elapsed":22,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def get_modified_labels():\n","    modified_id2label = {\n","        0: \"road\", 1: \"sidewalk\", 2: \"construction_fence\",\n","        3: \"rail_raised_rail_embedded\", 4: \"pole_traffic_light_traffic_sign\",\n","        5: \"sky\", 6: \"human\", 7: \"tram_track_rail_track\", 8: \"car_truck\",\n","        9: \"on_rails\", 10: \"vegetation\", 11: \"trackbed\",\n","        12: \"background_terrain\"\n","    }\n","    modified_label2id = {\n","        label: id for id, label in modified_id2label.items()\n","    }\n","    modified_labels = [label for label in modified_id2label.values()]\n","    return modified_id2label, modified_label2id, modified_labels"],"metadata":{"id":"vjwF6kvLOnzG","executionInfo":{"status":"ok","timestamp":1709383286894,"user_tz":-60,"elapsed":22,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class RemapBackground():\n","    \"\"\" Remap background to label 19 \"\"\"\n","    def __call__(self, mask):\n","        return torch.where(mask > 18, 19, mask)\n","\n","\n","class RemapLabels():\n","    def __init__(self):\n","        self.class_mapping = get_labels()[\"id2label\"]\n","        self.class_coding = {v: k for k, v in self.class_mapping.items()}\n","\n","        # Remapping of original 20 labels to 13 labels\n","        self.modified_labels = {\n","            0: [\"road\"], 1: [\"sidewalk\"],\n","            2: [\"construction\", \"fence\"],\n","            3: [\"rail_raised\", \"rail_embedded\"],\n","            4: [\"pole\", \"traffic_light\", \"traffic_sign\"],\n","            5: [\"sky\"], 6: [\"human\"],\n","            7: [\"tram_track\", \"rail_track\"],\n","            8: [\"car\", \"truck\"], 9: [\"on_rails\"],\n","            10: [\"vegetation\"], 11: [\"trackbed\"],\n","            12: [\"background\", \"terrain\"]\n","        }\n","        self.modified_ids = {}\n","        for k, v in self.modified_labels.items():\n","            self.modified_ids[k] = [self.class_coding[label] for label in v]\n","\n","    def __call__(self, mask):\n","        final_label = np.zeros_like(mask)\n","        for key, val in self.modified_ids.items():\n","            specific_label = np.zeros_like(mask)\n","            specific_label = np.where(np.isin(mask, np.array(val)), 1, 0)\n","            specific_label *= key\n","            final_label = np.add(final_label, specific_label)\n","        return torch.from_numpy(final_label)\n","\n"],"metadata":{"id":"lqdJP_AqOVEf","executionInfo":{"status":"ok","timestamp":1709383286894,"user_tz":-60,"elapsed":22,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class PatchQueue(Dataset):\n","    '''\n","    Extract random patches from an image -> put them in queue\n","    -> feed them to the dataloader\n","    ref : https://github.com/fepegar/torchio/blob/main/src/torchio/data/queue.py\n","    '''\n","    def __init__(self, dataset, max_length,  # Queue Length\n","                 samples_per_image,  # Patches per image\n","                 queue_workers=4,  # Num Workers\n","                 patch_size=[512, 512],  # Patch Size\n","                 shuffle=False):\n","        self.dataset = dataset\n","        self.max_length = max_length\n","        self.queue_workers = queue_workers\n","        self.samples_per_image = samples_per_image\n","        self.shuffle_queue = shuffle\n","        self.patch_size = patch_size\n","        self._images_iterable = None\n","        self.patch_list = list()\n","        self._num_sampled_images = 0\n","        self.steps_per_epoch = len(self.dataset) * self.samples_per_image\n","        self.resize_transform = tvt.Resize(patch_size[::-1])\n","\n","    def __len__(self):\n","        return self.steps_per_epoch\n","\n","    def __getitem__(self, item):\n","        if not self.patch_list:\n","            print(\"[WARN]: Patch List is empty...\")\n","            self._fill()\n","            self.patch_list.reverse()\n","        sample_patch = self.patch_list.pop()\n","        return sample_patch\n","\n","    @staticmethod\n","    def _get_first_item(batch):\n","        return batch[0]\n","\n","    def initialize_images_iterable(self):\n","        self._images_iterable = self._get_images_iterable()\n","\n","    @property\n","    def images_iterable(self):\n","        if self._images_iterable is None:\n","            self.initialize_images_iterable()\n","        return self.images_iterable\n","\n","    def _get_images_iterable(self):\n","        if torch.distributed.is_available() and torch.distributed.is_initialized():\n","            sampler = torch.utils.data.DistributedSampler(\n","                self.dataset,\n","                num_replicas=torch.distributed.get_world_size(),\n","                rank=torch.distributed.get_rank(), shuffle=self.shuffle_queue\n","            )\n","        else:\n","            sampler = None\n","        loader = DataLoader(\n","            self.dataset,  # num_workers=self.queue_workers, #in multigpu, >0 not wrkng\n","            batch_size=1, collate_fn=self._get_first_item, pin_memory=True,\n","            shuffle=False, sampler=sampler\n","        )\n","        self._num_sampled_images = 0\n","        return iter(loader)\n","\n","    def _get_next_image(self):\n","        try:\n","            image = next(self._images_iterable)\n","        except Exception as e:\n","            print(f\"[WARN]:Exception while getting image for patching: {e}\")\n","            self.initialize_images_iterable()\n","            image = next(self._images_iterable)\n","        return image\n","\n","    def extract_patches(self, image_pair, samples_per_image, patch_size):\n","        image_patches = []\n","        img_height = image_pair['image'].shape[-2]\n","        img_width = image_pair['image'].shape[-1]\n","        # Check if image size is smaller than patch size\n","        if img_height < patch_size[0] or img_width < patch_size[1]:\n","            # Resize the image to match the patch size\n","            image_pair[\"image\"] = self.resize_transform(image_pair[\"image\"])\n","            image_pair[\"mask\"] = self.resize_transform(image_pair[\"mask\"])\n","            img_height = image_pair[\"image\"].shape[-2]\n","            img_width = image_pair[\"image\"].shape[-1]\n","        for _ in range(samples_per_image):\n","            left = torch.randint(\n","                low=0, high=img_width-patch_size[0], size=[1,]\n","            ).item()\n","            top = torch.randint(\n","                low=0, high=img_height-patch_size[1], size=[1,]\n","            ).item()\n","            cropped_image = crop(\n","                img=image_pair[\"image\"], top=top, left=left,\n","                height=patch_size[1], width=patch_size[0]\n","            )\n","            cropped_labels = crop(\n","                img=image_pair[\"mask\"], top=top, left=left,\n","                height=patch_size[1], width=patch_size[0]\n","            )\n","            image_patches.append({\"image\": cropped_image,\n","                                  \"mask\": cropped_labels})\n","        return image_patches\n","\n","    def _fill(self):\n","        while True:\n","            image_pair = self._get_next_image()\n","            samples_per_image = self.samples_per_image\n","            patch_size = self.patch_size\n","            patches = self.extract_patches(\n","                image_pair, samples_per_image, patch_size\n","            )\n","            self.patch_list.extend(patches)\n","            self._num_sampled_images += 1\n","            islistfull = len(self.patch_list) >= self.max_length\n","            if islistfull:\n","                break"],"metadata":{"id":"UPCD4IacPnBI","executionInfo":{"status":"ok","timestamp":1709383286894,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def get_transforms():\n","    # Transforms\n","    image_transforms = tvt.Compose([tvt.ToTensor(),])\n","    mask_transforms = tvt.Compose([tvt.PILToTensor(),\n","                                  RemapBackground(),\n","                                  RemapLabels(),])\n","    return image_transforms, mask_transforms"],"metadata":{"id":"IKTtSkwnOajS","executionInfo":{"status":"ok","timestamp":1709383286894,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, dataset, transforms=None):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image = self.dataset[idx][\"image\"]\n","        semantic_mask = self.dataset[idx][\"semantic_mask_label\"]\n","        if self.transforms:\n","            image_transforms, mask_transforms = self.transforms\n","            transformed_image = image_transforms(image)\n","            transformed_mask = mask_transforms(semantic_mask)\n","        else:\n","            image_transforms, mask_transforms = get_transforms()\n","            transformed_image = image_transforms(image)\n","            transformed_mask = mask_transforms(semantic_mask)\n","        return {\n","            \"image\": transformed_image, \"mask\": transformed_mask\n","        }"],"metadata":{"id":"abSPValFOXsh","executionInfo":{"status":"ok","timestamp":1709383286894,"user_tz":-60,"elapsed":20,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def _collate_fn(batch):\n","    images = torch.stack([sample[\"image\"] for sample in batch])\n","    segmentation_maps = torch.cat([sample[\"mask\"] for sample in batch])\n","    batch = processor(images, segmentation_maps=segmentation_maps, task_inputs=[\"semantic\"] * len(images), return_tensors=\"pt\")\n","    batch[\"original_images\"] = images\n","    batch[\"original_segmentation_maps\"] = segmentation_maps\n","    return batch\n","\n","def _prepare_dataloader(dataset: torch.utils.data.Dataset):\n","    sampler = None\n","    is_train=True\n","    dataloader = DataLoader(dataset, batch_size=2, pin_memory=True, shuffle=(is_train and sampler is None), sampler=sampler, collate_fn=_collate_fn)\n","    return dataloader"],"metadata":{"id":"cjskHobAR8XE","executionInfo":{"status":"ok","timestamp":1709383286894,"user_tz":-60,"elapsed":20,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["####Token Embedding:\n","\n","The text tensor represents the input tokens with shape `(batch_size*234, 77)`, where 234 is the number of text sequences and 77 is the maximum sequence length.\n","The token_embedding layer maps each token to a continuous vector representation. After passing through this layer, hidden_state will have a shape of `(batch_size*234, 77, width)`.\n","\n","####Positional Embedding Addition:\n","\n","The positional_embedding parameter provides positional information to the input tokens to help the model understand their position in the sequence.\n","This positional embedding is added to the token embeddings element-wise, maintaining the shape of hidden_state as (batch_size*234, 77, width).\n","####Permutation:\n","\n","The dimensions of the hidden_state tensor are permuted to bring the batch dimension to the second position. After permutation, the shape becomes (77, batch_size*234, width).\n","####Transformer Layer:\n","\n","The hidden_state tensor is passed through the transformer model (self.transformer). The transformer processes the input tokens, capturing their contextual dependencies.\n","The output of the transformer layer retains the shape (77, batch_size*234, width).\n","####Permutation (Back to Original Shape):\n","\n","After the transformer layer, the dimensions are permuted back to the original shape, with the batch dimension in the first position. Now, the shape is (batch_size*234, 77, width).\n","####Layer Normalization:\n","\n","The ln_final layer applies layer normalization along the last dimension of the hidden_state tensor. This layer ensures that the mean activation of each neuron is close to 0 and the standard deviation is close to 1.\n","After layer normalization, the shape remains (batch_size*234, 77, width).\n","####Extract Token Representations:\n","\n","Finally, the token representation for each text sequence is extracted. This is done by selecting the token with the highest value along the last dimension (argmax operation) since this dimension corresponds to the token embeddings.\n","The resulting hidden_state tensor has a shape of (batch_size*234, width), where each row represents the token representation for a specific text sequence.\n","\n","####Note on permutation on step 3\n","The permutation step in the forward function of the OneFormerTextEncoder module is necessary to ensure that the input tensor has the correct shape before being passed through the transformer layer. Let's break down why this permutation is needed:\n","\n","- Initial Embedding: Initially, the input tensor hidden_state has the shape (batch_size*234, 77, width). This shape corresponds to (sequence_length, batch_size, embedding_dim), where:\n","\n","- sequence_length represents the length of each text sequence (77 tokens in this case).\n","batch_size*234 represents the total number of text sequences in the batch, obtained by concatenating the text sequences along the batch dimension.\n","Permutation: The permutation step hidden_state.permute(1, 0, 2) changes the order of dimensions in the tensor. Specifically, it rearranges the dimensions such that:\n","\n","The dimension corresponding to the sequence length (77) becomes the first dimension.\n","The dimension corresponding to the batch size multiplied by the number of text sequences (batch_size*234) becomes the second dimension.\n","The embedding dimension (width) remains unchanged as the third dimension.\n","After permutation, the shape of the tensor becomes (77, batch_size*234, width), which is the expected shape for input to the transformer layer.\n","\n","Transformer Layer: The permuted tensor is then passed through the transformer layer, which operates on this input shape (sequence_length, batch_size*234, width).\n","https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n","In summary, the permutation step ensures that the input tensor is properly shaped to match the requirements of the transformer layer, enabling correct processing of the text sequences."],"metadata":{"id":"FU0D1d0cXbB3"}},{"cell_type":"markdown","source":["``` override oneformer processor _preprocess_text to get input_masks and attention_mask```\n","\n","```\n","from transformers import CLIPProcessor\n","\n","class CustomOneFormerProcessor(OneFormerProcessor):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","    \n","    def _preprocess_text(self, text_list=None, max_length=77, text_inputs=False):\n","        # Override the method to change the preprocessing logic\n","        if text_list is None:\n","            raise ValueError(\"tokens cannot be None.\")\n","\n","        tokens = self.tokenizer(text_list, padding=\"max_length\", max_length=max_length, truncation=True)\n","\n","        attention_masks, input_ids = tokens[\"attention_mask\"], tokens[\"input_ids\"]\n","        \n","        if text_inputs:\n","           return input_ids, attention_masks\n","\n","        token_inputs = []\n","        for attn_mask, input_id in zip(attention_masks, input_ids):\n","            token = torch.tensor(attn_mask) * torch.tensor(input_id)\n","            token_inputs.append(token.unsqueeze(0))\n","\n","        token_inputs = torch.cat(token_inputs, dim=0)\n","        \n","        return token_inputs\n","\n","    def __call__(self, images=None, task_inputs=None, segmentation_maps=None, **kwargs):\n","\n","        if task_inputs is None:\n","            raise ValueError(\"You have to specify the task_input. Found None.\")\n","        elif images is None:\n","            raise ValueError(\"You have to specify the image. Found None.\")\n","\n","        if not all(task in [\"semantic\", \"instance\", \"panoptic\"] for task in task_inputs):\n","            raise ValueError(\"task_inputs must be semantic, instance, or panoptic.\")\n","\n","        encoded_inputs = self.image_processor(images, task_inputs, segmentation_maps, **kwargs)\n","\n","        if isinstance(task_inputs, str):\n","            task_inputs = [task_inputs]\n","\n","        if isinstance(task_inputs, List) and all(isinstance(task_input, str) for task_input in task_inputs):\n","            task_token_inputs = []\n","            for task in task_inputs:\n","                task_input = f\"the task is {task}\"\n","                task_token_inputs.append(task_input)\n","            encoded_inputs[\"task_inputs\"] = self._preprocess_text(task_token_inputs, max_length=self.task_seq_length)\n","        else:\n","            raise TypeError(\"Task Inputs should be a string or a list of strings.\")\n","\n","        if hasattr(encoded_inputs, \"text_inputs\"):\n","            texts_list = encoded_inputs.text_inputs\n","\n","            text_inputs = []\n","            for texts in texts_list:\n","                text_input_list = self._preprocess_text(texts, max_length=self.max_seq_length, text_inputs=True)\n","                # text_inputs.append(text_input_list.unsqueeze(0))\n","                text_inputs.append(text_input_list)\n","\n","            encoded_inputs[\"text_inputs\"] = torch.cat(text_inputs, dim=0)\n","\n","        return encoded_inputs\n","\n","    def encode_inputs(self, images=None, task_inputs=None, segmentation_maps=None, **kwargs):\n","        \"\"\"\n","        This method forwards all its arguments to [`OneFormerImageProcessor.encode_inputs`] and then tokenizes the\n","        task_inputs. Please refer to the docstring of this method for more information.\n","        \"\"\"\n","\n","        if task_inputs is None:\n","            raise ValueError(\"You have to specify the task_input. Found None.\")\n","        elif images is None:\n","            raise ValueError(\"You have to specify the image. Found None.\")\n","\n","        if not all(task in [\"semantic\", \"instance\", \"panoptic\"] for task in task_inputs):\n","            raise ValueError(\"task_inputs must be semantic, instance, or panoptic.\")\n","\n","        encoded_inputs = self.image_processor.encode_inputs(images, task_inputs, segmentation_maps, **kwargs)\n","\n","        if isinstance(task_inputs, str):\n","            task_inputs = [task_inputs]\n","\n","        if isinstance(task_inputs, List) and all(isinstance(task_input, str) for task_input in task_inputs):\n","            task_token_inputs = []\n","            for task in task_inputs:\n","                task_input = f\"the task is {task}\"\n","                task_token_inputs.append(task_input)\n","            encoded_inputs[\"task_inputs\"] = self._preprocess_text(task_token_inputs, max_length=self.task_seq_length)\n","        else:\n","            raise TypeError(\"Task Inputs should be a string or a list of strings.\")\n","\n","        if hasattr(encoded_inputs, \"text_inputs\"):\n","            texts_list = encoded_inputs.text_inputs\n","\n","            text_inputs = []\n","            for texts in texts_list:\n","                text_input_list = self._preprocess_text(texts, max_length=self.max_seq_length, text_inputs=True)\n","                # text_inputs.append(text_input_list.unsqueeze(0))\n","                text_inputs.append(text_input_list)\n","\n","            encoded_inputs[\"text_inputs\"] = torch.cat(text_inputs, dim=0)\n","\n","        return encoded_inputs\n","```"],"metadata":{"id":"gu6Hr1hj3ekD"}},{"cell_type":"markdown","source":["- make `oneformer.text_mapper.text_encoder_width = 512` for oneformer and then project it to 256 dimension. we can perform feature distillation in this case\n","- use clip to create text embedding for each text of image\n","- use the transformer using in oneformer, but reduce the layers of the text mapper transformer using `text_encoder_num_layers` may be to 2 . keep in ind of attention heads creeating from embedding dimension\n","- attention heads = width // 64 = 512 // 64 = 8 , 256 // 64 = 4"],"metadata":{"id":"QYENFFjbZqIY"}},{"cell_type":"code","source":["# import torch\n","# from torch import nn\n","\n","# batch, sentence_length, embedding_dim = 20, 5, 10\n","# embedding = torch.randn(batch, sentence_length, embedding_dim)\n","# layer_norm = nn.LayerNorm(embedding_dim)\n","# hidden_state = layer_norm(embedding) #20, 5, 10\n","# hidden_state.shape, hidden_state"],"metadata":{"id":"6BkGw849ZeN_","executionInfo":{"status":"ok","timestamp":1709383286895,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from torch import nn, Tensor\n","from transformers import OneFormerConfig\n","from transformers.models.oneformer.modeling_oneformer import OneFormerTextMapper, OneFormerTextEncoder, OneFormerMLPPredictionHead\n","\n","\n","# Define your custom subclass of OneFormerTextMapper to override encode_text to return distillation logits\n","# FIXME: Can we improve this by moving embedding layer to OneFormerModel and concat there?\n","class CustomOneFormerTextMapper(OneFormerTextMapper):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        if config.text_encoder_n_ctx > 0:\n","            self.prompt_ctx = nn.Embedding(\n","                    config.text_encoder_n_ctx,\n","                    # config.text_encoder_width,\n","                    config.hidden_dim, #config.text_encoder_width change it to hidden_dim as we are chaging text_encoder_width = 512 , withoutit we get concatentation failure in forward bcz of shape mismatch of projected encoded text\n","                )\n","\n","    def forward(self, inputs: Tensor) -> Tensor:\n","        text_queries, text_queries_distillation_logits = self.encode_text(inputs)\n","        return text_queries, text_queries_distillation_logits\n","\n","    def encode_text(self, text):\n","        print(\"custom text mapper encoder output\")\n","        if text.ndim is None:\n","            raise ValueError(\"text must not be NoneType\")\n","        if text.ndim not in [2, 3]:\n","            raise ValueError(\"Number of dimensions in text must be 2 or 3\")\n","        squeeze_dim = False\n","        num_text = 1\n","        if text.ndim == 3:\n","            num_text = text.shape[1]\n","            batch_size, num_text, hidden_dim = text.shape\n","            print(\"text shape in encode_text textmapper- (bs, num_text, hidden_dim)\")\n","            print(text.shape)\n","            text = text.reshape(batch_size * num_text, hidden_dim)\n","            squeeze_dim = True\n","\n","        encoded_text = self.text_encoder(text)  # bsx234 , width =  bsx234, 512 for ours\n","        print(\"encoded_text\")\n","        print(encoded_text.shape)\n","        _, hidden_dim = encoded_text.shape\n","        text_queries_before_ctx = encoded_text.reshape(batch_size, num_text, hidden_dim) #bs, 234, 512\n","        print(\"text_queries_before_ctx\")\n","        print(text_queries_before_ctx.shape)\n","        #projection\n","        text_queries = self.text_projector(encoded_text)  # bsx234, 256\n","        print(\"text_queries after projection\")\n","        print(text_queries.shape)\n","        if squeeze_dim:\n","            _, hidden_dim = text_queries.shape\n","            text_queries = text_queries.reshape(batch_size, num_text, hidden_dim)\n","            if self.prompt_ctx is not None:\n","                text_queries_ctx = self.prompt_ctx.weight.unsqueeze(0).repeat(text_queries.shape[0], 1, 1)\n","                text_queries = torch.cat([text_queries, text_queries_ctx], dim=1)\n","                # RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 256 but got size 512 for tensor number 1 in the list.\n","        return text_queries, text_queries_before_ctx"],"metadata":{"id":"Ltv0jyfz_XO1","executionInfo":{"status":"ok","timestamp":1709383286895,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from transformers.models.oneformer.modeling_oneformer import OneFormerModelOutput\n","from typing import Optional\n","from dataclasses import dataclass\n","from transformers.utils import ModelOutput\n","\n","@dataclass\n","class CustomOneFormerModelOutput(ModelOutput):\n","    print(\"inside custm one former model output\")\n","    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    pixel_decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    transformer_decoder_hidden_states: Optional[torch.FloatTensor] = None\n","    transformer_decoder_object_queries: torch.FloatTensor = None\n","    transformer_decoder_contrastive_queries: Optional[torch.FloatTensor] = None\n","    transformer_decoder_mask_predictions: torch.FloatTensor = None\n","    transformer_decoder_class_predictions: torch.FloatTensor = None\n","    transformer_decoder_auxiliary_predictions: Optional[Tuple[Dict[str, torch.FloatTensor]]] = None\n","    text_queries: Optional[torch.FloatTensor] = None\n","    text_queries_distillation_logits: Optional[torch.FloatTensor] = None  # new\n","    task_token: torch.FloatTensor = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iYkv2KdGY3Wi","executionInfo":{"status":"ok","timestamp":1709383286895,"user_tz":-60,"elapsed":21,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"829de213-b1c6-45f8-830e-737e5ffbac98"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["inside custm one former model output\n"]}]},{"cell_type":"code","source":["from torch import nn, Tensor\n","from transformers import OneFormerConfig\n","from transformers.models.oneformer.modeling_oneformer import OneFormerModel, OneFormerModelOutput, OneFormerLoss\n","\n","\n","class CustomOneFormerModel(OneFormerModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        if self.is_training:\n","            self.text_mapper = CustomOneFormerTextMapper(config)\n","        else:\n","            self.text_mapper = None\n","\n","    def forward(self, pixel_values, task_inputs, text_inputs=None, pixel_mask=None, output_hidden_states=None, output_attentions=None, return_dict=None,) -> CustomOneFormerModelOutput:\n","        print(\"inside my custom model\")\n","        if pixel_values is None:\n","            raise ValueError(\"You have to specify pixel_values\")\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        batch_size, _, height, width = pixel_values.shape\n","\n","        if pixel_mask is None:\n","            pixel_mask = torch.ones((batch_size, height, width), device=pixel_values.device)\n","\n","        pixel_level_module_output = self.pixel_level_module(pixel_values, output_hidden_states)\n","\n","        multi_scale_features = pixel_level_module_output.decoder_features\n","        mask_features = pixel_level_module_output.decoder_last_feature\n","\n","        task_token = self.task_encoder(task_inputs.to(self.dtype))\n","\n","        if self.is_training:\n","            text_queries, text_queries_distillation_logits = self.text_mapper(text_inputs)  # new\n","        else:\n","            text_queries = None\n","            text_queries_distillation_logits = None\n","\n","        transformer_module_output = self.transformer_module(\n","            multi_scale_features=multi_scale_features,\n","            mask_features=mask_features,\n","            task_token=task_token,\n","            output_attentions=output_attentions,\n","        )\n","\n","        queries = transformer_module_output.object_queries\n","\n","        encoder_hidden_states = None\n","        pixel_decoder_hidden_states = None\n","        transformer_decoder_hidden_states = None\n","\n","        if output_hidden_states:\n","            encoder_hidden_states = pixel_level_module_output.encoder_features\n","            pixel_decoder_hidden_states = (pixel_level_module_output.decoder_last_feature,)\n","            for f in pixel_level_module_output.decoder_features:\n","                pixel_decoder_hidden_states += (f,)\n","            transformer_decoder_hidden_states = transformer_module_output.auxiliary_predictions\n","\n","        output = CustomOneFormerModelOutput(\n","            encoder_hidden_states=encoder_hidden_states,\n","            pixel_decoder_hidden_states=pixel_decoder_hidden_states,\n","            transformer_decoder_hidden_states=transformer_decoder_hidden_states,\n","            transformer_decoder_object_queries=queries,\n","            transformer_decoder_contrastive_queries=transformer_module_output.contrastive_logits,\n","            transformer_decoder_mask_predictions=transformer_module_output.prediction_masks,\n","            transformer_decoder_class_predictions=transformer_module_output.prediction_class,\n","            transformer_decoder_auxiliary_predictions=transformer_module_output.auxiliary_predictions,\n","            text_queries=text_queries,\n","            text_queries_distillation_logits=text_queries_distillation_logits,  # new\n","            task_token=task_token,\n","            attentions=transformer_module_output.attentions,\n","        )\n","\n","        if not return_dict:\n","            output = tuple(v for v in output.values())\n","\n","        return output"],"metadata":{"id":"4Uvg2VZ_TTLp","executionInfo":{"status":"ok","timestamp":1709383286895,"user_tz":-60,"elapsed":14,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["@dataclass\n","class CustomOneFormerForUniversalSegmentationOutput(ModelOutput):\n","    loss: Optional[torch.FloatTensor] = None\n","    class_queries_logits: torch.FloatTensor = None\n","    masks_queries_logits: torch.FloatTensor = None\n","    auxiliary_predictions: List[Dict[str, torch.FloatTensor]] = None\n","    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    pixel_decoder_hidden_states: Optional[List[torch.FloatTensor]] = None\n","    transformer_decoder_hidden_states: Optional[torch.FloatTensor] = None\n","    transformer_decoder_object_queries: torch.FloatTensor = None\n","    transformer_decoder_contrastive_queries: Optional[torch.FloatTensor] = None\n","    transformer_decoder_mask_predictions: torch.FloatTensor = None\n","    transformer_decoder_class_predictions: torch.FloatTensor = None\n","    transformer_decoder_auxiliary_predictions: Optional[List[Dict[str, torch.FloatTensor]]] = None\n","    text_queries: Optional[torch.FloatTensor] = None\n","    text_queries_distillation_logits: Optional[torch.FloatTensor] = None  # new\n","    task_token: torch.FloatTensor = None\n","    attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None"],"metadata":{"id":"wQU_RfIHgOZ4","executionInfo":{"status":"ok","timestamp":1709383286895,"user_tz":-60,"elapsed":14,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from torch import nn\n","def sigmoid_cross_entropy_loss(inputs: torch.Tensor, labels: torch.Tensor, num_masks: int) -> torch.Tensor:\n","    #new\n","    # resized_inputs = nn.functional.interpolate(inputs, size=(labels.size(-2), labels.size(-1)), mode=\"nearest\") #, align_corners=False ##ValueError: Target size (torch.Size([13, 1, 512, 512])) must be the same as input size (torch.Size([13, 1, 128, 128])) when predmasks and target masks are sent\n","    # print(f\"resized_inputs: {resized_inputs.shape}\") #torch.Size([17, 1, 512, 512])\n","\n","    print(f\"sigmoid_cross_entropy_loss_fn - inputs: {inputs.shape}, labels: {labels.shape}, num_masks: {num_masks}\")\n","    #inputs: torch.Size([17, 12544]), labels: torch.Size([17, 12544]), num_masks: tensor([17.], device='cuda:0')\n","    # for resized_inputs - torch.Size([17, 1, 128, 128]) #torch.Size([17, 1, 512, 512]) #tensor([17.]\n","\n","    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    # apply ohem here\n","    cross_entropy_loss = criterion(inputs, labels) # resized_inputs\n","    print(f\"cross_entropy_loss: {cross_entropy_loss.shape}, {cross_entropy_loss}\")\n","    # torch.Size([17, 12544])\n","    # for resized_inputs - torch.Size([17, 1, 512, 512])\n","\n","    loss = cross_entropy_loss.mean(1).sum() / num_masks\n","    print(f\"mean_cross_entropy_loss: {loss}\") # mean_cross_entropy_loss: tensor([0.4132])\n","    return loss\n","\n","def dice_loss(inputs: Tensor, labels: Tensor, num_masks: int) -> Tensor:\n","    print(f\"diceloss: inputs - {inputs.shape}, labels - {labels.shape}, num_masks: {num_masks.shape}\")\n","    # torch.Size([17, 12544]), torch.Size([17, 12544]), tensor([17.], num_masks: torch.Size([1])??? bug?\n","\n","    probs = inputs.sigmoid().flatten(1)\n","    print(f\"probs: {probs.shape}\") # torch.Size([17, 12544])\n","\n","    numerator = 2 * (probs * labels).sum(-1)\n","    denominator = probs.sum(-1) + labels.sum(-1)\n","    print(f\"numerator: {numerator}, denominator:{denominator}\") # torch.Size([17]), torch.Size([17])\n","    #example numerator: tensor([1.3901e+04, 4.6304e+02, 1.4267e+03, 1.3075e+04, 7.6254e+01, 3.5534e+03, 5.6537e+03, 1.2245e+02, 1.6539e+03, 4.5592e+02, 4.6478e-01, 1.6648e+03, 2.6228e+02, 7.2001e+03]),\n","    #example denominator:tensor([17006.8262,   669.5397,  1667.3591, 13333.7393,   247.1372,  6696.2036, 11886.2363,   280.5628,  1848.1116,  3018.4907,    87.1838,  4434.2354, 3855.7500, 11448.5762])\n","\n","    loss = 1 - (numerator + 1) / (denominator + 1)\n","    loss = loss.sum() / num_masks\n","    # apply ohem here\n","    print(f\"dice_loss: {loss.shape}, {loss}\") # dice_loss: torch.Size([17]) #tensor([0.2231, 0.0690, 0.9012, 0.5166, 0.7142, 0.4936, 0.1366, 0.4007, 0.5544, 0.0995, 0.3701, 0.7276, 0.0405, 0.4789, 0.2160, 0.6159, 0.1205]\n","    # dice_loss: torch.Size([1]), tensor([0.4830])??\n","    print(f\"mean_dice_loss:  {loss}\") # mean_dice_loss: tensor([0.3928])\n","    return loss"],"metadata":{"id":"eOovyycO1XB1","executionInfo":{"status":"ok","timestamp":1709386422794,"user_tz":-60,"elapsed":3,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def sample_point(input_features: torch.Tensor, point_coordinates: torch.Tensor, add_dim=False, **kwargs) -> torch.Tensor:\n","    print(f\"sample point: input_features: {input_features.shape}, point_coordinates: {point_coordinates.shape}\")\n","    # input_features: torch.Size([17, 1, 128, 128]), point_coordinates: torch.Size([17, 37632, 2]) #for pred_mask from sample_points_using_uncertainty before calling sample_point, oversampling\n","    # input_features: torch.Size([17, 1, 512, 512]), point_coordinates: torch.Size([17, 12544, 2]) #for targetlabel from sample_point\n","    # input_features: torch.Size([17, 1, 128, 128]), point_coordinates: torch.Size([17, 12544, 2]) #for pred_mask from sample_point\n","    if point_coordinates.dim() == 3:\n","        add_dim = True\n","        point_coordinates = point_coordinates.unsqueeze(2)\n","        print(f\"inside point_coordinates.dim() == 3, point_features: {point_coordinates.shape}\")\n","        #point_features: torch.Size([17, 37632, 1, 2]) #for pred_mask from sample_points_using_uncertainty before calling sample_point, oversample\n","        #point_features: torch.Size([17, 12544, 1, 2]) #for targetlabel from sample_point\n","        #point_features: torch.Size([17, 12544, 1, 2]) #for pred_mask from sample_point\n","    point_features = torch.nn.functional.grid_sample(input_features, 2.0 * point_coordinates - 1.0, **kwargs) # point_coordinates: This tensor contains the coordinates of the points where the sampling should be performed. It's assumed to be in the range of [-1, 1] for both dimensions, where (-1, -1) represents the top-left corner of the input feature map, and (1, 1) represents the bottom-right corner. The expression 2.0 * point_coordinates - 1.0 is used to normalize the coordinates to the range [-1, 1].\n","    print(f\"point_features: {point_features.shape}\")\n","    #torch.Size([17, 1, 37632, 1]) ##for pred_mask from sample_points_using_uncertainty before calling sample_point, oversample\n","    #torch.Size([17, 1, 12544, 1]) ##for targetlabel from sample_point\n","    #torch.Size([17, 1, 12544, 1]) ##for pred_mask from sample_point\n","    if add_dim:\n","        point_features = point_features.squeeze(3)\n","        print(f\"inside add_dim, point_features: {point_features.shape}\")\n","        #torch.Size([17, 1, 37632]) ###for pred_mask from sample_points_using_uncertainty before calling sample_point, oversample\n","        #torch.Size([17, 1, 12544]) ###for targetlabel from sample_point\n","        #torch.Size([17, 1, 12544]) ###for pred_mask from sample_point\n","    return point_features"],"metadata":{"id":"uWmTKsBLTtVA","executionInfo":{"status":"ok","timestamp":1709386421397,"user_tz":-60,"elapsed":323,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["class OhemCELoss(nn.Module):\n","\n","    def __init__(self, score_thresh, n_min=None, ignore_index=255):\n","        super(OhemCELoss, self).__init__()\n","        self.score_thresh = score_thresh\n","        self.ignore_lb = ignore_index\n","        self.n_min = n_min\n","        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_index, reduction='mean')\n","\n","    def forward(self, logits, labels):\n","        n_min = labels.numel() // 16 if self.n_min is None else self.n_min\n","        labels = ohem_cpp.score_ohem_label(logits.float(), labels,\n","                self.ignore_lb, self.score_thresh, n_min).detach()\n","        loss = self.criteria(logits, labels)\n","        return loss"],"metadata":{"id":"0FSWp2TNa31f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import nn\n","loss_manual_no_reduction = nn.CrossEntropyLoss(weight=ce_class_weights, reduction=\"none\")\n","loss_ce_without_reduction = loss_manual_no_reduction(pred_logits_transposed, target_classes) # (2, 17, 250), (2, 250)\n","print(f\"loss_ce_without_reduction: {loss_ce_without_reduction.shape}, {loss_ce_without_reduction}\") # torch.Size([2, 250])\n","loss_ce_manual_mean = loss_ce_without_reduction.sum() / self.empty_weight[target_classes].sum()\n","\n","class OhemCrossEntropy2d(nn.Module):\n","    def __init__(self, thresh=0.7, min_kept=0, ignore_index=255, is_binary=False, **kwargs):\n","        super().__init__()\n","        self.ignore_label = ignore_index\n","        self.is_binary = is_binary\n","        self.thresh = float(thresh)\n","        self.min_kept = int(min_kept)\n","        self.criterion = nn.CrossEntropyLoss(**kwargs) #https://discuss.pytorch.org/t/softmax-cross-entropy-loss/125383\n","\n","    def forward(self, logits, labels, **_):\n","        \"\"\"\n","            Args:\n","                predict:(n, c, h, w), pred_logits_transposed: (n, num_labels, num_queries)\n","                labels:(n, h, w), target_labels: (n, num_queries)\n","        \"\"\"\n","\n","        if self.is_binary:\n","            predict = torch.sigmoid(logits)\n","        else:\n","            predict = F.softmax(logits, dim=1)\n","\n","        n, c, h, w = predict.size()\n","        input_label = labels.detach().cpu().numpy().ravel().astype(np.int32)\n","        x = np.rollaxis(predict.detach().cpu().numpy(), 1).reshape((c, -1))\n","        input_prob = np.exp(x - x.max(axis=0).reshape((1, -1)))\n","        input_prob /= input_prob.sum(axis=0).reshape((1, -1))\n","\n","        valid_flag = input_label != self.ignore_label\n","        valid_inds = np.where(valid_flag)[0]\n","        label = input_label[valid_flag]\n","        num_valid = valid_flag.sum()\n","        if self.min_kept >= num_valid:\n","            print('Labels: {}'.format(num_valid))\n","        elif num_valid > 0:\n","            prob = input_prob[:, valid_flag]\n","            pred = prob[label-1, np.arange(len(label), dtype=np.int32)]\n","            threshold = self.thresh\n","            if self.min_kept > 0:\n","                index = pred.argsort()\n","                threshold_index = index[min(len(index), self.min_kept) - 1]\n","                if pred[threshold_index] > self.thresh:\n","                    threshold = pred[threshold_index]\n","            kept_flag = pred <= threshold\n","            valid_inds = valid_inds[kept_flag]\n","            #print('hard ratio: {} = {} / {} '.format(round(len(valid_inds)/num_valid, 4), len(valid_inds), num_valid))\n","\n","        label = input_label[valid_inds].copy()\n","        input_label.fill(self.ignore_label)\n","        input_label[valid_inds] = label\n","        #print(np.sum(input_label != self.ignore_label))\n","        labels = torch.from_numpy(input_label.reshape(labels.size())).type_as(predict).to(labels.device)\n","\n","        predict = predict.squeeze()          # in case we're dealing with B/W images instead of RGB\n","        return self.criterion(predict, labels)"],"metadata":{"id":"NIxcZG0LaWHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import nn\n","\n","# loss_manual_no_reduction = nn.CrossEntropyLoss(weight=ce_class_weights, reduction=\"none\")\n","# loss_ce_without_reduction = loss_manual_no_reduction(pred_logits_transposed, target_classes) # (2, 17, 250), (2, 250)\n","# print(f\"loss_ce_without_reduction: {loss_ce_without_reduction.shape}, {loss_ce_without_reduction}\") # torch.Size([2, 250])\n","# loss_ce_manual_mean = loss_ce_without_reduction.sum() / self.empty_weight[target_classes].sum()\n","\n","class OhemCELoss(nn.Module):\n","    \"\"\"\n","    OhemCELoss - Online Hard Example Mining Cross Entropy Loss\n","    \"\"\"\n","\n","    def __init__(self,\n","                 weights: torch.Tensor,\n","                 threshold: float=0.4,\n","                 mining_percent: float=0.8,\n","                 ignore_index: int=255,\n","                 num_masks_exclude_ignored: bool = True):\n","        \"\"\"\n","        Args:\n","            threshold: Sample below probability threshold, is considered hard.\n","            num_masks_exclude_ignored: How to calculate total masks from which extract mining percent of the samples\n","            ignore_index: label index to be ignored in loss calculation.\n","            criteria: loss to mine the examples from.\n","\n","          Example: for num_masks=100, ignore_masks=30(from ignore_index label), mining_percent=0.1:\n","                        num_masks_exclude_ignored=False => num_mining = 100 * 0.1 = 10\n","                        num_masks_exclude_ignored=True  => num_mining = (100 - 30) * 0.1 = 7\n","        \"\"\"\n","        super().__init__()\n","\n","        if mining_percent < 0 or mining_percent > 1:\n","            raise ValueError(f\"mining percent should be between (0, 1] but given {mining_percent}\")\n","\n","        self.thresh = -torch.log(torch.tensor(threshold, dtype=torch.float))\n","        self.mining_percent = mining_percent\n","        self.ignore_index = 255 if ignore_index is None or ignore_index < 0 else ignore_index\n","        self.num_masks_exclude_ignored = num_masks_exclude_ignored\n","        self.class_weights = weights\n","        self.criteria = nn.CrossEntropyLoss(weight=self.class_weights, ignore_index=ignore_index, reduction=\"none\") #https://discuss.pytorch.org/t/softmax-cross-entropy-loss/125383\n","        if self.criteria.reduction != \"none\":\n","            raise ValueError(f\"Criteria reduction should be none but given {self.criteria.reduction}\")\n","\n","\n","    def forward(self, logits, labels):\n","        # pred_logits_transposed => logits : (n, num_labels_in_batch, num_queries)\n","        # target_labels=> labels : (n, num_queries)\n","\n","        loss = self.criteria(logits, labels).view(-1)\n","\n","\n","       # Flatten the labels tensor\n","       flattened_labels = labels.view(-1)\n","\n","        # Excluded Masks of ignore_index class label\n","        if self.num_masks_exclude_ignored:\n","            ignore_masks = labels.view(-1) != self.ignore_index\n","            flattened_labels = labels[ignore_masks]\n","            loss = loss[ignore_masks] # remove ignore label elements\n","            num_masks = loss.numel() # total num masks in a batch -> num_masks = batch_size * num_masks_in_batch * num_queries - ignore_masks\n","        else:\n","            num_masks = labels.numel()\n","\n","        if num_masks == 0: # if all masks are ignore labels, return empty loss tensor\n","            return torch.tensor([0.0]).requires_grad_(True)\n","\n","        #Hard Mining\n","        num_mining = int(self.mining_percent * num_masks)\n","        num_mining = min(num_mining, num_masks - 1) # in case mining_percent=1, prevent out of bound exception\n","\n","        self.thresh = self.thresh.to(logits.device)\n","        # orting\n","        loss, sorted_indices = torch.sort(loss, descending=True)\n","        sorted_labels = flattened_labels[sorted_indices]\n","\n","        if loss[num_mining] > self.thresh: #check if value at num_mining index is > thresh\n","            mask = loss > self.thresh\n","            loss = loss[mask] #get the loss where the mask is true #original shape\n","            selected_labels = sorted_labels[mask]\n","        else:\n","            loss = loss[:num_mining] # num_mining #If there are fewer loss values above the threshold than num_mining, it selects the top num_mining loss values.\n","            selected_labels = sorted_labels[:num_mining]\n","\n","        if len(loss) == 0:  # Check if no loss values are selected i.e no loss values are greater than threshold\n","           return torch.tensor([0.0]).requires_grad_(True)\n","\n","        # Calculate the class weights for the selected masks\n","        # selected_labels = labels.view(-1)[loss.view(-1) > self.thresh]\n","\n","        #calculate weighted meanloss\n","        loss_mean = loss.sum() / self.class_weights[selected_labels].sum() #we need to get labels after mining\n","        return loss_mean\n"],"metadata":{"id":"JJ4f5pUemrrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomOneFormerLoss(OneFormerLoss):\n","    def __init__(self, *args, **kwargs, class_specific_weights):\n","        super().__init__(*args, **kwargs)\n","        print(f\"Empty Weight: {self.empty_weight}\") #existing weights logic,ones to all classes\n","\n","        if class_specific_weights is not None: #new for class weights #adding also weight for bg class\n","            class_specific_weights.append(self.eos_coef)\n","        self.register_buffer(\"class_specific_weights\", class_specific_weights)\n","\n","\n","    def calculate_uncertainty(self, logits: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        In Mask2Former paper, uncertainty is estimated as L1 distance between 0.0 and the logit prediction in 'logits' for the foreground class in `classes`.\n","        Args:\n","            logits (`torch.Tensor`): A tensor of shape (R, 1, ...) for class-specific or class-agnostic, where R is the total number of predicted masks in all images and C is:\n","            the number of foreground classes. The values are logits.\n","        Returns:\n","            scores (`torch.Tensor`): A tensor of shape (R, 1, ...) that contains uncertainty scores with the most uncertain locations having the highest uncertainty score.\n","        \"\"\"\n","        uncertainty_scores = -(torch.abs(logits))\n","        return uncertainty_scores\n","\n","    def sample_points_using_uncertainty(self, logits: torch.Tensor, uncertainty_function, num_points: int, oversample_ratio: int, importance_sample_ratio: float,) -> torch.Tensor:\n","        \"\"\"\n","        This function is meant for sampling points in [0, 1] * [0, 1] coordinate space based on their uncertainty. The\n","        uncertainty is calculated for each point using the passed `uncertainty function` that takes points logit\n","        prediction as input.\n","        Args:\n","            logits (`float`): Logit predictions for P points.\n","            uncertainty_function: A function that takes logit predictions for P points and returns their uncertainties.\n","            num_points (`int`): The number of points P to sample.\n","            oversample_ratio (`int`): Oversampling parameter.\n","            importance_sample_ratio (`float`): Ratio of points that are sampled via importance sampling.\n","        Returns:\n","            point_coordinates (`torch.Tensor`): Coordinates for P sampled points.\n","        \"\"\"\n","        print(\"sample_points_using_uncertainty\")\n","        num_boxes = logits.shape[0] # torch.Size([17, 1, 128, 128])\n","        print(f\"num_boxes - {num_boxes}\") #num_boxes - 17\n","\n","        num_points_sampled = int(num_points * oversample_ratio) # 12544 * 3.0\n","        print(f\"num_points_sampled- {num_points_sampled}\") # 37632\n","\n","        # Get random point coordinates\n","        point_coordinates = torch.rand(num_boxes, num_points_sampled, 2, device=logits.device)\n","        print(f\"point_coordinates - {point_coordinates.shape}\") # point_coordinates - torch.Size([17, 37632, 2])\n","        # Get sampled prediction value for the point coordinates\n","        point_logits = sample_point(logits, point_coordinates, align_corners=False)\n","        print(f\"point_logits - {point_logits.shape}\") # torch.Size([17, 1, 37632])\n","        # Calculate the uncertainties based on the sampled prediction values of the points\n","        point_uncertainties = uncertainty_function(point_logits) #\n","        print(f\"point_uncertainties - {point_uncertainties.shape}\") # torch.Size([17, 1, 37632])\n","        num_uncertain_points = int(importance_sample_ratio * num_points) #0.75 * 12544 = 9408\n","        print(f\"num_uncertain_points: {num_uncertain_points}\") #num_uncertain_points: 9408\n","        num_random_points = num_points - num_uncertain_points #12544 - 9408\n","        print(f\"num_random_points: {num_random_points}\") #num_random_points: 3136\n","\n","        idx = torch.topk(point_uncertainties[:, 0, :], k=num_uncertain_points, dim=1)[1] #\n","        print(f\"idx: {idx.shape}\") # torch.Size([17, 9408])\n","        #idx: tensor([[23785, 27780, 31340,  ..., 28887,  8046, 30607],\n","        # [18878, 13895, 31258,  ..., 28003,  6732,  2762],\n","        # [ 7469, 19368, 21082,  ..., 19072, 14708, 11367],\n","        # ...,\n","        # [18381, 33648, 35147,  ..., 32872,   717, 21641],\n","        # [  321, 24568, 10217,  ..., 11078, 27017, 13298],\n","        # [32526, 12735, 34185,  ..., 13917, 17858, 12344]], device='cuda:0')\n","        shift = num_points_sampled * torch.arange(num_boxes, dtype=torch.long, device=logits.device)\n","        print(f\"shift: {shift.shape}\") #shift: torch.Size([17])\n","        idx += shift[:, None]\n","        point_coordinates = point_coordinates.view(-1, 2)[idx.view(-1), :].view(num_boxes, num_uncertain_points, 2)\n","        print(f\"point_coordinates: {point_coordinates.shape}\") #torch.Size([17, 9408, 2])\n","        if num_random_points > 0:\n","            point_coordinates = torch.cat(\n","                [point_coordinates, torch.rand(num_boxes, num_random_points, 2, device=logits.device)],\n","                dim=1,\n","            )\n","            print(f\"inside num_random_points>0, point_coordinates: {point_coordinates.shape}\") #torch.Size([17, 12544, 2])\n","        return point_coordinates\n","\n","    def _get_predictions_permutation_indices(self, indices):\n","        # permute predictions following indices\n","        batch_indices = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n","        predictions_indices = torch.cat([src for (src, _) in indices])\n","        return batch_indices, predictions_indices\n","\n","    def _get_targets_permutation_indices(self, indices):\n","        # permute labels following indices\n","        batch_indices = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n","        target_indices = torch.cat([tgt for (_, tgt) in indices])\n","        return batch_indices, target_indices\n","\n","    def _max_by_axis(self, the_list: List[List[int]]) -> List[int]:\n","        maxes = the_list[0]\n","        for sublist in the_list[1:]:\n","            for index, item in enumerate(sublist):\n","                maxes[index] = max(maxes[index], item)\n","        return maxes\n","\n","    def _pad_images_to_max_in_batch(self, tensors: List[Tensor]) -> Tuple[Tensor, Tensor]:\n","        # get the maximum size in the batch\n","        max_size = self._max_by_axis([list(tensor.shape) for tensor in tensors])\n","        batch_size = len(tensors)\n","        # compute finel size\n","        batch_shape = [batch_size] + max_size\n","        b, _, h, w = batch_shape\n","        # get metadata\n","        dtype = tensors[0].dtype\n","        device = tensors[0].device\n","        padded_tensors = torch.zeros(batch_shape, dtype=dtype, device=device)\n","        padding_masks = torch.ones((b, h, w), dtype=torch.bool, device=device)\n","        # pad the tensors to the size of the biggest one\n","        for tensor, padded_tensor, padding_mask in zip(tensors, padded_tensors, padding_masks):\n","            padded_tensor[: tensor.shape[0], : tensor.shape[1], : tensor.shape[2]].copy_(tensor)\n","            padding_mask[: tensor.shape[1], : tensor.shape[2]] = False\n","\n","        return padded_tensors, padding_masks\n","\n","    def loss_labels(self, class_queries_logits: Tensor, class_labels: List[Tensor], indices: Tuple[np.array]) -> Dict[str, Tensor]:\n","        pred_logits = class_queries_logits\n","        print(f\"loss_labels - class_queries_logits or pred_logits: {pred_logits.shape}\") # torch.Size([2, 250, 17])\n","\n","        batch_size, num_queries, _ = pred_logits.shape\n","        criterion = nn.CrossEntropyLoss(weight=self.empty_weight)\n","        # INFO empty_weight = torch.ones(self.num_classes + 1) and empty_weight[-1] = self.eos_coef=0.1\n","        idx = self._get_predictions_permutation_indices(indices) #batch_indices, queries_indices\n","\n","        target_classes_o = torch.cat([target[j] for target, (_, j) in zip(class_labels, indices)]) # shape = (batch_size, num_queries)\n","        target_classes = torch.full((batch_size, num_queries), fill_value=self.num_classes, dtype=torch.int64, device=pred_logits.device) # shape = (batch_size, num_queries)\n","        target_classes[idx] = target_classes_o\n","        print(f\"target_classes: {target_classes.shape}, {target_classes}\") #torch.Size([2, 250])\n","\n","        # permute pred_logits (batch_size, num_queries, num_labels) -> (batch_size, num_labels, num_queries)\n","        pred_logits_transposed = pred_logits.transpose(1, 2)\n","        print(f\"pred_logits_transposed: {pred_logits_transposed.shape}\") # torch.Size([2, 17, 250])\n","\n","        loss_ce = criterion(pred_logits_transposed, target_classes)\n","        print(f\"loss_cross_entropy loss_ce: {loss_ce}\") # tensor(2.5460)\n","\n","        # New - so we need to implement ohem here for masks 250 batchwise based on threshold, minkept, also class importance\n","        if self.class_specific_weights is not None:\n","            ce_class_weights = self.class_specific_weights\n","        else:\n","            ce_class_weights = self.empty_weight\n","        loss_manual_no_reduction = nn.CrossEntropyLoss(weight=ce_class_weights, reduction=\"none\") # new to get losses per query\n","        loss_ce_without_reduction = loss_manual_no_reduction(pred_logits_transposed, target_classes)\n","        print(f\"loss_ce_without_reduction: {loss_ce_without_reduction.shape}, {loss_ce_without_reduction}\") # torch.Size([2, 250])\n","        loss_ce_manual_mean = loss_ce_without_reduction.sum() / self.empty_weight[target_classes].sum() # this is equal to usual weighted ce loss with reduction\n","        print(f\"loss_cross_entropy mean loss_ce_manual: {loss_ce_manual_mean}\") # tensor(2.5460)\n","\n","        # losses = {\"loss_cross_entropy\": loss_ce}\n","        losses = {\"loss_cross_entropy\": loss_ce_manual_mean} #new\n","        print(f\"loss_cross_entropy losses: {losses}\") #{'loss_cross_entropy': tensor(2.5460, device='cuda:0', grad_fn=<NllLoss2DBackward0>)}\n","        return losses\n","\n","    def loss_masks(self, masks_queries_logits: Tensor, mask_labels: List[Tensor], indices: Tuple[np.array], num_masks: int) -> Dict[str, Tensor]:\n","        \"\"\"\n","            masks_queries_logits `batch_size, num_queries, height, width`\n","            mask_labels :List of mask labels of shape `(labels, height, width)`.\n","            indices (`Tuple[np.array])`:\n","            num_masks (`int)`: The number of masks, used for normalization.\n","        \"\"\"\n","        src_idx = self._get_predictions_permutation_indices(indices)\n","        tgt_idx = self._get_targets_permutation_indices(indices)\n","        print(f\"src_idx: {src_idx}, tgt_idx: {tgt_idx}\")\n","        #src_idx: (tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([  2,  29,  39, 182, 194, 249,   0,  23,  29,  51, 160, 178, 204, 211, 215, 236, 249])),\n","        #tgt_idx: (tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([ 0,  5,  1,  3,  2,  4,  0,  7,  3,  5,  1,  2,  4,  6, 10,  8,  9]))\n","\n","        # masks_queries_logits - shape (batch_size, num_queries, height, width) = (2, 250, 128, 128)\n","        pred_masks = masks_queries_logits[src_idx]\n","\n","        # shape (batch_size, num_queries, height, width)\n","        # pad all and stack the targets to the num_masks dimension\n","        # upsample predictions to the target size, we have to add one dim to use interpolate\n","        target_masks, _ = self._pad_images_to_max_in_batch(mask_labels)\n","        target_masks = target_masks[tgt_idx]\n","\n","        print(f\"loss_masks: pred_masks- {pred_masks.shape}, target_masks- {target_masks.shape}\")\n","        #loss_masks: pred_masks- torch.Size([17, 128, 128]), target_masks- torch.Size([17, 512, 512])\n","\n","        pred_masks = pred_masks[:, None] # a new axis is added to the pred_masks tensor. Specifically, a singleton dimension (dimension of size 1) is inserted at the second position (index 1) of the tensor's shape.\n","        target_masks = target_masks[:, None]\n","        print(f\"pred_masks- {pred_masks.shape}, target_masks- {target_masks.shape}\")\n","        # pred_masks- torch.Size([17, 1, 128, 128]), target_masks- torch.Size([17, 1, 512, 512])\n","        with torch.no_grad():\n","            # sample point_coords - PointRend (mask2former)\n","            point_coords = self.sample_points_using_uncertainty(\n","                pred_masks, #torch.Size([17, 1, 128, 128])\n","                self.calculate_uncertainty, #uncertainty function\n","                self.num_points, #12544\n","                self.oversample_ratio, #3.0\n","                self.importance_sample_ratio, #0.75\n","            )\n","            print(f\"point_coords- {point_coords.shape}\") # torch.Size([17, 12544, 2]) , where K = num_points sampled = 12544 = 112x112\n","            # get ground-truth labels\n","            point_labels = sample_point(target_masks, point_coords, align_corners=False).squeeze(1)\n","            print(f\"point_labels: {point_labels.shape}\") #point_labels: torch.Size([17, 12544]),\n","        point_logits = sample_point(pred_masks, point_coords, align_corners=False).squeeze(1)\n","        print(f\"point_logits: {point_logits.shape}\") # point_logits: torch.Size([17, 12544])\n","        print(f\"num_masks: {num_masks}\") # num_masks: tensor([17.])\n","\n","        losses = {\n","            \"loss_mask\": sigmoid_cross_entropy_loss(point_logits, point_labels, num_masks),\n","            # \"loss_mask\": sigmoid_cross_entropy_loss(pred_masks.squeeze(), target_masks.squeeze(), num_masks),\n","            \"loss_dice\": dice_loss(point_logits, point_labels, num_masks),\n","        }\n","        print(f\"loss_masks losses: {losses}\")\n","        del pred_masks\n","        del target_masks\n","        return losses\n","\n","    def loss_distillation(self, teacher_queries_distillation_logits: Tensor, student_queries_distillation_logits: Tensor):  # new\n","        student_queries_distillation_logits = student_queries_distillation_logits.float() #bs, 234, 512\n","        # print(\"loss_distillation student_queries_distillation_logits, teacher_queries_distillation_logits\")\n","        # print(student_queries_distillation_logits.shape)\n","        # print(teacher_queries_distillation_logits.shape)\n","        # print(torch.unique(torch.eq(student_queries_distillation_logits, teacher_queries_distillation_logits)))\n","        # normalize\n","        teacher_logits = nn.functional.normalize(teacher_queries_distillation_logits.flatten(1), dim=-1) # FixMe: Do we need for logit scale?\n","        student_logits = nn.functional.normalize(student_queries_distillation_logits.flatten(1), dim=-1)\n","        loss_distillation = nn.functional.mse_loss(student_logits, teacher_logits) # FixMe: Get the criterion from args later,, we huber loss, or l1 loss or mse loss\n","        losses = {\"loss_distillation\": loss_distillation}\n","        return losses\n","\n","    def forward(self, masks_queries_logits: Tensor, class_queries_logits: Tensor, contrastive_queries_logits: Tensor, mask_labels: List[Tensor],\n","                class_labels: List[Tensor], text_queries: Tensor, teacher_queries_distillation_logits: Tensor,\n","                student_queries_distillation_logits: Tensor, auxiliary_predictions: Optional[Dict[str, Tensor]] = None, calculate_contrastive_loss: bool = True,\n","        ) -> Dict[str, Tensor]:\n","\n","        # retrieve the matching between the outputs of the last layer and the labels\n","        indices = self.matcher(masks_queries_logits, class_queries_logits, mask_labels, class_labels)\n","        # compute the average number of target masks for normalization purposes\n","        num_masks = self.get_num_masks(class_labels, device=class_labels[0].device)\n","        # get all the losses\n","        losses: Dict[str, Tensor] = {\n","            **self.loss_masks(masks_queries_logits, mask_labels, indices, num_masks),\n","            **self.loss_labels(class_queries_logits, class_labels, indices),\n","        }\n","        if calculate_contrastive_loss:\n","            losses = {\n","                **losses,\n","                **self.loss_contrastive(contrastive_queries_logits, text_queries),\n","                **self.loss_distillation(teacher_queries_distillation_logits, student_queries_distillation_logits)  # new\n","            }\n","\n","        # in case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n","        if auxiliary_predictions is not None:\n","            for idx, aux_outputs in enumerate(auxiliary_predictions):\n","                masks_queries_logits = aux_outputs[\"masks_queries_logits\"]\n","                class_queries_logits = aux_outputs[\"class_queries_logits\"]\n","                loss_dict = self.forward(\n","                    masks_queries_logits,\n","                    class_queries_logits,\n","                    None,\n","                    mask_labels,\n","                    class_labels,\n","                    None,\n","                    None,\n","                    None,  # new\n","                    calculate_contrastive_loss=False,\n","                )\n","                loss_dict = {f\"{key}_{idx}\": value for key, value in loss_dict.items()}\n","                print(\"auxiliary losses\")\n","                print(len(auxiliary_predictions)) #9\n","                print(len(loss_dict)) #3\n","                losses.update(loss_dict)\n","\n","        return losses"],"metadata":{"id":"EyXIwaZ3Z374","executionInfo":{"status":"ok","timestamp":1709387438431,"user_tz":-60,"elapsed":394,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["``` self explanation of loss_masks\n","#masks_queries_logits `batch_size, num_queries, height, width`\n","\n","import torch\n","masks_queries_logits = torch.randn(size=(2, 250, 128, 128))\n","masks_queries_logits.shape\n","src_idx = (torch.tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n","           torch.tensor([  2,  29,  39, 182, 194, 249,   0,  23,  29,  51, 160, 178, 204, 211, 215, 236, 249]))\n","tgt_idx = (torch.tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n","           torch.tensor([ 0,  5,  1,  3,  2,  4,  0,  7,  3,  5,  1,  2,  4,  6, 10,  8,  9]))\n","        \n","# shape (batch_size * num_queries, height, width)\n","pred_masks = masks_queries_logits[src_idx]\n","pred_masks.shape #torch.Size([17, 128, 128])\n","```"],"metadata":{"id":"Xl8ogGggtk8H"}},{"cell_type":"code","source":["from transformers.models.oneformer.modeling_oneformer import OneFormerForUniversalSegmentation\n","from transformers import CLIPTextModel\n","\n","class CustomOneFormerForUniversalSegmentation(OneFormerForUniversalSegmentation):\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.model = CustomOneFormerModel(config)\n","        self.is_training = config.is_training\n","        if self.is_training:\n","            print(\"Initliazing the pretrained teacher model\")\n","            self.teacher_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","        else:\n","            self.teacher_model = None\n","\n","        self.weight_dict: Dict[str, float] = {\n","             \"loss_cross_entropy\": config.class_weight, \"loss_mask\": config.mask_weight, \"loss_dice\": config.dice_weight,\n","             \"loss_contrastive\": config.contrastive_weight, \"loss_distillation\": config.contrastive_weight,  # new # FIX ME: Setting the weight for distillation loss later. For now, we are using same weight as constrastive weight = 0.5\n","        }\n","        if \"class_specific_weights\" in config:\n","            class_specific_weights = config.class_specific_weights\n","        else:\n","            class_specific_weights = None\n","\n","        self.criterion = CustomOneFormerLoss(\n","            class_specific_weights=class_specific_weights, #new for class weights\n","            num_classes=config.num_labels, matcher=self.matcher, weight_dict=self.weight_dict, eos_coef=config.no_object_weight, num_points=config.train_num_points,\n","            oversample_ratio=config.oversample_ratio, importance_sample_ratio=config.importance_sample_ratio, contrastive_temperature=config.contrastive_temperature,\n","        )\n","\n","    def get_loss_dict(self, masks_queries_logits: Tensor, class_queries_logits: Tensor, contrastive_queries_logits: Tensor, mask_labels: Tensor,\n","                      class_labels: Tensor, text_queries: Tensor, auxiliary_predictions: Dict[str, Tensor], teacher_queries_distillation_logits: Tensor,\n","                      student_queries_distillation_logits: Tensor, calculate_contrastive_loss: bool,\n","        ) -> Dict[str, Tensor]:\n","        loss_dict: Dict[str, Tensor] = self.criterion(\n","            masks_queries_logits=masks_queries_logits, class_queries_logits=class_queries_logits, contrastive_queries_logits=contrastive_queries_logits,\n","            mask_labels=mask_labels, class_labels=class_labels, text_queries=text_queries, teacher_queries_distillation_logits=teacher_queries_distillation_logits,\n","            student_queries_distillation_logits=student_queries_distillation_logits,\n","            auxiliary_predictions=auxiliary_predictions, calculate_contrastive_loss=calculate_contrastive_loss,\n","        )  # new\n","        # weight each loss by `self.weight_dict[<LOSS_NAME>]` including auxiliary losses\n","        print(\"loss weights\")\n","        print(f\"weight_dict, {self.weight_dict.items()}\")\n","        # weight_dict, dict_items([('loss_cross_entropy', 2.0), ('loss_mask', 5.0), ('loss_dice', 5.0), ('loss_contrastive', 0.5), ('loss_distillation', 0.5)])\n","        print(f\"loss_dict: {loss_dict.items()}\")\n","        ##loss_dict: dict_items(\n","        # batch mean losses [('loss_mask', tensor([0.5695], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice', tensor([0.4705], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy', tensor(2.6274, device='cuda:0', grad_fn=<NllLoss2DBackward0>)), ('loss_contrastive', tensor(0., device='cuda:0', grad_fn=<AddBackward0>)), ('loss_distillation', tensor(1.7493e-05, device='cuda:0', grad_fn=<MseLossBackward0>)),\n","        # auxiliary losses: ('loss_mask_0', tensor([0.4521], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_0', tensor([0.5151], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_0', tensor(2.6126, device='cuda:0', grad_fn=<NllLoss2DBackward0>)),\n","        # ('loss_mask_1', tensor([0.5129], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_1', tensor([0.5169], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_1', tensor(2.8194, device='cuda:0', grad_fn=<NllLoss2DBackward0>)),\n","        # ('loss_mask_2', tensor([0.6444], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_2', tensor([0.4852], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_2', tensor(2.6235, device='cuda:0', grad_fn=<NllLoss2DBackward0>)),\n","        # ('loss_mask_3', tensor([0.5162], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_3', tensor([0.4862], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_3', tensor(2.5991, device='cuda:0', grad_fn=<NllLoss2DBackward0>)),\n","        # ('loss_mask_4', tensor([0.5660], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_4', tensor([0.5032], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_4', tensor(2.6130, device='cuda:0', grad_fn=<NllLoss2DBackward0>)),\n","        # ('loss_mask_5', tensor([0.5792], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_5', tensor([0.5023], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_5', tensor(2.7068, device='cuda:0', grad_fn=<NllLoss2DBackward0>)),\n","        # ('loss_mask_6', tensor([0.6443], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_6', tensor([0.5041], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_6', tensor(2.6910, device='cuda:0', grad_fn=<NllLoss2DBackward0>)),\n","        # ('loss_mask_7', tensor([0.6240], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_7', tensor([0.5005], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_7', tensor(2.6339, device='cuda:0', grad_fn=<NllLoss2DBackward0>)),\n","        # ('loss_mask_8', tensor([0.5074], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice_8', tensor([0.4951], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy_8', tensor(2.5680, device='cuda:0', grad_fn=<NllLoss2DBackward0>))])\n","        for key, weight in self.weight_dict.items():\n","            for loss_key, loss in loss_dict.items():\n","                if key in loss_key:\n","                    print(loss_key, loss, weight)\n","                    loss *= weight\n","        return loss_dict\n","\n","    def get_loss(self, loss_dict: Dict[str, Tensor]) -> Tensor: #ohem\n","        # if use_ohem: # prepare someflag to enable ohem\n","\n","        return sum(loss_dict.values())\n","\n","    def get_teacher_logits(self, teacher_text_inputs):  # new\n","        if teacher_text_inputs.ndim is None:\n","            raise ValueError(\"text must not be NoneType\")\n","        if teacher_text_inputs.ndim not in [2, 3]:\n","            raise ValueError(\"Number of dimensions in text must be 2 or 3\")\n","        batch_size, num_text, hidden_dim_seq = teacher_text_inputs.shape\n","        teacher_text_inputs = teacher_text_inputs.reshape(batch_size * num_text, hidden_dim_seq)\n","        self.teacher_model.to(self.model.device)\n","        self.teacher_model.eval()\n","        with torch.inference_mode():\n","            print(\"teacher logits generation\")\n","            teacher_text_queries_distillation_logits = self.teacher_model(teacher_text_inputs)\n","            _, hidden_dim = teacher_text_queries_distillation_logits[\"pooler_output\"].shape\n","            teacher_text_queries_distillation_logits = teacher_text_queries_distillation_logits[\"pooler_output\"].reshape(batch_size, num_text, hidden_dim).float() #bs, 234, 512\n","            print(\"teacher_text_queries_distillation_logits shape\")\n","            print(teacher_text_queries_distillation_logits.shape)\n","        return teacher_text_queries_distillation_logits\n","\n","    def forward(self, pixel_values: Tensor, task_inputs: Tensor, text_inputs: Optional[Tensor] = None, mask_labels: Optional[List[Tensor]] = None,\n","                 class_labels: Optional[List[Tensor]] = None, pixel_mask: Optional[Tensor] = None, output_auxiliary_logits: Optional[bool] = None,\n","                 output_hidden_states: Optional[bool] = None, output_attentions: Optional[bool] = None, return_dict: Optional[bool] = None,\n","        ) -> CustomOneFormerForUniversalSegmentationOutput:\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states)\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.model(pixel_values=pixel_values, task_inputs=task_inputs, text_inputs=text_inputs, pixel_mask=pixel_mask,\n","                             output_hidden_states=output_hidden_states or self.config.use_auxiliary_loss,\n","                             output_attentions=output_attentions, return_dict=True,)\n","\n","        loss, loss_dict, auxiliary_predictions = None, None, None\n","        class_queries_logits = outputs.transformer_decoder_class_predictions\n","        masks_queries_logits = outputs.transformer_decoder_mask_predictions\n","        contrastive_queries_logits = outputs.transformer_decoder_contrastive_queries\n","        auxiliary_predictions = outputs.transformer_decoder_auxiliary_predictions\n","        text_queries = outputs.text_queries\n","\n","        if self.is_training:  # new\n","            teacher_text_queries_distillation_logits = self.get_teacher_logits(text_inputs)\n","            student_text_queries_distillation_logits = outputs[\"text_queries_distillation_logits\"]\n","        else:\n","            teacher_text_queries_distillation_logits, student_text_queries_distillation_logits = None, None\n","\n","        if mask_labels is not None and class_labels is not None:\n","            loss_dict: Dict[str, Tensor] = self.get_loss_dict(masks_queries_logits=masks_queries_logits, class_queries_logits=class_queries_logits,\n","                                                              contrastive_queries_logits=contrastive_queries_logits, mask_labels=mask_labels,\n","                                                              class_labels=class_labels, text_queries=text_queries, auxiliary_predictions=auxiliary_predictions,\n","                                                              teacher_queries_distillation_logits=teacher_text_queries_distillation_logits,  # new\n","                                                              student_queries_distillation_logits=student_text_queries_distillation_logits,\n","                                                              calculate_contrastive_loss=self.config.contrastive_temperature is not None,\n","                                                              )\n","            loss = self.get_loss(loss_dict)\n","\n","        output_auxiliary_logits = (self.config.output_auxiliary_logits if output_auxiliary_logits is None else output_auxiliary_logits)\n","        if not output_auxiliary_logits:\n","            auxiliary_predictions = None\n","\n","        output = CustomOneFormerForUniversalSegmentationOutput(class_queries_logits=class_queries_logits, masks_queries_logits=masks_queries_logits,\n","                                                               auxiliary_predictions=auxiliary_predictions, loss=loss, **outputs,)\n","        if not return_dict:\n","            output = tuple(v for v in output.values())\n","            if loss is not None:\n","                output = (loss) + output\n","        return output"],"metadata":{"id":"Mw9_VV1JeY41","executionInfo":{"status":"ok","timestamp":1709386432878,"user_tz":-60,"elapsed":326,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["```\n","'no_object_weight': 0.1,\n"," 'class_weight': 2.0,\n"," 'mask_weight': 5.0,\n"," 'dice_weight': 5.0,\n"," 'contrastive_weight': 0.5,\n"," 'contrastive_temperature': 0.07,\n"," 'train_num_points': 12544,\n"," 'oversample_ratio': 3.0,\n","  'importance_sample_ratio': 0.75,\n"," ```"],"metadata":{"id":"2Ls48Rl_vL3e"}},{"cell_type":"code","source":["from transformers import AutoProcessor, AutoModelForUniversalSegmentation, OneFormerForUniversalSegmentation, AutoConfig, CLIPTextModel\n","import torch.nn.functional as F\n","from torch import nn\n","import gc\n","\n","# # Processor\n","# model_ckpt = \"shi-labs/oneformer_cityscapes_swin_large\"\n","# processor = AutoProcessor.from_pretrained(model_ckpt, ignore_index=255, do_reduce_labels=False)\n","# processor.image_processor.num_text = 234\n","# model_config = AutoConfig.from_pretrained(model_ckpt, text_encoder_width=512, text_encoder_num_layers=2, is_training=True)\n","# model = CustomOneFormerForUniversalSegmentation.from_pretrained(model_ckpt, ignore_mismatched_sizes=True, config=model_config)\n","\n","# id2label\n","id2label, label2id, labels = get_modified_labels()\n","# model and processor\n","model_ckpt = \"shi-labs/oneformer_cityscapes_swin_large\"\n","model_config = AutoConfig.from_pretrained(\n","    model_ckpt, text_encoder_width=512, text_encoder_num_layers=2, is_training=True, id2label=id2label, label2id=label2id, num_labels=len(id2label),\n","    use_auxiliary_loss=False, output_auxiliary_logits=False\n",")\n","model = CustomOneFormerForUniversalSegmentation.from_pretrained(model_ckpt, ignore_mismatched_sizes=True, config=model_config)\n","# Freeze the model backbone params\n","for name, params in model.named_parameters():\n","    if name.startswith(\"model.pixel_level_module\"):\n","        params.requires_grad = False\n","processor = AutoProcessor.from_pretrained(model_ckpt, ignore_index=255, do_reduce_labels=False, do_rescale=False, do_resize=False)\n","metadata_id2label = {str(k): v for k, v in id2label.items()}\n","metadata = deepcopy(metadata_id2label)\n","metadata[\"thing_ids\"] = [11, 12, 13, 14]\n","metadata[\"class_names\"] = labels\n","processor.image_processor.metadata = metadata\n","processor.image_processor.num_labels = len(id2label)\n","processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx\n","\n","#dataset\n","mydataset = load_railsem_dataset()\n","total_size = len(mydataset[\"data\"])\n","print(f\"[INFO]: Total images: {total_size}\")\n","train_size = 8160\n","train_percentage = train_size / total_size\n","# Use train_test_split with specified percentages\n","splits = mydataset[\"data\"].train_test_split(train_size=train_percentage, shuffle=True)\n","train_split = splits[\"train\"]\n","val_split = splits[\"test\"].select(indices=(range(320))) #320\n","test_split = splits[\"test\"].select(indices=(range(20)))\n","print(f\"[INFO]: Total Training images: {len(train_split)}\")\n","print(f\"[INFO]: Total Validation images: {len(val_split)}\")\n","print(f\"[INFO]: Total Test images: {len(test_split)}\")\n","# transforms\n","transforms = get_transforms()\n","customdataset = CustomDataset(train_split.select(range(0, 6)), transforms)\n","patch_size = [512, 512]\n","train_queue = PatchQueue(customdataset, max_length=10, samples_per_image=5, patch_size=patch_size)\n","dataloader = _prepare_dataloader(train_queue)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","if torch.cuda.is_available():\n","    torch.cuda.reset_max_memory_allocated()\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","model.to(device)\n","\n","#model.eval()\n","model.train()\n","for step, batch in enumerate(dataloader):\n","    print(f\"Step: {step}\")\n","    batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"mask_labels\": [labels.to(device)\n","                            for labels in batch[\"mask_labels\"]],\n","            \"class_labels\": [labels.to(device)\n","                                for labels in batch[\"class_labels\"]],\n","            \"pixel_mask\": batch[\"pixel_mask\"].to(device),\n","            \"text_inputs\": batch[\"text_inputs\"].to(device),\n","            \"task_inputs\": batch[\"task_inputs\"].to(device),\n","    }\n","    #with torch.no_grad():\n","    with torch.autocast(device_type='cuda', dtype=torch.float16):\n","        print(f'mask_labels: {batch[\"mask_labels\"][0].shape}, {batch[\"mask_labels\"][1].shape}') # torch.Size([6, 512, 512]), torch.Size([11, 512, 512])\n","        print(f'class_labels: {batch[\"class_labels\"][0]}, {batch[\"class_labels\"][1]}') # class_labels: tensor([ 2,  4,  5,  8,  9, 10]), tensor([ 0,  1,  2,  3,  4,  5,  7,  9, 10, 11, 12])\n","        outputs = model(**batch_dict)\n","        print(outputs.keys())\n","        print(f'class_queries_logits: {outputs[\"class_queries_logits\"].shape}')\n","        print(f'masks_queries_logits: {outputs[\"masks_queries_logits\"].shape}')\n","        loss = outputs.loss\n","        print(\"final loss\")\n","        print(loss)\n","    loss.backward()\n","    del batch\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Up8-vxcaqMwN","executionInfo":{"status":"ok","timestamp":1709387596099,"user_tz":-60,"elapsed":10173,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}},"outputId":"de07754a-c132-472f-abd9-38cb7903dec2"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Initliazing the pretrained teacher model\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of CustomOneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized: ['model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_projector.layers.1.0.weight', 'teacher_model.text_model.embeddings.position_embedding.weight', 'teacher_model.text_model.embeddings.token_embedding.weight', 'teacher_model.text_model.encoder.layers.0.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.0.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.0.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.0.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.0.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.0.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.0.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.0.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.0.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.0.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.0.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.0.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.0.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.0.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.0.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.0.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.1.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.1.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.1.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.1.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.1.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.1.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.1.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.1.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.1.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.1.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.1.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.1.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.1.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.1.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.1.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.1.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.10.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.10.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.10.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.10.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.10.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.10.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.10.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.10.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.10.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.10.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.10.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.10.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.10.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.10.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.10.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.10.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.11.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.11.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.11.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.11.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.11.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.11.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.11.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.11.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.11.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.11.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.11.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.11.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.11.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.11.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.11.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.11.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.2.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.2.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.2.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.2.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.2.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.2.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.2.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.2.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.2.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.2.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.2.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.2.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.2.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.2.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.2.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.2.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.3.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.3.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.3.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.3.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.3.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.3.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.3.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.3.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.3.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.3.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.3.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.3.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.3.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.3.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.3.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.3.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.4.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.4.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.4.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.4.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.4.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.4.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.4.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.4.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.4.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.4.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.4.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.4.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.4.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.4.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.4.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.4.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.5.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.5.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.5.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.5.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.5.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.5.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.5.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.5.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.5.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.5.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.5.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.5.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.5.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.5.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.5.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.5.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.6.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.6.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.6.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.6.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.6.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.6.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.6.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.6.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.6.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.6.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.6.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.6.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.6.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.6.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.6.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.6.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.7.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.7.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.7.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.7.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.7.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.7.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.7.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.7.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.7.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.7.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.7.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.7.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.7.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.7.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.7.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.7.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.8.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.8.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.8.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.8.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.8.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.8.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.8.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.8.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.8.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.8.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.8.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.8.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.8.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.8.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.8.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.8.self_attn.v_proj.weight', 'teacher_model.text_model.encoder.layers.9.layer_norm1.bias', 'teacher_model.text_model.encoder.layers.9.layer_norm1.weight', 'teacher_model.text_model.encoder.layers.9.layer_norm2.bias', 'teacher_model.text_model.encoder.layers.9.layer_norm2.weight', 'teacher_model.text_model.encoder.layers.9.mlp.fc1.bias', 'teacher_model.text_model.encoder.layers.9.mlp.fc1.weight', 'teacher_model.text_model.encoder.layers.9.mlp.fc2.bias', 'teacher_model.text_model.encoder.layers.9.mlp.fc2.weight', 'teacher_model.text_model.encoder.layers.9.self_attn.k_proj.bias', 'teacher_model.text_model.encoder.layers.9.self_attn.k_proj.weight', 'teacher_model.text_model.encoder.layers.9.self_attn.out_proj.bias', 'teacher_model.text_model.encoder.layers.9.self_attn.out_proj.weight', 'teacher_model.text_model.encoder.layers.9.self_attn.q_proj.bias', 'teacher_model.text_model.encoder.layers.9.self_attn.q_proj.weight', 'teacher_model.text_model.encoder.layers.9.self_attn.v_proj.bias', 'teacher_model.text_model.encoder.layers.9.self_attn.v_proj.weight', 'teacher_model.text_model.final_layer_norm.bias', 'teacher_model.text_model.final_layer_norm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of CustomOneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_cityscapes_swin_large and are newly initialized because the shapes did not match:\n","- model.transformer_module.decoder.class_embed.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([14, 256]) in the model instantiated\n","- model.transformer_module.decoder.class_embed.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n","- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([14]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["[INFO]: Extracting Railsem19 dataset from hub on\n","[INFO]: Total images: 8500\n","[INFO]: Total Training images: 8160\n","[INFO]: Total Validation images: 320\n","[INFO]: Total Test images: 20\n","[INFO]: Found labels_info.json.Skipping Download...\n","[WARN]: Patch List is empty...\n","[WARN]:Exception while getting image for patching: 'NoneType' object is not an iterator\n","Step: 0\n","mask_labels: torch.Size([4, 512, 512]), torch.Size([10, 512, 512])\n","class_labels: tensor([ 2,  4,  5, 10]), tensor([ 0,  1,  2,  3,  4,  7,  8, 10, 11, 12])\n","inside my custom model\n","custom text mapper encoder output\n","text shape in encode_text textmapper- (bs, num_text, hidden_dim)\n","torch.Size([2, 234, 77])\n","encoded_text\n","torch.Size([468, 512])\n","text_queries_before_ctx\n","torch.Size([2, 234, 512])\n","text_queries after projection\n","torch.Size([468, 256])\n","teacher logits generation\n","teacher_text_queries_distillation_logits shape\n","torch.Size([2, 234, 512])\n","loss_masks: pred_masks- torch.Size([14, 128, 128]), target_masks- torch.Size([14, 512, 512])\n","src_idx: (tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([ 29,  61,  75, 249,   0,  15,  24, 168, 196, 206, 211, 215, 247, 249])), tgt_idx: (tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([3, 0, 1, 2, 4, 7, 2, 1, 6, 3, 0, 9, 5, 8]))\n","pred_masks- torch.Size([14, 1, 128, 128]), target_masks- torch.Size([14, 1, 512, 512])\n","sample_points_using_uncertainty\n","num_boxes - 14\n","num_points_sampled- 37632\n","point_coordinates - torch.Size([14, 37632, 2])\n","sample point: input_features: torch.Size([14, 1, 128, 128]), point_coordinates: torch.Size([14, 37632, 2])\n","inside point_coordinates.dim() == 3, point_features: torch.Size([14, 37632, 1, 2])\n","point_features: torch.Size([14, 1, 37632, 1])\n","inside add_dim, point_features: torch.Size([14, 1, 37632])\n","point_logits - torch.Size([14, 1, 37632])\n","point_uncertainties - torch.Size([14, 1, 37632])\n","num_uncertain_points: 9408\n","num_random_points: 3136\n","idx: torch.Size([14, 9408])\n","shift: torch.Size([14])\n","point_coordinates: torch.Size([14, 9408, 2])\n","inside num_random_points>0, point_coordinates: torch.Size([14, 12544, 2])\n","sample point: input_features: torch.Size([14, 1, 512, 512]), point_coordinates: torch.Size([14, 12544, 2])\n","inside point_coordinates.dim() == 3, point_features: torch.Size([14, 12544, 1, 2])\n","point_features: torch.Size([14, 1, 12544, 1])\n","inside add_dim, point_features: torch.Size([14, 1, 12544])\n","sample point: input_features: torch.Size([14, 1, 128, 128]), point_coordinates: torch.Size([14, 12544, 2])\n","inside point_coordinates.dim() == 3, point_features: torch.Size([14, 12544, 1, 2])\n","point_features: torch.Size([14, 1, 12544, 1])\n","inside add_dim, point_features: torch.Size([14, 1, 12544])\n","point_coords- torch.Size([14, 12544, 2]), point_logits: torch.Size([14, 12544]), point_labels: torch.Size([14, 12544]), num_masks: tensor([14.], device='cuda:0')\n","sigmoid_cross_entropy_loss_fn - inputs: torch.Size([14, 12544]), labels: torch.Size([14, 12544]), num_masks: tensor([14.], device='cuda:0')\n","cross_entropy_loss: torch.Size([14, 12544]), tensor([[6.9221e-01, 6.9220e-01, 6.9416e-01,  ..., 4.0173e-03, 1.1495e-03,\n","         2.5915e-03],\n","        [7.1602e-01, 6.7027e-01, 6.6871e-01,  ..., 0.0000e+00, 1.1921e-07,\n","         0.0000e+00],\n","        [6.9614e-01, 7.0106e-01, 7.0118e-01,  ..., 7.1526e-07, 0.0000e+00,\n","         0.0000e+00],\n","        ...,\n","        [6.9318e-01, 6.9082e-01, 6.9612e-01,  ..., 1.1955e-02, 0.0000e+00,\n","         0.0000e+00],\n","        [6.9125e-01, 6.9077e-01, 6.9075e-01,  ..., 2.9142e-04, 1.9599e-03,\n","         1.7079e-03],\n","        [6.9334e-01, 6.9341e-01, 6.9345e-01,  ..., 6.7587e+00, 3.8568e-03,\n","         1.8379e-02]], device='cuda:0',\n","       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n","mean_cross_entropy_loss: tensor([0.4132], device='cuda:0', grad_fn=<DivBackward0>)\n","diceloss: inputs - torch.Size([14, 12544]), labels - torch.Size([14, 12544]), num_masks: torch.Size([1])\n","probs: torch.Size([14, 12544]), numerator: tensor([1.3901e+04, 4.6304e+02, 1.4267e+03, 1.3075e+04, 7.6254e+01, 3.5534e+03,\n","        5.6537e+03, 1.2245e+02, 1.6539e+03, 4.5592e+02, 4.6478e-01, 1.6648e+03,\n","        2.6228e+02, 7.2001e+03], device='cuda:0', grad_fn=<MulBackward0>), denominator:tensor([17006.8262,   669.5397,  1667.3591, 13333.7393,   247.1372,  6696.2036,\n","        11886.2363,   280.5628,  1848.1116,  3018.4907,    87.1838,  4434.2354,\n","         3855.7500, 11448.5762], device='cuda:0', grad_fn=<AddBackward0>)\n","dice_loss: torch.Size([1]), tensor([0.4830], device='cuda:0', grad_fn=<DivBackward0>)\n","mean_dice_loss:  tensor([0.4830], device='cuda:0', grad_fn=<DivBackward0>)\n","loss_masks losses: {'loss_mask': tensor([0.4132], device='cuda:0', grad_fn=<DivBackward0>), 'loss_dice': tensor([0.4830], device='cuda:0', grad_fn=<DivBackward0>)}\n","loss_ce loss_labels\n","loss_without_reduction: torch.Size([2, 250]), tensor([[0.2924, 0.2416, 0.2393, 0.2635, 0.2518, 0.2598, 0.2422, 0.2539, 0.2541,\n","         0.2512, 0.2547, 0.2516, 0.2588, 0.2570, 0.2744, 0.2574, 0.2732, 0.2666,\n","         0.2570, 0.2754, 0.2600, 0.2361, 0.2500, 0.2752, 0.2641, 0.2705, 0.2514,\n","         0.2438, 0.2658, 2.4531, 0.2662, 0.2658, 0.2543, 0.2555, 0.2582, 0.2375,\n","         0.2637, 0.2553, 0.2332, 0.2352, 0.2480, 0.2645, 0.2609, 0.2674, 0.2631,\n","         0.2496, 0.2709, 0.2727, 0.2467, 0.2334, 0.2520, 0.2998, 0.2682, 0.2672,\n","         0.2672, 0.2619, 0.2297, 0.2516, 0.2639, 0.2652, 0.2379, 2.1582, 0.2590,\n","         0.2334, 0.2768, 0.2705, 0.2596, 0.2553, 0.2615, 0.2539, 0.2746, 0.2535,\n","         0.2314, 0.2529, 0.2297, 2.3555, 0.2572, 0.2543, 0.2488, 0.2662, 0.2418,\n","         0.2635, 0.2322, 0.2666, 0.2617, 0.2434, 0.2447, 0.2525, 0.2719, 0.2393,\n","         0.2658, 0.2646, 0.2516, 0.2615, 0.2707, 0.2439, 0.2436, 0.2432, 0.2707,\n","         0.2486, 0.2477, 0.2674, 0.2461, 0.2426, 0.2668, 0.2625, 0.2223, 0.2875,\n","         0.2193, 0.2357, 0.2611, 0.2568, 0.2521, 0.2660, 0.2559, 0.2543, 0.2629,\n","         0.2580, 0.2488, 0.2299, 0.2430, 0.2572, 0.2359, 0.2562, 0.2408, 0.2723,\n","         0.2617, 0.2391, 0.2352, 0.2338, 0.2676, 0.2795, 0.2162, 0.2459, 0.2566,\n","         0.2730, 0.2650, 0.2494, 0.2592, 0.2357, 0.2447, 0.2594, 0.2678, 0.2484,\n","         0.2492, 0.2404, 0.2668, 0.2572, 0.2408, 0.2387, 0.2496, 0.2594, 0.2666,\n","         0.2676, 0.2307, 0.2492, 0.2539, 0.2617, 0.2629, 0.2367, 0.2344, 0.2621,\n","         0.2613, 0.2715, 0.2508, 0.2625, 0.2279, 0.2605, 0.2475, 0.2559, 0.2615,\n","         0.2379, 0.2584, 0.2613, 0.2740, 0.2555, 0.2473, 0.2313, 0.2949, 0.2717,\n","         0.2348, 0.2625, 0.2637, 0.2342, 0.2283, 0.2455, 0.2578, 0.2588, 0.2693,\n","         0.2609, 0.2520, 0.2482, 0.2246, 0.2537, 0.2395, 0.2365, 0.2500, 0.2582,\n","         0.2531, 0.2613, 0.2559, 0.2652, 0.2604, 0.2506, 0.2445, 0.2695, 0.2814,\n","         0.2592, 0.2553, 0.2467, 0.2670, 0.2564, 0.2371, 0.2475, 0.2756, 0.2635,\n","         0.2768, 0.2541, 0.2570, 0.2479, 0.2402, 0.2580, 0.2471, 0.2689, 0.2234,\n","         0.2451, 0.2656, 0.2551, 0.2439, 0.2561, 0.2631, 0.2727, 0.2635, 0.2205,\n","         0.2373, 0.2594, 0.2496, 0.2699, 0.2432, 0.2570, 0.2494, 0.2543, 0.2623,\n","         0.2715, 0.2699, 0.2453, 0.2520, 0.2635, 0.2703, 2.7949],\n","        [2.9160, 0.2521, 0.2367, 0.2277, 0.2262, 0.2418, 0.2287, 0.2338, 0.2295,\n","         0.2332, 0.2332, 0.2188, 0.2498, 0.2389, 0.2559, 2.6055, 0.2549, 0.2545,\n","         0.2496, 0.2521, 0.2293, 0.2123, 0.2498, 0.2318, 2.6641, 0.2883, 0.2623,\n","         0.2270, 0.2508, 0.2191, 0.2426, 0.2428, 0.2316, 0.2152, 0.2504, 0.2609,\n","         0.2350, 0.2223, 0.2246, 0.2215, 0.2250, 0.2451, 0.2178, 0.2330, 0.2600,\n","         0.2377, 0.2285, 0.2705, 0.2439, 0.2328, 0.2516, 0.2889, 0.2412, 0.2500,\n","         0.2424, 0.2398, 0.2453, 0.2430, 0.2373, 0.2484, 0.2271, 0.2387, 0.2545,\n","         0.2346, 0.2494, 0.2553, 0.2309, 0.2324, 0.2352, 0.2219, 0.2348, 0.2303,\n","         0.2377, 0.2369, 0.2406, 0.2549, 0.2461, 0.2285, 0.2562, 0.2328, 0.2305,\n","         0.2537, 0.2434, 0.2578, 0.2254, 0.2125, 0.2318, 0.2262, 0.2619, 0.2234,\n","         0.2328, 0.2619, 0.2246, 0.2457, 0.2436, 0.2244, 0.2271, 0.2336, 0.2627,\n","         0.2631, 0.2443, 0.2406, 0.2314, 0.2457, 0.2666, 0.2314, 0.2240, 0.2684,\n","         0.2252, 0.2250, 0.2346, 0.2588, 0.2430, 0.2438, 0.2533, 0.2467, 0.2418,\n","         0.2271, 0.2318, 0.2262, 0.2404, 0.2434, 0.2279, 0.2330, 0.2316, 0.2701,\n","         0.2490, 0.2385, 0.2324, 0.2387, 0.2537, 0.2615, 0.2295, 0.2359, 0.2408,\n","         0.2484, 0.2564, 0.2379, 0.2365, 0.2355, 0.2496, 0.2289, 0.2303, 0.2510,\n","         0.2320, 0.2305, 0.2479, 0.2596, 0.2326, 0.2396, 0.2146, 0.2367, 0.2646,\n","         0.2191, 0.2516, 0.2406, 0.2475, 0.2535, 0.2432, 0.2254, 0.2539, 0.2408,\n","         0.2699, 0.2484, 0.2443, 0.2350, 0.2301, 0.2262, 2.6523, 0.2447, 0.2443,\n","         0.2426, 0.2445, 0.2277, 0.2572, 0.2293, 0.2311, 0.2332, 0.2652, 0.2549,\n","         0.2234, 0.2268, 0.2545, 0.2496, 0.2348, 0.2514, 0.2268, 0.2371, 0.2197,\n","         0.2457, 0.2432, 0.2500, 0.2316, 0.2387, 0.2576, 0.2230, 2.8555, 0.2326,\n","         0.2457, 0.2180, 0.2219, 0.2371, 0.2277, 0.2348, 0.2406, 0.2457, 2.3906,\n","         0.2359, 0.2756, 0.2381, 0.2242, 2.7266, 0.2336, 0.2236, 0.2467, 2.1934,\n","         0.2322, 0.2438, 0.2674, 0.2484, 0.2387, 0.2346, 0.2502, 0.2619, 0.2428,\n","         0.2420, 0.2932, 0.2490, 0.2346, 0.2328, 0.2613, 0.2299, 0.2352, 0.2400,\n","         0.2248, 0.2811, 0.2396, 0.2447, 0.2340, 0.2383, 0.2211, 0.2467, 0.2611,\n","         0.2771, 0.2467, 0.2199, 0.2572, 2.4434, 0.2529, 2.9902]],\n","       device='cuda:0', grad_fn=<ViewBackward0>)\n","inputs\n","pred_logits_transposed: torch.Size([2, 14, 250])\n","target_classes: torch.Size([2, 250]), tensor([[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 10, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13,  2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13,  4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,  5],\n","        [ 4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 10, 13, 13,\n","         13, 13, 13, 13, 13, 13,  2, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13,  1, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,  8, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13,  3, 13, 13, 13, 13,  0, 13, 13, 13, 12,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n","         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,  7, 13, 11]],\n","       device='cuda:0')\n","loss_labels losses: {'loss_cross_entropy': tensor(2.5032, device='cuda:0', grad_fn=<NllLoss2DBackward0>)}\n","loss weights\n","weight_dict, dict_items([('loss_cross_entropy', 2.0), ('loss_mask', 5.0), ('loss_dice', 5.0), ('loss_contrastive', 0.5), ('loss_distillation', 0.5)])\n","loss_dict: dict_items([('loss_mask', tensor([0.4132], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_dice', tensor([0.4830], device='cuda:0', grad_fn=<DivBackward0>)), ('loss_cross_entropy', tensor(2.5032, device='cuda:0', grad_fn=<NllLoss2DBackward0>)), ('loss_contrastive', tensor(1.3838, device='cuda:0', grad_fn=<AddBackward0>)), ('loss_distillation', tensor(1.6336e-05, device='cuda:0', grad_fn=<MseLossBackward0>))])\n","loss_cross_entropy tensor(2.5032, device='cuda:0', grad_fn=<NllLoss2DBackward0>) 2.0\n","loss_mask tensor([0.4132], device='cuda:0', grad_fn=<DivBackward0>) 5.0\n","loss_dice tensor([0.4830], device='cuda:0', grad_fn=<DivBackward0>) 5.0\n","loss_contrastive tensor(1.3838, device='cuda:0', grad_fn=<AddBackward0>) 0.5\n","loss_distillation tensor(1.6336e-05, device='cuda:0', grad_fn=<MseLossBackward0>) 0.5\n","odict_keys(['loss', 'class_queries_logits', 'masks_queries_logits', 'encoder_hidden_states', 'pixel_decoder_hidden_states', 'transformer_decoder_object_queries', 'transformer_decoder_contrastive_queries', 'transformer_decoder_mask_predictions', 'transformer_decoder_class_predictions', 'text_queries', 'text_queries_distillation_logits', 'task_token', 'attentions'])\n","class_queries_logits: torch.Size([2, 250, 14])\n","masks_queries_logits: torch.Size([2, 250, 128, 128])\n","final loss\n","tensor([10.1797], device='cuda:0', grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["len(dataloader), len(train_queue)"],"metadata":{"id":"BEd0P4Ehr4Cf","executionInfo":{"status":"aborted","timestamp":1709383918591,"user_tz":-60,"elapsed":27,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoProcessor, AutoModelForUniversalSegmentation, OneFormerForUniversalSegmentation, AutoConfig, CLIPTextModel\n","import torch.nn.functional as F\n","from torch import nn\n","\n","customdataset = CustomDataset(test_split.select(range(0, 6)))\n","testdataloader = _prepare_dataloader(customdataset)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Processor\n","model_ckpt = \"shi-labs/oneformer_cityscapes_swin_large\"\n","processor = AutoProcessor.from_pretrained(model_ckpt, ignore_index=255, do_reduce_labels=False)\n","processor.image_processor.num_text = 234\n","model_config = AutoConfig.from_pretrained(model_ckpt, text_encoder_width=512, text_encoder_num_layers=2, is_training=True)\n","model = CustomOneFormerForUniversalSegmentation.from_pretrained(model_ckpt, ignore_mismatched_sizes=True, config=model_config)\n","teacher_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","teacher_model.to(model.device)\n","teacher_model.eval()\n","distillation_criterion = F.mse_loss # FixMe: Get the criterion from args later,, we huber loss, or l1 loss or mse loss\n","distillation_weight = 0.5\n","distillation_temperature = 0.07\n","# logit_scale = nn.Parameter(torch.tensor(np.log(1 / distillation_temperature)))\n","\n","def get_features(dataset):\n","    all_features = []\n","    teacher_model.eval()\n","    with torch.no_grad():\n","        for step, batch in enumerate(testdataloader):\n","            print(batch[\"text_inputs\"][0].shape, batch[\"text_inputs\"][1].shape) #each text embedding is of torch.Size([234, 77])(num_queries, seqlength) => whole batch is (2, 234, 77),\n","            #concatenate the batchdimension\n","            text = batch[\"text_inputs\"]\n","            batch_size, num_text, hidden_dim_seq = text.shape\n","            text = text.reshape(batch_size * num_text, hidden_dim_seq)\n","\n","            # take features from the eot embedding (eot_token is the highest number in each sequence)\n","            text_queries = teacher_model(text)\n","            print(text_queries.keys())\n","            print(text_queries[\"last_hidden_state\"].shape) #torch.Size([234, 77, 512])  #in clip  # text_embeds.shape = [batch_size, sequence_length, transformer.width], but we have 234 sequences considering as batchsize for each ex\n","            print(text_queries[\"text_embeds\"].shape) #pooler_output for withoutprojection #torch.Size([234, 512])\n","            _, hidden_dim = text_queries[\"text_embeds\"].shape\n","            text_queries = text_queries[\"text_embeds\"].reshape(batch_size, num_text, hidden_dim)\n","            # our actual model has  [batch_size, 234, 256]\n","            # text_features = [teacher_model(example) for example in batch[\"text_inputs\"]] #text_inputs contains input_ids with attention_mask multiplied by imageprocesor of oneformer, we need texts to tokenize\n","            # all_features.extend(text_features)\n","            del batch\n","            break\n","        #, text_features\n","    return  text_queries# torch.stack(all_features), all_features\n","\n","model.eval()\n","for step, batch in enumerate(testdataloader):\n","    batch_dict = {\n","            \"pixel_values\": batch[\"pixel_values\"].to(device),\n","            \"mask_labels\": [labels.to(device)\n","                            for labels in batch[\"mask_labels\"]],\n","            \"class_labels\": [labels.to(device)\n","                                for labels in batch[\"class_labels\"]],\n","            \"pixel_mask\": batch[\"pixel_mask\"].to(device),\n","            \"text_inputs\": batch[\"text_inputs\"].to(device),\n","            \"task_inputs\": batch[\"task_inputs\"].to(device),\n","    }\n","    text = batch[\"text_inputs\"]\n","    batch_size, num_text, hidden_dim_seq = text.shape\n","    text = text.reshape(batch_size * num_text, hidden_dim_seq)\n","    #with torch.autocast():\n","    with torch.no_grad():\n","        outputs = model(**batch_dict)\n","        print(outputs.keys())\n","        loss = outputs.loss\n","        print(loss)\n","    with torch.inference_mode():\n","        print(\"teacher\")\n","        teacher_queries_logits = teacher_model(text)\n","        _, hidden_dim = teacher_queries_logits[\"pooler_output\"].shape\n","        teacher_queries_logits = teacher_queries_logits[\"pooler_output\"].reshape(batch_size, num_text, hidden_dim).float() #bs, 234, 512\n","        student_queries_logits = outputs[\"text_queries_distillation_logits\"].float() #bs, 234, 512\n","        # normalize\n","        teacher_queries_logits = nn.functional.normalize(teacher_queries_logits.flatten(1), dim=-1) #check whether the embeddings are normalixing already and need for logit scale\n","        student_queries_logits = nn.functional.normalize(student_queries_logits.flatten(1), dim=-1)\n","        ## logit scale clamp\n","        # logit_scale = torch.clamp(logit_scale.exp(), max=100)\n","        #teacher_queries_logits = teacher_queries_logits * logit_scale\n","        # student_queries_logits = student_queries_logits * logit_scale\n","        distill_loss = distillation_criterion(student_queries_logits, teacher_queries_logits)\n","        print(distill_loss)\n","        loss = distill_loss * distillation_weight + loss\n","        print(loss)\n","    # loss.backward()\n","\n","    del batch\n","    break\n"],"metadata":{"id":"2aW_CtPU9YIJ","executionInfo":{"status":"aborted","timestamp":1709383918591,"user_tz":-60,"elapsed":27,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs[\"text_queries_distillation_logits\"].shape, outputs[\"text_queries\"].shape, outputs[\"transformer_decoder_contrastive_queries\"].shape"],"metadata":{"id":"0LLZV91sm_Db","executionInfo":{"status":"aborted","timestamp":1709383918591,"user_tz":-60,"elapsed":26,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create in oneformertrainer - check the config is_training\n","def distill_oneformertextencoder():\n","    model_ckpt = \"shi-labs/oneformer_cityscapes_swin_large\"\n","    processor = AutoProcessor.from_pretrained(model_ckpt, ignore_index=255, do_reduce_labels=False)\n","    processor.image_processor.num_text = 234\n","    model_config = AutoConfig.from_pretrained(model_ckpt, text_encoder_width=512, text_encoder_num_layers=2, is_training=True)\n","    student_model = CustomOneFormerForUniversalSegmentation.from_pretrained(model_ckpt, ignore_mismatched_sizes=True, config=model_config)\n","    return student_model\n","\n","## Distllator class\n","class TextDistillator(nn.Module):\n","    #self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs\n","    def __init__(self, teacher_model, student_model, temperature: float = 1.0, training=False) -> None:\n","        super(TextDistillator, self).__init__()\n","        self.student = student_model\n","        self.training = student_model.config.is_training\n","        self.temperature = temperature\n","        self.teacher = teacher_model #clip model\n","        # place teacher on same device as student\n","        self.teacher.to(self.student.device)\n","        self.teacher.eval()\n","\n","    @property\n","    def temperature(self) -> float:\n","        return self._temperature if self.training else 1 #1 for inference\n","\n","    @temperature.setter\n","    def temperature(self, value : float) -> None:\n","        if value < 1:\n","            raise(ValueError(f\"Temperature must be above 1, it cannot be {value}\"))\n","        self._temperature = value\n","\n","    def get_logits(self, input_ids: Tensor=None, attention_mask: Tensor=None, from_teacher: bool = False) -> Tensor:\n","        if from_teacher:\n","            return self.teacher.classifier(self.roberta(input_ids, attention_mask)[0])\n","        return self.student.classifier(self.student(input_ids, attention_mask)[0])\n","\n","    def forward(self, batch_dict : Tensor) -> Tuple[Tensor, Tensor]:\n","        student_ouputs = self.get_logits(**batch_dict)\n","        student_text_encoder_queries_logits = student_ouputs[\"text_queries\"]\n","        teacher_text_queries_embeddings = batch_dict[\"text_inputs\"]\n","        teacher_logits = self.get_logits()\n","        return student_logits.softmax(1), self.loss(teacher_logits, student_logits, labels)\n","\n","    def compute_loss(self, teacher_logits, student_logits, return_outputs=False):\n","        student_output = self.student(**inputs)\n","\n","        with torch.no_grad():\n","          teacher_output = self.teacher(**inputs)\n","\n","        # Compute soft targets for teacher and student\n","        soft_teacher = F.softmax(teacher_output.logits / self.temperature, dim=-1)\n","        soft_student = F.log_softmax(student_output.logits / self.temperature, dim=-1)\n","\n","        # Compute the loss\n","        distillation_loss = self.loss_function(soft_student, soft_teacher) * (self.temperature ** 2)\n","\n","        # Compute the true label loss\n","        student_target_loss = student_output.loss\n","\n","        # Calculate final loss\n","        loss = (1. - self.lambda_param) * student_target_loss + self.lambda_param * distillation_loss\n","        return (loss, student_output) if return_outputs else loss\n","\n","    # def loss(self,\n","    #     teacher_logits : Tensor,\n","    #     student_logits : Tensor,\n","    #     labels : Tensor,\n","    # ) -> Tensor:\n","    #     \"\"\"\n","    #     The distillation loss for distilating a BERT-like model.\n","    #     The loss takes the (teacher_logits), (student_logits) and (labels) for various losses.\n","    #     \"\"\"\n","    #     # Temperature and sotfmax\n","    #     student_logits, teacher_logits = (student_logits / self.temperature).softmax(1), (teacher_logits / self.temperature).softmax(1)\n","    #     # Classification loss (problem-specific loss)\n","    #     loss = CrossEntropyLoss()(student_logits, labels)\n","    #     # CrossEntropy teacher-student loss\n","    #     loss = loss + CrossEntropyLoss()(student_logits, teacher_logits)\n","    #     # Cosine loss\n","    #     loss = loss + CosineEmbeddingLoss()(teacher_logits, student_logits, torch.ones(teacher_logits.size()[0]))\n","    #     # Average the loss and return it\n","    #     loss = loss / 3\n","    #     return loss"],"metadata":{"id":"hKSSnj_MJ7BJ","executionInfo":{"status":"aborted","timestamp":1709383918592,"user_tz":-60,"elapsed":27,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import CLIPTextModel, CLIPTextModelWithProjection, CLIPTextConfig\n","\n","#teacher_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","teacher_config = CLIPTextConfig.from_pretrained(\"openai/clip-vit-base-patch32\")\n","# teacher_config.projection_dim = 256  # RuntimeError: Error(s) in loading state_dict for CLIPTextModelWithProjection: size mismatch for text_projection.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([256, 512]). You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\n","teacher_model = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\", config=teacher_config)\n","\n","customdataset = CustomDataset(test_split.select(range(0, 6)))\n","testdataloader = _prepare_dataloader(customdataset)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(len(testdataloader))\n","\n","def get_features(dataset):\n","    all_features = []\n","    teacher_model.eval()\n","    with torch.no_grad():\n","        for step, batch in enumerate(testdataloader):\n","            print(batch[\"text_inputs\"][0].shape, batch[\"text_inputs\"][1].shape) #each text embedding is of torch.Size([234, 77])(num_queries, seqlength) => whole batch is (2, 234, 77),\n","            #concatenate the batchdimension\n","            text = batch[\"text_inputs\"]\n","            batch_size, num_text, hidden_dim_seq = text.shape\n","            text = text.reshape(batch_size * num_text, hidden_dim_seq)\n","\n","            # take features from the eot embedding (eot_token is the highest number in each sequence)\n","            text_queries = teacher_model(text)\n","            print(text_queries.keys())\n","            print(text_queries[\"last_hidden_state\"].shape) #torch.Size([234, 77, 512])  #in clip  # text_embeds.shape = [batch_size, sequence_length, transformer.width], but we have 234 sequences considering as batchsize for each ex\n","            print(text_queries[\"text_embeds\"].shape) #pooler_output for withoutprojection #torch.Size([234, 512])\n","            _, hidden_dim = text_queries[\"text_embeds\"].shape\n","            text_queries = text_queries[\"text_embeds\"].reshape(batch_size, num_text, hidden_dim)\n","            # our actual model has  [batch_size, 234, 256]\n","            # text_features = [teacher_model(example) for example in batch[\"text_inputs\"]] #text_inputs contains input_ids with attention_mask multiplied by imageprocesor of oneformer, we need texts to tokenize\n","            # all_features.extend(text_features)\n","            del batch\n","            break\n","        #, text_features\n","    return  text_queries# torch.stack(all_features), all_features\n","\n","text_queries = get_features(customdataset)"],"metadata":{"id":"MIfuUUx0uJ4i","executionInfo":{"status":"aborted","timestamp":1709383918592,"user_tz":-60,"elapsed":26,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"ITsrL97DuO_f","executionInfo":{"status":"aborted","timestamp":1709383918592,"user_tz":-60,"elapsed":26,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import clip\n","\n","customdataset = CustomDataset(test_split.select(range(0, 5)))\n","testdataloader = _prepare_dataloader(customdataset)\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","teacher_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","teacher_model.eval()\n","# for step, batch in enumerate(testdataloader):\n","#     print(batch.keys())\n","#     print(batch[\"text_inputs\"].shape) #[2, 234, 77]\n","#     print(batch[\"pixel_values\"].shape) #[2, 3, 1080, 1920]\n","#     with torch.no_grad():\n","#         #teacher_outputs = teacher_model(batch[\"pixel_values\"], batch[\"text_inputs\"])\n","#         text_features = teacher_model.encode_text(batch[\"text_inputs\"].squeeze())\n","#         text_features.shape # our actual model has  [batch_size, 234, 256]\n","#         break\n","\n","def get_features(dataset):\n","    all_features = []\n","    with torch.no_grad():\n","        for step, batch in enumerate(testdataloader):\n","            # outputs = teacher_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n","            text_features = [teacher_model.encode_text(example) for example in batch[\"text_inputs\"]]\n","            all_features.extend(text_features)\n","            del batch, text_features\n","    return torch.stack(all_features), all_features\n","\n","# Calculate the image features\n","train_features, all_features = get_features(customdataset)\n","\n","\n","\n","#image.shape, text.shape, text_features.shape, image_features.shape\n","# (torch.Size([1, 3, 224, 224]),\n","#  torch.Size([3, 77]),\n","#  torch.Size([1, 3, 512]),\n","#  torch.Size([1, 1, 512]))"],"metadata":{"id":"Q6tj3Ua4WPbs","executionInfo":{"status":"aborted","timestamp":1709383918592,"user_tz":-60,"elapsed":26,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_features.shape, len(all_features), all_features[0].shape #len(all_features)"],"metadata":{"id":"yjgnQoqg8EY-","executionInfo":{"status":"aborted","timestamp":1709383918592,"user_tz":-60,"elapsed":26,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.stack(train_features[1]).shape"],"metadata":{"id":"jRtBaH-4DMzs","executionInfo":{"status":"aborted","timestamp":1709383918592,"user_tz":-60,"elapsed":25,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note: we can get text lits from oneformer imageprocessor"],"metadata":{"id":"pDNpL0-wG62j"}},{"cell_type":"code","source":["from transformers import CLIPTokenizer, CLIPTextModel\n","\n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n","teacher_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","teacher_model.eval()\n","\n","inputs = tokenizer([\"a photo of a cat\"]*20, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\")\n","print(inputs.keys())\n","print(inputs[\"input_ids\"].shape)\n","print(inputs[\"attention_mask\"].shape)\n","\n","\n","# adding batch dimension\n","inputs[\"input_ids\"] = inputs[\"input_ids\"].unsqueeze(dim=0)\n","inputs[\"attention_mask\"]= inputs[\"attention_mask\"].unsqueeze(dim=0)\n","print(inputs[\"input_ids\"].shape)\n","print(inputs[\"attention_mask\"].shape)\n","outputs = teacher_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n","print(outputs.keys(), outputs[\"last_hidden_state\"].shape, outputs[\"pooler_output\"].shape)\n"],"metadata":{"id":"PuTLPXoVYAyU","executionInfo":{"status":"aborted","timestamp":1709383918592,"user_tz":-60,"elapsed":25,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["teacher_model.config"],"metadata":{"id":"Zc3BfsRbXQ39","executionInfo":{"status":"aborted","timestamp":1709383918592,"user_tz":-60,"elapsed":25,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["teacher_model"],"metadata":{"id":"PWOtluNEd8eC","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":25,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torchinfo.summary(teacher_model)"],"metadata":{"id":"QFz8z3emhnlV","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":25,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import clip\n","from PIL import Image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","#image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n","text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"], [\"a guava\", \"an elephant\"]).to(device)\n","with torch.no_grad():\n","    embedding = model.token_embedding(text)\n","# with torch.no_grad():\n","#     image_features = model.encode_image(image)\n","#     text_features = model.encode_text(text)\n","#     new_image_features = image_features.unsqueeze(dim=0)\n","#     new_text_features = text_features.unsqueeze(dim=0)\n","#     logits_per_image, logits_per_text = model(image, text)\n","#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","# print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"],"metadata":{"id":"aNUWdHrohqQy","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":25,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding.shape"],"metadata":{"id":"m9H1krKYwtAK","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":24,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image.shape, text.shape, text_features.shape, image_features.shape"],"metadata":{"id":"Y9a3WOzivxPK","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":24,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"id":"VifTiGNDTOBE","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":24,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchmetrics\n","\n","\n","# Create target tensor with half 0s and half 1s\n","target_half_zeros = torch.zeros(540, 1920, dtype=torch.long)\n","target_half_ones = torch.ones(540, 1920, dtype=torch.long)\n","target = torch.cat((target_half_zeros, target_half_ones), dim=0)\n","\n","\n","pred = torch.zeros(1080, 1920, dtype=torch.long)\n","pred[:540, :960] = 1\n","pred[540:, 960:] = 1\n","\n","\n"],"metadata":{"id":"5deEp-aFv91w","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":24,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred.shape, target.shape, pred, target"],"metadata":{"id":"3Tj-dbbiTmKq","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":23,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy = torchmetrics.classification.Accuracy(task=\"binary\", num_classes=2, average=\"weighted\")\n","accuracy(pred, target)"],"metadata":{"id":"BJK6z4UiYgB5","executionInfo":{"status":"aborted","timestamp":1709383918593,"user_tz":-60,"elapsed":23,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchmetrics.functional as F\n","print(F.confusion_matrix(pred, target, task=\"binary\", num_classes=2))\n","print(F.precision(pred, target, num_classes=2, task=\"binary\",))\n","print(F.accuracy(pred, target, num_classes=2, task=\"binary\",))"],"metadata":{"id":"I9VBR-IMbNav","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":24,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jaccard = torchmetrics.classification.JaccardIndex(task=\"binary\", num_classes=2)\n","jaccard(pred, target)"],"metadata":{"id":"mTIpiCJ4b_4k","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":24,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchmetrics.functional as F\n","print(F.jaccard_index(pred, target, task=\"binary\", num_classes=2))"],"metadata":{"id":"bRP0T6Mcb7Hu","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":23,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchmetrics.functional as F\n","\n","preds = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n","target = torch.tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1])\n","\n","print(F.confusion_matrix(preds, target, task=\"binary\", num_classes=2))\n","print(F.precision(preds, target, num_classes=2, task=\"binary\",))\n","print(F.accuracy(preds, target, num_classes=2, task=\"binary\",))"],"metadata":{"id":"pmQl0Arca8TP","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":23,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GNYF9KiSaheg","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":23,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jaccard = torchmetrics.classification.JaccardIndex(task=\"binary\", num_classes=2, ignore_index=255, average=\"none\")\n","jaccard(pred, target)"],"metadata":{"id":"z_Q-aENyY-Rd","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":23,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jaccard = torchmetrics.classification.MulticlassJaccardIndex(num_classes=20, ignore_index=255, average=\"weighted\")\n","jaccard(pred, target)"],"metadata":{"id":"dzBHFko1T78q","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":22,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jaccard = torchmetrics.classification.MulticlassJaccardIndex(num_classes=20, ignore_index=255, average=\"macro\")\n","jaccard(pred, target)"],"metadata":{"id":"gr7iR2LDUzAv","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":22,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jaccard = torchmetrics.classification.MulticlassJaccardIndex(num_classes=20, ignore_index=255, average=\"micro\")\n","jaccard(pred, target)"],"metadata":{"id":"n6hbOt2jU66T","executionInfo":{"status":"aborted","timestamp":1709383918594,"user_tz":-60,"elapsed":22,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zGqb3J0xU9-C","executionInfo":{"status":"aborted","timestamp":1709383918595,"user_tz":-60,"elapsed":23,"user":{"displayName":"Bhavana Malla","userId":"13569433150164017713"}}},"execution_count":null,"outputs":[]}]}